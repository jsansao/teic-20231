{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsansao/teic-20231/blob/main/TEIC_Licao36_EngenhariaPrompt_SelfConsistencyReAct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDPQlwpsRaBb"
      },
      "source": [
        "# üöÄ Engenharia de Prompt Avan√ßada: Self-Consistency e ReAct (Tool Calling) com Gemini\n",
        "\n",
        "Este notebook demonstra duas t√©cnicas avan√ßadas de engenharia de prompt que v√£o al√©m do Chain-of-Thought (CoT) b√°sico.\n",
        "\n",
        "1.  **Self-Consistency:** Como melhorar a confiabilidade das respostas CoT rodando o prompt m√∫ltiplas vezes e fazendo uma \"vota√ß√£o\".\n",
        "2.  **ReAct (Reason + Act):** Como permitir que o LLM use ferramentas externas para responder perguntas. Mostraremos isso usando a funcionalidade nativa de **Tool Calling** do Gemini, que √© a evolu√ß√£o moderna do padr√£o ReAct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H2FFVIQRaBd"
      },
      "source": [
        "## Passo 1: Instala√ß√£o e Configura√ß√£o\n",
        "\n",
        "Primeiro, instalamos a biblioteca e configuramos a API Key.\n",
        "\n",
        "**Instru√ß√µes (Importante):**\n",
        "1.  Gere sua API Key no [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
        "2.  Clique no √≠cone de **chave** (üîë) na barra lateral esquerda do Colab.\n",
        "3.  Adicione um novo segredo chamado `GOOGLE_API_KEY` e cole sua chave l√°."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mgKakHiURaBd"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUmJDVjeRaBe",
        "outputId": "f77700a8-c4af-4688-a83a-0059ce554f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key configurada com sucesso!\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\"API Key configurada com sucesso!\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"Erro: 'GOOGLE_API_KEY' n√£o encontrada nos Secrets do Colab.\")\n",
        "    print(\"Por favor, adicione sua API Key na aba 'Secrets' (√≠cone de chave) √† esquerda.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFeXNi8_RaBe"
      },
      "source": [
        "## Parte 2: Self-Consistency (Autoconsist√™ncia)\n",
        "\n",
        "**O Conceito:** O Chain-of-Thought (CoT) √© √≥timo, mas e se o modelo cometer um erro no \"passo a passo\"? A resposta final estar√° errada.\n",
        "\n",
        "**A T√©cnica:**\n",
        "1.  Usamos um prompt CoT (ex: \"Pense passo a passo\").\n",
        "2.  Definimos a `temperature` do modelo como > 0 (ex: 0.7). Isso introduz aleatoriedade, fazendo com que o modelo gere *caminhos de racioc√≠nio diferentes* a cada vez.\n",
        "3.  Rodamos o mesmo prompt N vezes (ex: 5 ou 7 vezes).\n",
        "4.  Extra√≠mos a resposta final de cada uma das N sa√≠das.\n",
        "5.  Usamos um \"voto majorit√°rio\": a resposta final que apareceu mais vezes √© provavelmente a correta, pois diferentes caminhos de racioc√≠nio convergiram para ela.\n",
        "\n",
        "Vamos usar um problema de l√≥gica que pode ter caminhos confusos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OOBs3v4eRaBf",
        "outputId": "b67b5ef4-e3e6-44bd-c601-8deab5a51c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rodando o prompt 5 vezes (pode levar um momento)...\n",
            "\n",
            "--- Amostra 1 ---\n",
            "Esta √© a resolu√ß√£o passo a passo:\n",
            "\n",
            "**1. Muffins iniciais:**\n",
            "Havia **50** muffins.\n",
            "\n",
            "**2. Venda da manh√£ (Subtra√ß√£o):**\n",
            "Foram vendidos 15.\n",
            "$50 - 15 = 35$ muffins restantes.\n",
            "\n",
            "**3. Assar mais (Adi√ß√£o):**\n",
            "Eles assaram mais 30.\n",
            "$35 + 30 = 65$ muffins no total.\n",
            "\n",
            "**4. Venda final (Subtra√ß√£o):**\n",
            "Um cliente comprou 8.\n",
            "$65 - 8 = 57$ muffins restantes.\n",
            "\n",
            "**Resposta:** Restam **57** muffins no final do dia.\n",
            "--------------------\n",
            "\n",
            "--- Amostra 2 ---\n",
            "Este √© o c√°lculo passo a passo:\n",
            "\n",
            "1. **In√≠cio do Dia:** Havia **50** muffins.\n",
            "2. **Vendas da Manh√£:** 15 foram vendidos.\n",
            "   $$50 - 15 = 35$$\n",
            "3. **Assaram Mais:** 30 foram adicionados.\n",
            "   $$35 + 30 = 65$$\n",
            "4. **√öltima Venda:** Um cliente comprou 8.\n",
            "   $$65 - 8 = 57$$\n",
            "\n",
            "**Resposta:** Restam **57** muffins no final do dia.\n",
            "--------------------\n",
            "\n",
            "--- Amostra 3 ---\n",
            "Esta √© a resolu√ß√£o passo a passo:\n",
            "\n",
            "**1. Muffins Iniciais:**\n",
            "Havia **50** muffins.\n",
            "\n",
            "**2. Venda da Manh√£:**\n",
            "15 foram vendidos.\n",
            "$50 - 15 = 35$ muffins restantes.\n",
            "\n",
            "**3. Assaram Mais:**\n",
            "Eles assaram mais 30 muffins.\n",
            "$35 + 30 = 65$ muffins no total.\n",
            "\n",
            "**4. √öltima Venda:**\n",
            "Um cliente comprou 8.\n",
            "$65 - 8 = 57$ muffins restantes.\n",
            "\n",
            "**R: Restam 57 muffins no final do dia.**\n",
            "--------------------\n",
            "\n",
            "--- Amostra 4 ---\n",
            "Este √© o passo a passo para calcular quantos muffins restam:\n",
            "\n",
            "1. **In√≠cio do Dia:** Havia **50** muffins.\n",
            "\n",
            "2. **Venda da Manh√£:** 15 foram vendidos.\n",
            "   $$50 - 15 = 35 \\text{ muffins restantes}$$\n",
            "\n",
            "3. **Assaram Mais:** 30 muffins foram adicionados.\n",
            "   $$35 + 30 = 65 \\text{ muffins no total}$$\n",
            "\n",
            "4. **Venda da Tarde:** Um cliente comprou 8.\n",
            "   $$65 - 8 = 57 \\text{ muffins restantes}$$\n",
            "\n",
            "**Resposta:** Restam **57** muffins no final do dia.\n",
            "--------------------\n",
            "\n",
            "--- Amostra 5 ---\n",
            "Este √© o passo a passo da contagem:\n",
            "\n",
            "1. **In√≠cio do dia:** Havia **50** muffins.\n",
            "2. **Vendas da manh√£:** 15 foram vendidos.\n",
            "   $$50 - 15 = 35$$\n",
            "   *(Restam 35 muffins.)*\n",
            "3. **Assaram mais:** Eles assaram mais 30 muffins.\n",
            "   $$35 + 30 = 65$$\n",
            "   *(Agora h√° 65 muffins.)*\n",
            "4. **√öltima venda:** Um cliente comprou 8.\n",
            "   $$65 - 8 = 57$$\n",
            "\n",
            "**Resposta:** Restam **57** muffins no final do dia.\n",
            "--------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# O problema\n",
        "problema_sc = \"\"\"\n",
        "P: Numa cafeteria, havia 50 muffins. 15 foram vendidos pela manh√£.\n",
        "√Ä tarde, eles assaram mais 30 muffins.\n",
        "Depois, um cliente comprou 8.\n",
        "Quantos muffins restam no final do dia?\n",
        "\n",
        "R: Pense passo a passo.\n",
        "\"\"\"\n",
        "\n",
        "# 1. Configurar o modelo com temperature > 0 para gerar caminhos diversos\n",
        "generation_config = {\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_p\": 1.0,\n",
        "    \"top_k\": 32,\n",
        "}\n",
        "\n",
        "model_sc = genai.GenerativeModel(\n",
        "    model_name='gemini-2.5-flash-preview-09-2025',\n",
        "    generation_config=generation_config\n",
        ")\n",
        "\n",
        "# 2. Rodar o prompt N vezes\n",
        "num_amostras = 5\n",
        "respostas = []\n",
        "\n",
        "print(f\"Rodando o prompt {num_amostras} vezes (pode levar um momento)...\\n\")\n",
        "\n",
        "for i in range(num_amostras):\n",
        "    try:\n",
        "        response = model_sc.generate_content(problema_sc)\n",
        "        respostas.append(response.text)\n",
        "        print(f\"--- Amostra {i+1} ---\")\n",
        "        print(response.text)\n",
        "        print(\"--------------------\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro na Amostra {i+1}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGZcs2oIRaBf",
        "outputId": "19b9f4f0-a179-44be-c779-dcb987b2b124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respostas finais extra√≠das: ['57', '57', '57', '57', '57']\n",
            "\\Contagem de votos: Counter({'57': 5})\n",
            "\n",
            "üèÜ A resposta mais consistente √©: 57\n",
            "\n",
            "(A resposta correta √© 50 - 15 + 30 - 8 = 57)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:23: SyntaxWarning: invalid escape sequence '\\C'\n",
            "<>:23: SyntaxWarning: invalid escape sequence '\\C'\n",
            "/tmp/ipython-input-1655072112.py:23: SyntaxWarning: invalid escape sequence '\\C'\n",
            "  print(f\"\\Contagem de votos: {votos}\")\n"
          ]
        }
      ],
      "source": [
        "# 3. e 4. Extrair a resposta final e fazer a vota√ß√£o\n",
        "\n",
        "def extrair_resposta_final(texto):\n",
        "    \"\"\"Encontra o √∫ltimo n√∫mero no texto, que geralmente √© a resposta final.\"\"\"\n",
        "    # Tenta encontrar padr√µes como \"A resposta √© 57\" ou \"restam 57\"\n",
        "    numeros = re.findall(r'\\b(\\d+)\\b', texto)\n",
        "    if numeros:\n",
        "        # Retorna o √∫ltimo n√∫mero encontrado como string\n",
        "        return numeros[-1]\n",
        "    return None\n",
        "\n",
        "respostas_finais_extraidas = []\n",
        "for r in respostas:\n",
        "    ans = extrair_resposta_final(r)\n",
        "    if ans:\n",
        "        respostas_finais_extraidas.append(ans)\n",
        "\n",
        "print(f\"Respostas finais extra√≠das: {respostas_finais_extraidas}\")\n",
        "\n",
        "# 5. Voto majorit√°rio\n",
        "if respostas_finais_extraidas:\n",
        "    votos = Counter(respostas_finais_extraidas)\n",
        "    print(f\"\\Contagem de votos: {votos}\")\n",
        "\n",
        "    resposta_mais_comum, _ = votos.most_common(1)[0]\n",
        "    print(f\"\\nüèÜ A resposta mais consistente √©: {resposta_mais_comum}\")\n",
        "    print(\"\\n(A resposta correta √© 50 - 15 + 30 - 8 = 57)\")\n",
        "else:\n",
        "    print(\"Nenhuma resposta final p√¥de ser extra√≠da.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqxdAEKDRaBg"
      },
      "source": [
        "## Parte 3: ReAct (Reason + Act) via Tool Calling\n",
        "\n",
        "**O Conceito:** LLMs n√£o sabem de tudo (ex: clima em tempo real, pre√ßos de a√ß√µes, dados em um banco de dados). Eles precisam de \"ferramentas\". O padr√£o ReAct (Reason + Act) permite que o modelo:\n",
        "1.  **Reason (Pensar):** Determine qual informa√ß√£o est√° faltando.\n",
        "2.  **Act (Agir):** Decida chamar uma ferramenta (ex: `get_weather(location)`).\n",
        "3.  **Observe (Observar):** Receba o resultado da ferramenta.\n",
        "4.  Repita ou gere a resposta final.\n",
        "\n",
        "A API do Gemini implementa isso nativamente com **Tool Calling**.\n",
        "\n",
        "**Nosso Exemplo:** Vamos criar uma ferramenta falsa `get_preco_produto` e ver o modelo decidir us√°-la."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEfSiz3aRaBg",
        "outputId": "0df451e0-29c6-487f-f352-d99cd443fca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo ReAct/Tool-Calling configurado.\n"
          ]
        }
      ],
      "source": [
        "# 1. Definir nossa ferramenta Python (a fun√ß√£o que ser√° chamada)\n",
        "def get_preco_produto(nome_produto, loja=\"qualquer\"):\n",
        "    \"\"\"Retorna o pre√ßo de um produto em uma loja espec√≠fica.\"\"\"\n",
        "    print(f\"*** Chamada de API Externa: get_preco_produto(nome_produto={nome_produto}, loja={loja}) ***\")\n",
        "\n",
        "    # Simula√ß√£o de dados de um banco de dados de e-commerce\n",
        "    db_precos = {\n",
        "        \"laptop gamer\": {\"preco\": 7500.00, \"loja\": \"LojaTech\", \"estoque\": 10},\n",
        "        \"smartphone 5g\": {\"preco\": 3200.50, \"loja\": \"MobileCenter\", \"estoque\": 5},\n",
        "        \"teclado mec√¢nico\": {\"preco\": 450.00, \"loja\": \"Perif√©ricosTop\", \"estoque\": 20}\n",
        "    }\n",
        "\n",
        "    produto_key = nome_produto.lower()\n",
        "    if produto_key in db_precos:\n",
        "        info_produto = db_precos[produto_key]\n",
        "        info_produto[\"produto\"] = nome_produto\n",
        "    else:\n",
        "        info_produto = {\"produto\": nome_produto, \"preco\": None, \"erro\": \"Produto n√£o encontrado\"}\n",
        "\n",
        "    return json.dumps(info_produto)\n",
        "\n",
        "# 2. Definir o *esquema* da ferramenta para a API do Gemini\n",
        "# Isso diz ao modelo O QUE a ferramenta faz e QUAIS s√£o seus argumentos.\n",
        "ferramenta_preco_produto = {\n",
        "    \"name\": \"get_preco_produto\",\n",
        "    \"description\": \"Retorna o pre√ßo e o estoque de um produto em uma loja online.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"nome_produto\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"O nome do produto a ser buscado, ex: Laptop Gamer\"\n",
        "            },\n",
        "            \"loja\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"A loja espec√≠fica onde buscar (opcional)\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"nome_produto\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# 3. Criar o modelo e informar sobre a ferramenta\n",
        "model_react = genai.GenerativeModel(\n",
        "    model_name='gemini-2.5-flash-preview-09-2025',\n",
        "    tools=[ferramenta_preco_produto] # Informa ao modelo quais ferramentas ele pode usar\n",
        ")\n",
        "\n",
        "# Iniciar o chat (necess√°rio para o loop de tool calling)\n",
        "chat = model_react.start_chat(enable_automatic_function_calling=False) # Manual para vermos os passos\n",
        "\n",
        "print(\"Modelo ReAct/Tool-Calling configurado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "y67I2BjfRaBh",
        "outputId": "5ce8dbf5-d5ab-4d4f-9326-8ccb2cea0f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usu√°rio: Quanto custa o teclado mec√¢nico?\n",
            "\n",
            "ü§ñ (Reasoning) O modelo decidiu chamar uma ferramenta: get_preco_produto\n",
            "ü§ñ (Action) Argumentos da chamada: <proto.marshal.collections.maps.MapComposite object at 0x7aea9754a1e0>\n",
            "\n",
            "*** Chamada de API Externa: get_preco_produto(nome_produto=teclado mec√¢nico, loja=qualquer) ***\n",
            "üêç (Observation) Resultado da ferramenta: {\"preco\": 450.0, \"loja\": \"Perif\\u00e9ricosTop\", \"estoque\": 20, \"produto\": \"teclado mec\\u00e2nico\"}\n",
            "\n",
            "ü§ñ (Final Answer) Resposta final do modelo:\n",
            "O teclado mec√¢nico na loja Perif√©ricosTop custa R$ 450,00 e tem 20 unidades em estoque.\n"
          ]
        }
      ],
      "source": [
        "# 4. O Loop ReAct (Reason, Act, Observe)\n",
        "\n",
        "\n",
        "prompt_preco = \"Quanto custa o laptop gamer?\"\n",
        "#prompt_preco = \"Quanto custa o laptop ?\"\n",
        "#prompt_preco = \"Quanto custa o teclado mec√¢nico?\"\n",
        "\n",
        "print(f\"Usu√°rio: {prompt_preco}\\n\")\n",
        "\n",
        "# PASSO 1: O modelo PENSA (Reason) e decide AGIR (Act)\n",
        "response_passo1 = chat.send_message(prompt_preco)\n",
        "candidate = response_passo1.candidates[0]\n",
        "\n",
        "# Verificar se o modelo decidiu chamar nossa fun√ß√£o\n",
        "if candidate.content.parts[0].function_call:\n",
        "    # O modelo decidiu AGIR!\n",
        "    chamada_de_funcao = candidate.content.parts[0].function_call\n",
        "    print(f\"ü§ñ (Reasoning) O modelo decidiu chamar uma ferramenta: {chamada_de_funcao.name}\")\n",
        "    print(f\"ü§ñ (Action) Argumentos da chamada: {chamada_de_funcao.args}\\n\")\n",
        "\n",
        "    # PASSO 2: N√≥s executamos a ferramenta (a parte \"Act\" no nosso c√≥digo)\n",
        "    # e obtemos a \"Observa√ß√£o\"\n",
        "    nome_funcao = chamada_de_funcao.name\n",
        "    args = chamada_de_funcao.args\n",
        "\n",
        "    if nome_funcao == \"get_preco_produto\":\n",
        "        # Chama a fun√ß√£o Python real\n",
        "        resultado_funcao = get_preco_produto(nome_produto=args['nome_produto'],\n",
        "                                            loja=args.get('loja', 'qualquer'))\n",
        "\n",
        "        print(f\"üêç (Observation) Resultado da ferramenta: {resultado_funcao}\\n\")\n",
        "\n",
        "        # PASSO 3: Devolver a \"Observa√ß√£o\" para o modelo\n",
        "        # O modelo vai usar isso para gerar a resposta final.\n",
        "        response_passo2 = chat.send_message(content=resultado_funcao)\n",
        "\n",
        "\n",
        "        # O modelo agora gera a resposta final em linguagem natural\n",
        "        print(\"ü§ñ (Final Answer) Resposta final do modelo:\")\n",
        "        print(response_passo2.text)\n",
        "\n",
        "    else:\n",
        "        print(f\"Erro: Modelo tentou chamar uma fun√ß√£o desconhecida: {nome_funcao}\")\n",
        "\n",
        "else:\n",
        "    # O modelo n√£o precisou de ferramenta (ou falhou)\n",
        "    print(\"ü§ñ O modelo respondeu diretamente:\")\n",
        "    print(response_passo1.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.send_message()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trjM4hM-SaAE",
        "outputId": "6a1c1160-48ad-4e41-bbd8-de33b42b9fce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "google.generativeai.types.content_types.PartDict"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNivKyz4RaBh"
      },
      "source": [
        "## Conclus√£o\n",
        "\n",
        "Voc√™ viu duas t√©cnicas poderosas:\n",
        "\n",
        "1.  **Self-Consistency:** Aumenta a **confiabilidade** das respostas de racioc√≠nio complexo atrav√©s de amostragem e vota√ß√£o. (Usando `temperature` e `Counter`).\n",
        "2.  **ReAct / Tool Calling:** D√° ao LLM \"bra√ßos e pernas\" para interagir com o mundo exterior, permitindo que ele acesse dados em tempo real e execute a√ß√µes. (Usando a defini√ß√£o de `tools` e o loop `FunctionCall` / `FunctionResponse`)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}