{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsansao/teic-20231/blob/main/TEIC_Licao26_NMT_RNN_Benchmark_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "# ==============================================================================\n",
        "# SCRIPT PYTHON PARA GOOGLE COLAB\n",
        "#\n",
        "# TAREFA: Avaliação de Benchmark de Tradução Automática (NMT)\n",
        "# MODELO: RNN (Seq2Seq com LSTM)\n",
        "# MÉTRICA: BLEU Score\n",
        "#\n",
        "# INSTRUÇÕES:\n",
        "# 1. Abra um novo notebook no Google Colab (https://colab.research.google.com/)\n",
        "# 2. Copie e cole o conteúdo deste arquivo, célula por célula, e execute.\n",
        "#\n",
        "# As seções marcadas com \"# --- CÉLULA X ---\" devem ser coladas\n",
        "# em células separadas do Colab.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- CÉLULA 1: Instalação e Imports ---\n",
        "# (Instala o NLTK para a métrica BLEU e baixa os pacotes necessários)\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import urllib.request\n",
        "\n",
        "print(f\"TensorFlow Versão: {tf.__version__}\")\n",
        "\n",
        "# --- CÉLULA 2: Parâmetros e Download dos Dados ---\n",
        "# (Vamos usar um dataset Português-Inglês do http://www.manythings.org/anki/)\n",
        "\n",
        "#@title Parâmetros do Modelo e Dados\n",
        "NUM_EXEMPLOS = 30000  # Quantidade de frases para treinar. Reduza se o Colab for lento.\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "LATENT_DIM = 256  # Dimensão da camada LSTM\n",
        "\n",
        "# URL do dataset\n",
        "DATA_URL = \"https://github.com/jsansao/transformers_pt/raw/refs/heads/main/por-eng.zip\"\n",
        "DATA_PATH = \"por-eng.zip\"\n",
        "EXTRACT_PATH = \"por-eng\"\n",
        "\n",
        "# Download\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    urllib.request.urlretrieve(DATA_URL, DATA_PATH)\n",
        "    print(\"Dataset baixado.\")\n",
        "\n",
        "# Extração\n",
        "if not os.path.exists(EXTRACT_PATH):\n",
        "    os.makedirs(EXTRACT_PATH)\n",
        "with zipfile.ZipFile(DATA_PATH, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(EXTRACT_PATH)\n",
        "    print(\"Dataset extraído.\")\n",
        "\n",
        "DATA_FILE = os.path.join(EXTRACT_PATH, \"por.txt\")\n",
        "print(f\"Arquivo de dados: {DATA_FILE}\")\n",
        "\n",
        "\n",
        "# --- CÉLULA 3: Carregamento e Pré-processamento ---\n",
        "\n",
        "def preprocess_sentence(s):\n",
        "    \"\"\"\n",
        "    Limpa e prepara uma única sentença.\n",
        "    Adiciona tokens de [start] e [end].\n",
        "    \"\"\"\n",
        "    s = s.lower().strip()\n",
        "    # Adiciona espaço antes da pontuação para tokenização\n",
        "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "    s = re.sub(r'[\" \"]+', \" \", s)\n",
        "    # Substitui tudo exceto letras e pontuação básica\n",
        "    s = re.sub(r\"[^a-zA-Záéíóúâêîôûãõç?.!,¿]+\", \" \", s)\n",
        "    s = s.strip()\n",
        "    # Adiciona os tokens de início e fim\n",
        "    s = '[start] ' + s + ' [end]'\n",
        "    return s\n",
        "\n",
        "def load_data(path, num_examples):\n",
        "    \"\"\"\n",
        "    Lê o arquivo .txt e retorna pares de frases (Inglês, Português).\n",
        "    \"\"\"\n",
        "    # O arquivo está no formato: Inglês \\t Português \\t Atribuição\n",
        "    pairs = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f.readlines()[:num_examples]:\n",
        "            parts = line.split('\\t')\n",
        "            # Usamos o Português como ENTRADA (input) e Inglês como SAÍDA (target)\n",
        "            # Você pode inverter se preferir (target_lang, input_lang)\n",
        "            input_text = preprocess_sentence(parts[1])  # Português\n",
        "            target_text = preprocess_sentence(parts[0]) # Inglês\n",
        "            pairs.append((input_text, target_text))\n",
        "\n",
        "    return zip(*pairs)\n",
        "\n",
        "# Carrega os dados\n",
        "input_lang_raw, target_lang_raw = load_data(DATA_FILE, NUM_EXEMPLOS)\n",
        "print(\"Exemplo de dados brutos:\")\n",
        "print(f\"Input (PT): {input_lang_raw[0]}\")\n",
        "print(f\"Target (EN): {target_lang_raw[0]}\")\n",
        "\n",
        "\n",
        "# --- CÉLULA 4: Tokenização e Padding ---\n",
        "# (Transforma as palavras em números (índices) e preenche as sequências)\n",
        "\n",
        "def tokenize(lang):\n",
        "    \"\"\"\n",
        "    Cria um tokenizador Keras para um idioma.\n",
        "    \"\"\"\n",
        "    # oov_token='<unk>' -> Palavras desconhecidas serão mapeadas para <unk>\n",
        "    lang_tokenizer = Tokenizer(filters='', oov_token='<unk>')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    return lang_tokenizer\n",
        "\n",
        "def texts_to_sequences(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Converte textos em sequências de inteiros.\n",
        "    \"\"\"\n",
        "    return tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Tokeniza o idioma de ENTRADA (Português)\n",
        "input_tokenizer = tokenize(input_lang_raw)\n",
        "input_tensor = texts_to_sequences(input_tokenizer, input_lang_raw)\n",
        "input_tensor = pad_sequences(input_tensor, padding='post')\n",
        "\n",
        "# Tokeniza o idioma ALVO (Inglês)\n",
        "target_tokenizer = tokenize(target_lang_raw)\n",
        "target_tensor = texts_to_sequences(target_tokenizer, target_lang_raw)\n",
        "target_tensor = pad_sequences(target_tensor, padding='post')\n",
        "\n",
        "# Índices (Word -> ID) e Índices Inversos (ID -> Word)\n",
        "input_word_index = input_tokenizer.word_index\n",
        "input_index_word = input_tokenizer.index_word\n",
        "target_word_index = target_tokenizer.word_index\n",
        "target_index_word = target_tokenizer.index_word\n",
        "\n",
        "# Tamanho dos vocabulários (adiciona 1 para o token 0 de padding)\n",
        "VOCAB_SIZE_INPUT = len(input_word_index) + 1\n",
        "VOCAB_SIZE_TARGET = len(target_word_index) + 1\n",
        "\n",
        "# Tamanho máximo das sequências\n",
        "MAX_LEN_INPUT = input_tensor.shape[1]\n",
        "MAX_LEN_TARGET = target_tensor.shape[1]\n",
        "\n",
        "print(\"\\n--- Estatísticas dos Dados ---\")\n",
        "print(f\"Frases de entrada (PT): {len(input_tensor)}\")\n",
        "print(f\"Frases de saída (EN): {len(target_tensor)}\")\n",
        "print(f\"Tamanho Vocabulário PT: {VOCAB_SIZE_INPUT}\")\n",
        "print(f\"Tamanho Vocabulário EN: {VOCAB_SIZE_TARGET}\")\n",
        "print(f\"Tamanho Máx. Sequência PT: {MAX_LEN_INPUT}\")\n",
        "print(f\"Tamanho Máx. Sequência EN: {MAX_LEN_TARGET}\")\n",
        "\n",
        "# --- CÉLULA 5: Preparação dos Dados para Treinamento (Teacher Forcing) ---\n",
        "# (O modelo Seq2Seq precisa de 3 partes: entrada do encoder, entrada do decoder e saída do decoder)\n",
        "\n",
        "# 1. encoder_input_data: A frase em Português\n",
        "encoder_input_data = input_tensor\n",
        "\n",
        "# 2. decoder_input_data: A frase em Inglês, \"deslocada\"\n",
        "#    (Ex: \"[start] a cat sat [end]\" -> \"[start] a cat sat\")\n",
        "#    Isso é o \"teacher forcing\".\n",
        "decoder_input_data = target_tensor[:, :-1]\n",
        "\n",
        "# 3. decoder_target_data: A frase em Inglês, \"deslocada\" para o alvo\n",
        "#    (Ex: \"[start] a cat sat [end]\" -> \"a cat sat [end]\")\n",
        "#    O modelo tenta prever esta sequência.\n",
        "decoder_target_data = target_tensor[:, 1:]\n",
        "\n",
        "# Ajusta a forma do target para a loss function\n",
        "decoder_target_data = np.expand_dims(decoder_target_data, -1)\n",
        "\n",
        "print(\"\\n--- Formas dos Dados de Treinamento ---\")\n",
        "print(f\"Encoder Input (PT): {encoder_input_data.shape}\")\n",
        "print(f\"Decoder Input (EN - Teacher Forcing): {decoder_input_data.shape}\")\n",
        "print(f\"Decoder Target (EN - Labels): {decoder_target_data.shape}\")\n",
        "\n",
        "# Cria um tf.data.Dataset para eficiência\n",
        "BUFFER_SIZE = len(input_tensor)\n",
        "# Modify the dataset creation to yield inputs as a tuple in the correct order\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((encoder_input_data, decoder_input_data), decoder_target_data))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Separa 10% para validação/benchmark\n",
        "validation_size = int(0.1 * len(input_tensor))\n",
        "train_size = len(input_tensor) - validation_size\n",
        "\n",
        "# Pega os dados de validação ANTES de embaralhar o dataset\n",
        "# (Usaremos esses dados crus para o benchmark BLEU)\n",
        "validation_inputs_raw = input_lang_raw[train_size:]\n",
        "validation_targets_raw = target_lang_raw[train_size:]\n",
        "\n",
        "# Pega os tensores de validação\n",
        "validation_encoder_input = encoder_input_data[train_size:]\n",
        "# (Não precisamos dos outros para a inferência)\n",
        "\n",
        "print(f\"\\nTamanho Treino: {train_size}\")\n",
        "print(f\"Tamanho Validação (Benchmark): {validation_size}\")\n",
        "\n",
        "# --- CÉLULA 6: Definição do Modelo Seq2Seq (RNN/LSTM) ---\n",
        "# (Este é o modelo de *treinamento* que usa teacher forcing)\n",
        "\n",
        "# --- ENCODER (Codificador) ---\n",
        "# Recebe a sequência de entrada (PT)\n",
        "encoder_inputs = Input(shape=(MAX_LEN_INPUT,), name=\"encoder_input\")\n",
        "# Camada de Embedding (transforma IDs em vetores densos)\n",
        "encoder_embedding = Embedding(VOCAB_SIZE_INPUT, LATENT_DIM, name=\"encoder_embedding\")(encoder_inputs)\n",
        "# Camada LSTM (o \"RNN\")\n",
        "# return_state=True faz o LSTM retornar seu estado interno (o \"vetor de pensamento\")\n",
        "encoder_lstm = LSTM(LATENT_DIM, return_state=True, name=\"encoder_lstm\")\n",
        "# Não precisamos das saídas (outputs) do encoder, apenas dos estados\n",
        "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "# Agrupamos os estados (hidden state e cell state)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# --- DECODER (Decodificador) ---\n",
        "# Recebe a sequência alvo (EN) para o teacher forcing\n",
        "decoder_inputs = Input(shape=(MAX_LEN_TARGET - 1,), name=\"decoder_input\") # (Lembre-se que removemos o último token)\n",
        "# Camada de Embedding (compartilhar pesos é avançado, aqui usamos separado)\n",
        "decoder_embedding_layer = Embedding(VOCAB_SIZE_TARGET, LATENT_DIM, name=\"decoder_embedding\")\n",
        "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
        "# Camada LSTM do Decoder\n",
        "# return_sequences=True faz o LSTM retornar a sequência completa de saídas\n",
        "decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
        "# IMPORTANTE: Inicializamos o estado do Decoder com os estados finais do Encoder\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "# Camada Densa final para classificação sobre o vocabulário alvo\n",
        "decoder_dense = Dense(VOCAB_SIZE_TARGET, activation='softmax', name=\"decoder_dense\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# --- MODELO COMPLETO (Treinamento) ---\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compila o modelo\n",
        "# Usamos 'sparse_categorical_crossentropy' porque nossos alvos (decoder_target_data) são inteiros, não one-hot.\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# --- CÉLULA 7: Treinamento do Modelo ---\n",
        "# (Isso pode levar alguns minutos no Colab)\n",
        "\n",
        "print(\"\\nIniciando o treinamento...\")\n",
        "history = model.fit(dataset.take(train_size // BATCH_SIZE),\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=dataset.skip(train_size // BATCH_SIZE),\n",
        "                    verbose=1)\n",
        "print(\"Treinamento concluído.\")\n",
        "\n",
        "\n",
        "# --- CÉLULA 8: Definição dos Modelos de Inferência (Tradução) ---\n",
        "# (Para traduzir, precisamos de um modelo diferente que gere palavra por palavra)\n",
        "\n",
        "# O processo de tradução (inferência) é diferente do treinamento.\n",
        "# 1. Codificamos a frase de entrada (PT) e obtemos os estados [state_h, state_c].\n",
        "# 2. Alimentamos o decoder com o token [start] e os estados do encoder.\n",
        "# 3. O decoder prevê a próxima palavra (ex: \"a\").\n",
        "# 4. Alimentamos o decoder com a palavra \"a\" e os *novos* estados internos do decoder.\n",
        "# 5. Repetimos até o decoder prever [end] ou atingir o limite de tamanho.\n",
        "\n",
        "# --- 1. Modelo Encoder de Inferência ---\n",
        "# (Pega a entrada PT e retorna os estados)\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# --- 2. Modelo Decoder de Inferência ---\n",
        "# (Pega o token anterior e os estados anteriores, e retorna o novo token e novos estados)\n",
        "\n",
        "# Inputs de estado para o decoder\n",
        "decoder_state_h_input = Input(shape=(LATENT_DIM,))\n",
        "decoder_state_c_input = Input(shape=(LATENT_DIM,))\n",
        "decoder_states_inputs = [decoder_state_h_input, decoder_state_c_input]\n",
        "\n",
        "# Input para o token (agora com tamanho 1)\n",
        "decoder_single_input = Input(shape=(1,))\n",
        "\n",
        "# Reutilizamos as camadas de embedding e LSTM do modelo treinado\n",
        "decoder_embedding_inference = decoder_embedding_layer(decoder_single_input)\n",
        "decoder_outputs_inference, state_h_inf, state_c_inf = decoder_lstm(\n",
        "    decoder_embedding_inference, initial_state=decoder_states_inputs)\n",
        "\n",
        "decoder_states_inference = [state_h_inf, state_c_inf]\n",
        "decoder_outputs_inference = decoder_dense(decoder_outputs_inference)\n",
        "\n",
        "# O modelo final do decoder\n",
        "decoder_model = Model(\n",
        "    [decoder_single_input] + decoder_states_inputs,\n",
        "    [decoder_outputs_inference] + decoder_states_inference\n",
        ")\n",
        "\n",
        "print(\"\\nModelos de inferência (encoder e decoder) criados.\")\n",
        "encoder_model.summary()\n",
        "decoder_model.summary()\n",
        "\n",
        "\n",
        "# --- CÉLULA 9: Função de Tradução (Inferência) ---\n",
        "\n",
        "def translate_sentence(input_seq):\n",
        "    \"\"\"\n",
        "    Traduz uma única sequência (pré-processada) usando os modelos de inferência.\n",
        "    \"\"\"\n",
        "    # 1. Obtém os estados do encoder\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # 2. Prepara o input inicial do decoder (o token [start])\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = target_word_index['[start]']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = []\n",
        "\n",
        "    while not stop_condition:\n",
        "        # 3. Prediz a próxima palavra\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
        "\n",
        "        # 4. Obtém o ID da palavra prevista (a mais provável)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = target_index_word[sampled_token_index]\n",
        "\n",
        "        # 5. Adiciona a palavra à sentença (if not [end])\n",
        "        if sampled_word == '[end]':\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence.append(sampled_word)\n",
        "\n",
        "        # 6. Check for stop condition\n",
        "        if stop_condition or len(decoded_sentence) > MAX_LEN_TARGET:\n",
        "            stop_condition = True\n",
        "\n",
        "        # 7. Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # 8. Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return ' '.join(decoded_sentence)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Versão: 2.19.0\n",
            "Dataset extraído.\n",
            "Arquivo de dados: por-eng/por.txt\n",
            "Exemplo de dados brutos:\n",
            "Input (PT): [start] vai . [end]\n",
            "Target (EN): [start] go . [end]\n",
            "\n",
            "--- Estatísticas dos Dados ---\n",
            "Frases de entrada (PT): 30000\n",
            "Frases de saída (EN): 30000\n",
            "Tamanho Vocabulário PT: 7206\n",
            "Tamanho Vocabulário EN: 4168\n",
            "Tamanho Máx. Sequência PT: 14\n",
            "Tamanho Máx. Sequência EN: 10\n",
            "\n",
            "--- Formas dos Dados de Treinamento ---\n",
            "Encoder Input (PT): (30000, 14)\n",
            "Decoder Input (EN - Teacher Forcing): (30000, 9)\n",
            "Decoder Target (EN - Labels): (30000, 9, 1)\n",
            "\n",
            "Tamanho Treino: 27000\n",
            "Tamanho Validação (Benchmark): 3000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,844,736\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │  \u001b[38;5;34m1,067,008\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m525,312\u001b[0m │ encoder_embeddin… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ decoder_embeddin… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m4168\u001b[0m)   │  \u001b[38;5;34m1,071,176\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,844,736</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,067,008</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ encoder_embeddin… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ decoder_embeddin… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4168</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,071,176</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,033,544\u001b[0m (19.20 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,033,544</span> (19.20 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,033,544\u001b[0m (19.20 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,033,544</span> (19.20 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando o treinamento...\n",
            "Epoch 1/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - accuracy: 0.5261 - loss: 3.2781 - val_accuracy: 0.6647 - val_loss: 2.0079\n",
            "Epoch 2/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.6716 - loss: 1.9339 - val_accuracy: 0.6893 - val_loss: 1.7266\n",
            "Epoch 3/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.6990 - loss: 1.6789 - val_accuracy: 0.7347 - val_loss: 1.4540\n",
            "Epoch 4/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.7451 - loss: 1.4004 - val_accuracy: 0.7735 - val_loss: 1.2153\n",
            "Epoch 5/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.7752 - loss: 1.1913 - val_accuracy: 0.7969 - val_loss: 1.0461\n",
            "Epoch 6/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.7997 - loss: 1.0311 - val_accuracy: 0.8189 - val_loss: 0.8988\n",
            "Epoch 7/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8216 - loss: 0.8847 - val_accuracy: 0.8379 - val_loss: 0.7847\n",
            "Epoch 8/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8417 - loss: 0.7595 - val_accuracy: 0.8621 - val_loss: 0.6457\n",
            "Epoch 9/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8633 - loss: 0.6416 - val_accuracy: 0.8824 - val_loss: 0.5448\n",
            "Epoch 10/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.8828 - loss: 0.5370 - val_accuracy: 0.9048 - val_loss: 0.4351\n",
            "Epoch 11/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9011 - loss: 0.4463 - val_accuracy: 0.9195 - val_loss: 0.3705\n",
            "Epoch 12/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9197 - loss: 0.3673 - val_accuracy: 0.9334 - val_loss: 0.3103\n",
            "Epoch 13/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9323 - loss: 0.3071 - val_accuracy: 0.9456 - val_loss: 0.2530\n",
            "Epoch 14/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9445 - loss: 0.2536 - val_accuracy: 0.9553 - val_loss: 0.2123\n",
            "Epoch 15/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9525 - loss: 0.2150 - val_accuracy: 0.9622 - val_loss: 0.1735\n",
            "Epoch 16/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9596 - loss: 0.1803 - val_accuracy: 0.9655 - val_loss: 0.1516\n",
            "Epoch 17/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9653 - loss: 0.1530 - val_accuracy: 0.9723 - val_loss: 0.1257\n",
            "Epoch 18/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9704 - loss: 0.1311 - val_accuracy: 0.9761 - val_loss: 0.1076\n",
            "Epoch 19/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9731 - loss: 0.1138 - val_accuracy: 0.9766 - val_loss: 0.1004\n",
            "Epoch 20/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9755 - loss: 0.1012 - val_accuracy: 0.9801 - val_loss: 0.0829\n",
            "Epoch 21/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9779 - loss: 0.0880 - val_accuracy: 0.9821 - val_loss: 0.0731\n",
            "Epoch 22/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9787 - loss: 0.0803 - val_accuracy: 0.9816 - val_loss: 0.0702\n",
            "Epoch 23/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9805 - loss: 0.0736 - val_accuracy: 0.9818 - val_loss: 0.0672\n",
            "Epoch 24/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9814 - loss: 0.0677 - val_accuracy: 0.9849 - val_loss: 0.0561\n",
            "Epoch 25/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9818 - loss: 0.0619 - val_accuracy: 0.9832 - val_loss: 0.0595\n",
            "Epoch 26/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9833 - loss: 0.0570 - val_accuracy: 0.9842 - val_loss: 0.0558\n",
            "Epoch 27/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9838 - loss: 0.0547 - val_accuracy: 0.9846 - val_loss: 0.0517\n",
            "Epoch 28/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9835 - loss: 0.0526 - val_accuracy: 0.9851 - val_loss: 0.0490\n",
            "Epoch 29/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9842 - loss: 0.0493 - val_accuracy: 0.9861 - val_loss: 0.0445\n",
            "Epoch 30/30\n",
            "\u001b[1m421/421\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9835 - loss: 0.0498 - val_accuracy: 0.9867 - val_loss: 0.0417\n",
            "Treinamento concluído.\n",
            "\n",
            "Modelos de inferência (encoder e decoder) criados.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_embedding (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │     \u001b[38;5;34m1,844,736\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │       \u001b[38;5;34m525,312\u001b[0m │\n",
              "│                                 │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]     │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,844,736</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
              "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]     │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,370,048\u001b[0m (9.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,370,048</span> (9.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,370,048\u001b[0m (9.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,370,048</span> (9.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │  \u001b[38;5;34m1,067,008\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m525,312\u001b[0m │ decoder_embeddin… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4168\u001b[0m)   │  \u001b[38;5;34m1,071,176\u001b[0m │ decoder_lstm[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,067,008</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ decoder_embeddin… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4168</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,071,176</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,663,496\u001b[0m (10.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,663,496</span> (10.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,663,496\u001b[0m (10.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,663,496</span> (10.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tWi9dved7fFJ",
        "outputId": "e60b02dd-c285-49a2-de58-0411d5a5699d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CÉLULA 10: Avaliação do Benchmark (BLEU Score) ---\n",
        "# (Calculamos a métrica BLEU no conjunto de validação)\n",
        "\n",
        "print(\"\\n--- Iniciando Avaliação do Benchmark (BLEU Score) ---\")\n",
        "\n",
        "references = [] # The real translations (gold)\n",
        "predictions = [] # The model's translations (hypotheses)\n",
        "\n",
        "# Iterate over the validation set (benchmark)\n",
        "# (Use .iloc to ensure we are getting the correct data)\n",
        "\n",
        "for i in range(len(validation_encoder_input)):\n",
        "\n",
        "    input_seq = validation_encoder_input[i:i+1] # Get the input sequence\n",
        "\n",
        "    # Translate the sequence\n",
        "    predicted_sentence = translate_sentence(input_seq)\n",
        "\n",
        "    # Get the real target sentence (reference)\n",
        "    # Remove [start] and [end] from the reference for BLEU calculation\n",
        "    reference_sentence_raw = validation_targets_raw[i]\n",
        "    reference_sentence = reference_sentence_raw.replace('[start] ', '').replace(' [end]', '')\n",
        "\n",
        "    # NLTK expects lists of words\n",
        "    references.append([reference_sentence.split()]) # BLEU can have multiple refs, hence [[]]\n",
        "    predictions.append(predicted_sentence.split())\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Evaluating {i}/{len(validation_encoder_input)}...\")\n",
        "        print(f\"  Input (PT): {validation_inputs_raw[i]}\")\n",
        "        print(f\"  Real (EN): {reference_sentence}\")\n",
        "        print(f\"  Prev (EN): {predicted_sentence}\")\n",
        "\n",
        "# --- FINAL BLEU CALCULATION ---\n",
        "# BLEU-1 (unigrams)\n",
        "bleu_1 = corpus_bleu(references, predictions, weights=(1, 0, 0, 0))\n",
        "# BLEU-4 (standard)\n",
        "bleu_4 = corpus_bleu(references, predictions, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "print(\"\\n--- BENCHMARK RESULT ---\")\n",
        "print(f\"Total sentences evaluated: {len(references)}\")\n",
        "print(f\"BLEU-1 Score (Unigrams): {bleu_1 * 100:.2f}\")\n",
        "print(f\"BLEU-4 Score (Corpus): {bleu_4 * 100:.2f}\")\n"
      ],
      "metadata": {
        "id": "swURhuszHmDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d01c4637-6135-4272-c66e-0c8ceba07510"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Iniciando Avaliação do Benchmark (BLEU Score) ---\n",
            "Evaluating 0/3000...\n",
            "  Input (PT): [start] tu és inflexível . [end]\n",
            "  Real (EN): you re inflexible .\n",
            "  Prev (EN): you re inflexible .\n",
            "Evaluating 100/3000...\n",
            "  Input (PT): [start] todos nós conhecemos tom . [end]\n",
            "  Real (EN): all of us know tom .\n",
            "  Prev (EN): we all know tom .\n",
            "Evaluating 200/3000...\n",
            "  Input (PT): [start] você é o médico ? [end]\n",
            "  Real (EN): are you the doctor ?\n",
            "  Prev (EN): are you the doctor ?\n",
            "Evaluating 300/3000...\n",
            "  Input (PT): [start] cheque aquele carro . [end]\n",
            "  Real (EN): check out that car .\n",
            "  Prev (EN): check out that car .\n",
            "Evaluating 400/3000...\n",
            "  Input (PT): [start] você tem o bastante ? [end]\n",
            "  Real (EN): do you have enough ?\n",
            "  Prev (EN): do you have enough ?\n",
            "Evaluating 500/3000...\n",
            "  Input (PT): [start] será que o gosto é bom ? [end]\n",
            "  Real (EN): does it taste good ?\n",
            "  Prev (EN): does it taste good ?\n",
            "Evaluating 600/3000...\n",
            "  Input (PT): [start] não escreva a tinta . [end]\n",
            "  Real (EN): don t write in ink .\n",
            "  Prev (EN): don t write in ink .\n",
            "Evaluating 700/3000...\n",
            "  Input (PT): [start] vai embora daqui . [end]\n",
            "  Real (EN): get away from here .\n",
            "  Prev (EN): get away from here .\n",
            "Evaluating 800/3000...\n",
            "  Input (PT): [start] ela sabe dirigir carro . [end]\n",
            "  Real (EN): he can drive a car .\n",
            "  Prev (EN): he can drive a car .\n",
            "Evaluating 900/3000...\n",
            "  Input (PT): [start] ele mora aqui perto . [end]\n",
            "  Real (EN): he lives near here .\n",
            "  Prev (EN): he lives near here .\n",
            "Evaluating 1000/3000...\n",
            "  Input (PT): [start] aqui está seu troco . [end]\n",
            "  Real (EN): here s your change .\n",
            "  Prev (EN): here s your change .\n",
            "Evaluating 1100/3000...\n",
            "  Input (PT): [start] já o fiz . [end]\n",
            "  Real (EN): i already did that .\n",
            "  Prev (EN): i already did it .\n",
            "Evaluating 1200/3000...\n",
            "  Input (PT): [start] posso esperá lo . [end]\n",
            "  Real (EN): i can wait for you .\n",
            "  Prev (EN): i can wait for you .\n",
            "Evaluating 1300/3000...\n",
            "  Input (PT): [start] eu não queria isso . [end]\n",
            "  Real (EN): i didn t want this .\n",
            "  Prev (EN): i didn t want this .\n",
            "Evaluating 1400/3000...\n",
            "  Input (PT): [start] eu sigo as regras . [end]\n",
            "  Real (EN): i follow the rules .\n",
            "  Prev (EN): i follow the rules .\n",
            "Evaluating 1500/3000...\n",
            "  Input (PT): [start] eu não tenho religião . [end]\n",
            "  Real (EN): i have no religion .\n",
            "  Prev (EN): i have no religion .\n",
            "Evaluating 1600/3000...\n",
            "  Input (PT): [start] eu beijei tom de novo . [end]\n",
            "  Real (EN): i kissed tom again .\n",
            "  Prev (EN): i kissed tom again .\n",
            "Evaluating 1700/3000...\n",
            "  Input (PT): [start] eu gosto de matemática . [end]\n",
            "  Real (EN): i like mathematics .\n",
            "  Prev (EN): i like math .\n",
            "Evaluating 1800/3000...\n",
            "  Input (PT): [start] eu me mudei mês passado . [end]\n",
            "  Real (EN): i moved last month .\n",
            "  Prev (EN): i moved last month .\n",
            "Evaluating 1900/3000...\n",
            "  Input (PT): [start] eu me lembro também . [end]\n",
            "  Real (EN): i remember it , too .\n",
            "  Prev (EN): i remember it , too .\n",
            "Evaluating 2000/3000...\n",
            "  Input (PT): [start] eu acho que você vai ganhar . [end]\n",
            "  Real (EN): i think you ll win .\n",
            "  Prev (EN): i think you ll win .\n",
            "Evaluating 2100/3000...\n",
            "  Input (PT): [start] quero que você cante . [end]\n",
            "  Real (EN): i want you to sing .\n",
            "  Prev (EN): i want you to sing .\n",
            "Evaluating 2200/3000...\n",
            "  Input (PT): [start] não te assistirei . [end]\n",
            "  Real (EN): i won t assist you .\n",
            "  Prev (EN): i won t assist you .\n",
            "Evaluating 2300/3000...\n",
            "  Input (PT): [start] irei parar mais tarde . [end]\n",
            "  Real (EN): i ll stop by later .\n",
            "  Prev (EN): i ll go by food .\n",
            "Evaluating 2400/3000...\n",
            "  Input (PT): [start] estou melhor . [end]\n",
            "  Real (EN): i m feeling better .\n",
            "  Prev (EN): i m better .\n",
            "Evaluating 2500/3000...\n",
            "  Input (PT): [start] eu não estou com raiva do tom . [end]\n",
            "  Real (EN): i m not mad at tom .\n",
            "  Prev (EN): i m not mad at tom .\n",
            "Evaluating 2600/3000...\n",
            "  Input (PT): [start] estou muito feliz agora . [end]\n",
            "  Real (EN): i m very happy now .\n",
            "  Prev (EN): i m very happy now .\n",
            "Evaluating 2700/3000...\n",
            "  Input (PT): [start] ela também vem ? [end]\n",
            "  Real (EN): is she coming , too ?\n",
            "  Prev (EN): is she coming , too ?\n",
            "Evaluating 2800/3000...\n",
            "  Input (PT): [start] não é nada fácil . [end]\n",
            "  Real (EN): it isn t very easy .\n",
            "  Prev (EN): it isn t very easy .\n",
            "Evaluating 2900/3000...\n",
            "  Input (PT): [start] ficará tudo bem . [end]\n",
            "  Real (EN): it ll be all right .\n",
            "  Prev (EN): it ll be all right .\n",
            "\n",
            "--- BENCHMARK RESULT ---\n",
            "Total sentences evaluated: 3000\n",
            "BLEU-1 Score (Unigrams): 97.70\n",
            "BLEU-4 Score (Corpus): 94.86\n",
            "\n",
            "--- Manual Test ---\n",
            "Input Sentence (PT): [start] todos entraram em pânico . [end]\n",
            "Real Translation (EN):    everybody panicked .\n",
            "Model Translation (EN): everybody panicked .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- Manual Test ---\")\n",
        "# Pick a random sentence from the validation set\n",
        "idx = np.random.randint(0, len(validation_encoder_input))\n",
        "\n",
        "input_seq_test = validation_encoder_input[idx:idx+1]\n",
        "input_raw_test = validation_inputs_raw[idx]\n",
        "target_raw_test = validation_targets_raw[idx].replace('[start] ', '').replace(' [end]', '')\n",
        "predicted_test = translate_sentence(input_seq_test)\n",
        "\n",
        "print(f\"Input Sentence (PT): {input_raw_test}\")\n",
        "print(f\"Real Translation (EN):    {target_raw_test}\")\n",
        "print(f\"Model Translation (EN): {predicted_test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spj8MhkePPQ7",
        "outputId": "f792ef52-b7d8-48bb-a3fc-ea23c4edf98b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Manual Test ---\n",
            "Input Sentence (PT): [start] não comprem essa . [end]\n",
            "Real Translation (EN):    don t buy that one .\n",
            "Model Translation (EN): don t buy that one .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CÉLULA 11: Função para Traduzir uma String Arbitrária ---\n",
        "# (Usa as funções de pré-processamento e inferência definidas anteriormente)\n",
        "\n",
        "def translate_string(input_string):\n",
        "    \"\"\"\n",
        "    Traduz uma string de texto em Português para Inglês.\n",
        "    \"\"\"\n",
        "    # 1. Pré-processa a string de entrada\n",
        "    processed_string = preprocess_sentence(input_string)\n",
        "\n",
        "    # 2. Converte a string processada para uma sequência de IDs\n",
        "    # O tokenizer espera uma lista de strings\n",
        "    input_seq = texts_to_sequences(input_tokenizer, [processed_string])\n",
        "    # Garante o padding correto\n",
        "    input_seq = pad_sequences(input_seq, maxlen=MAX_LEN_INPUT, padding='post')\n",
        "\n",
        "    # 3. Chama a função de inferência para traduzir a sequência\n",
        "    translated_sentence = translate_sentence(input_seq)\n",
        "\n",
        "    return translated_sentence\n",
        "\n",
        "# --- Exemplo de uso ---\n",
        "test_string = \"Olá, como você está?\"\n",
        "translated_string = translate_string(test_string)\n",
        "print(f\"\\nOriginal (PT): {test_string}\")\n",
        "print(f\"Traduzido (EN): {translated_string}\")\n",
        "\n",
        "test_string_2 = \"Amostra grátis\"\n",
        "translated_string_2 = translate_string(test_string_2)\n",
        "print(f\"\\nOriginal (PT): {test_string_2}\")\n",
        "print(f\"Traduzido (EN): {translated_string_2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8B1IbgzaXQE",
        "outputId": "f85d003d-f65a-4130-ed65-5c33b22ec5a5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original (PT): Olá, como você está?\n",
            "Traduzido (EN): hello , how are you ?\n",
            "\n",
            "Original (PT): Amostra grátis\n",
            "Traduzido (EN): attendance is free .\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}