{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsansao/teic-20231/blob/main/TEIC_Licao23_generate_Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejOL-Aq-QkNy"
      },
      "source": [
        "# Geração de Texto Literário com LSTM e Word2Vec Pré-treinado\n",
        "\n",
        "Este notebook demonstra como treinar uma rede LSTM (Long Short-Term Memory) para gerar texto literário em inglês.\n",
        "\n",
        "O principal diferencial aqui é o uso de embeddings de palavras pré-treinados, especificamente o **Word2Vec** (treinado no Google News), para inicializar a camada de Embedding da nossa rede. Isso permite que o modelo já comece com um \"entendimento\" semântico das palavras, acelerando o treinamento e melhorando a qualidade do texto gerado, especialmente com datasets menores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5spCl3kQkN7"
      },
      "source": [
        "## 1. Instalação e Imports\n",
        "\n",
        "Primeiro, importamos as bibliotecas necessárias. O `gensim` será usado para carregar o modelo Word2Vec."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4Xo47KBRUqX",
        "outputId": "e2c01f6b-f0d2-49a6-dee1-38aa99c565a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-Cgupe2qQkOA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import gensim.downloader\n",
        "import numpy as np\n",
        "import string\n",
        "import requests # Para baixar o texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvcBVrU4QkOE"
      },
      "source": [
        "## 2. Download dos Dados\n",
        "\n",
        "### a) Corpus de Texto Literário\n",
        "\n",
        "Vamos usar \"Alice's Adventures in Wonderland\" de Lewis Carroll, disponível no Projeto Gutenberg."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-04RFUsgQkOH",
        "outputId": "7a2c3fe3-a54d-4642-eb9c-0e28898d84f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto baixado. Tamanho: 144696 caracteres.\n",
            "--- Início do Texto ---\n",
            " Mock Turtle’s Story\n",
            " CHAPTER X.     The Lobster Quadrille\n",
            " CHAPTER XI.    Who Stole the Tarts?\n",
            " CHAPTER XII.   Alice’s Evidence\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I.\n",
            "Down the Rabbit-Hole\n",
            "\n",
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothing to do: once or twice she had peeped into\n",
            "the book her sister was reading, but it had no pictures or\n",
            "conversations in it, “and what is the use of a book,” thought Alice\n",
            "“without pictures or conversations?”\n",
            "\n",
            "So she was considering in her\n",
            "-----------------------\n"
          ]
        }
      ],
      "source": [
        "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
        "response = requests.get(url)\n",
        "data = response.text\n",
        "\n",
        "# Salvar uma cópia local (opcional)\n",
        "with open(\"alice_wonderland.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(data)\n",
        "\n",
        "print(f\"Texto baixado. Tamanho: {len(data)} caracteres.\")\n",
        "print(\"--- Início do Texto ---\")\n",
        "print(data[500:1000])\n",
        "print(\"-----------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQeYZ-xuQkOM"
      },
      "source": [
        "### b) Modelo Word2Vec Pré-treinado\n",
        "\n",
        "Vamos usar o `gensim.downloader` para carregar o modelo Word2Vec clássico treinado no Google News.\n",
        "\n",
        "**AVISO:** Este modelo é grande (1.5GB) e o download/carregamento pode levar alguns minutos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSm79OcDQkOO",
        "outputId": "9d4fa3bd-0bc1-4b14-ed02-a23ed8d2d2e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando modelo Word2Vec (word2vec-google-news-300)...\n",
            "Isso pode levar alguns minutos...\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "\n",
            "Modelo Word2Vec carregado com sucesso!\n",
            "Dimensão do Embedding: 300\n",
            "\n",
            "Teste de similaridade (king - man + woman):\n",
            "Resultado: [('queen', 0.7118193507194519)]\n"
          ]
        }
      ],
      "source": [
        "# Nome do modelo: 'word2vec-google-news-300'\n",
        "print(\"Carregando modelo Word2Vec (word2vec-google-news-300)...\")\n",
        "print(\"Isso pode levar alguns minutos...\")\n",
        "word2vec_model = gensim.downloader.load('word2vec-google-news-300')\n",
        "\n",
        "embedding_dim = word2vec_model.vector_size\n",
        "print(f\"\\nModelo Word2Vec carregado com sucesso!\")\n",
        "print(f\"Dimensão do Embedding: {embedding_dim}\")\n",
        "\n",
        "# Teste rápido para ver se funciona\n",
        "try:\n",
        "    print(\"\\nTeste de similaridade (king - man + woman):\")\n",
        "    similar = word2vec_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
        "    print(f\"Resultado: {similar}\")\n",
        "except KeyError as e:\n",
        "    print(f\"Erro no teste de similaridade: {e}. (Alguma palavra pode estar faltando)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Ui8SDwQkOR"
      },
      "source": [
        "## 3. Pré-processamento do Texto\n",
        "\n",
        "Agora, vamos limpar o texto e transformá-lo em sequências que a LSTM possa entender."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru5u8Kj7QkOV",
        "outputId": "53ee207f-ce5f-4571-d15f-bf2a83bb8ffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Marcadores do Gutenberg não encontrados. Usando o texto completo.\n",
            "Total de tokens no corpus limpo: 26476\n",
            "Exemplo de tokens: ['tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', 'and']\n"
          ]
        }
      ],
      "source": [
        "def clean_text(text):\n",
        "    # Remove cabeçalhos/rodapés do Gutenberg (aproximação)\n",
        "    start_marker = \"*** START OF THIS PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***\"\n",
        "    end_marker = \"*** END OF THIS PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***\"\n",
        "    try:\n",
        "        start_index = text.index(start_marker) + len(start_marker)\n",
        "        end_index = text.index(end_marker)\n",
        "        text = text[start_index:end_index]\n",
        "    except ValueError:\n",
        "        print(\"Marcadores do Gutenberg não encontrados. Usando o texto completo.\")\n",
        "\n",
        "    # Converte para minúsculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove pontuação e números\n",
        "    translator = str.maketrans('', '', string.punctuation + string.digits)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    # Remove quebras de linha e espaços extras\n",
        "    text = text.replace('\\r\\n', ' ').replace('\\n', ' ')\n",
        "    text = ' '.join(text.split()) # Remove espaços múltiplos\n",
        "    return text\n",
        "\n",
        "cleaned_data = clean_text(data)\n",
        "tokens = cleaned_data.split()\n",
        "\n",
        "print(f\"Total de tokens no corpus limpo: {len(tokens)}\")\n",
        "print(f\"Exemplo de tokens: {tokens[100:110]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-I6TFnXQkOc",
        "outputId": "34c6cfd3-cb31-4a8f-9d2b-9b94b3e5cbde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de sequências de treinamento: 26426\n",
            "\n",
            "Exemplo de sequência (como texto):\n",
            "start of the project gutenberg ebook illustration alice’s adventures in wonderland by lewis carroll the millennium fulcrum edition contents chapter i down the rabbithole chapter ii the pool of tears chapter iii a caucusrace and a long tale chapter iv the rabbit sends in a little bill chapter v advice from\n"
          ]
        }
      ],
      "source": [
        "# Criar sequências de N palavras para prever a (N+1)-ésima\n",
        "# Ex: [palavra1, palavra2, ..., palavra50] -> [palavra51]\n",
        "\n",
        "seq_length = 50 # Usar 50 palavras anteriores para prever a próxima\n",
        "sequences = []\n",
        "\n",
        "for i in range(seq_length, len(tokens)):\n",
        "    # Pega 50 palavras + 1 palavra alvo\n",
        "    seq = tokens[i-seq_length:i+1]\n",
        "    sequences.append(' '.join(seq))\n",
        "\n",
        "print(f\"Total de sequências de treinamento: {len(sequences)}\")\n",
        "print(f\"\\nExemplo de sequência (como texto):\\n{sequences[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioCWFDJoQkOf",
        "outputId": "4423e16a-61b4-4372-88e3-9f2f904e3e80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho total do vocabulário (do nosso corpus): 3489\n"
          ]
        }
      ],
      "source": [
        "# Tokenização com o Keras Tokenizer\n",
        "# Isso criará o mapeamento palavra -> índice\n",
        "tokenizer = Tokenizer(oov_token='<oov>') # <oov> para palavras fora do vocabulário\n",
        "tokenizer.fit_on_texts(sequences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "total_words = len(word_index) + 1 # +1 para o índice 0 (que é reservado)\n",
        "\n",
        "print(f\"Tamanho total do vocabulário (do nosso corpus): {total_words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irmGdNkQQkOj",
        "outputId": "3458eda9-607c-44cf-cf07-3c9b769adffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de X (input): (26426, 50)\n",
            "Shape de y (target): (26426,)\n",
            "Tamanho máximo da sequência de entrada: 50\n"
          ]
        }
      ],
      "source": [
        "# Converter as sequências de texto em sequências de inteiros (índices)\n",
        "input_sequences_int = tokenizer.texts_to_sequences(sequences)\n",
        "input_sequences = np.array(input_sequences_int)\n",
        "\n",
        "# Separar X (features) e y (label)\n",
        "X = input_sequences[:, :-1]  # Todas as palavras, exceto a última\n",
        "y = input_sequences[:, -1]   # Apenas a última palavra\n",
        "\n",
        "# Nosso 'y' são índices inteiros. Para usar 'categorical_crossentropy',\n",
        "# precisaríamos de one-hot encoding.\n",
        "# Alternativa: Usar 'sparse_categorical_crossentropy' que aceita 'y' como índices.\n",
        "# Vamos usar sparse_categorical_crossentropy.\n",
        "\n",
        "max_sequence_len = X.shape[1] # Deve ser igual a 'seq_length'\n",
        "\n",
        "print(f\"Shape de X (input): {X.shape}\")\n",
        "print(f\"Shape de y (target): {y.shape}\")\n",
        "print(f\"Tamanho máximo da sequência de entrada: {max_sequence_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBnGEJiKQkOl"
      },
      "source": [
        "## 4. Criação da Matriz de Embedding\n",
        "\n",
        "Este é o passo crucial. Vamos criar uma matriz onde a linha `i` contém o vetor Word2Vec para a palavra de índice `i` do *nosso* vocabulário (do tokenizer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58jpgMi5QkOm",
        "outputId": "b0f3afb0-e7c6-4319-bebf-29791e368389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de embedding criada com shape: (3489, 300)\n",
            "Palavras do nosso vocabulário encontradas no Word2Vec: 2331\n",
            "Palavras não encontradas (OOV) no Word2Vec: 1157\n"
          ]
        }
      ],
      "source": [
        "# Inicializar a matriz de embedding com zeros\n",
        "# Shape: (total_words, embedding_dim)\n",
        "embedding_matrix = np.zeros((total_words, embedding_dim))\n",
        "\n",
        "words_in_vocab = 0\n",
        "words_not_in_vocab = 0\n",
        "\n",
        "# Iterar sobre o nosso vocabulário (word_index do Keras Tokenizer)\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model:\n",
        "        # Se a palavra existe no Word2Vec, pegamos o vetor\n",
        "        embedding_matrix[i] = word2vec_model[word]\n",
        "        words_in_vocab += 1\n",
        "    else:\n",
        "        # Se não, o vetor permanece como zeros (ou poderia ser aleatório)\n",
        "        words_not_in_vocab += 1\n",
        "\n",
        "print(f\"Matriz de embedding criada com shape: {embedding_matrix.shape}\")\n",
        "print(f\"Palavras do nosso vocabulário encontradas no Word2Vec: {words_in_vocab}\")\n",
        "print(f\"Palavras não encontradas (OOV) no Word2Vec: {words_not_in_vocab}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXo7W_rcQkOn"
      },
      "source": [
        "## 5. Construção do Modelo LSTM\n",
        "\n",
        "Definimos a arquitetura da rede. A camada `Embedding` será inicializada com a `embedding_matrix` que acabamos de criar e será **congelada** (`trainable=False`) para que o treinamento não altere os pesos do Word2Vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "m0hFxt9OQkOo",
        "outputId": "16964d8e-5b5c-449b-d3f8-6f9e29bbf5b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │     \u001b[38;5;34m1,046,700\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,046,700</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,046,700\u001b[0m (3.99 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,046,700</span> (3.99 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,046,700\u001b[0m (3.99 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,046,700</span> (3.99 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Camada de Embedding\n",
        "model.add(Embedding(\n",
        "    input_dim=total_words,       # Tamanho do vocabulário\n",
        "    output_dim=embedding_dim,  # Dimensão do Word2Vec (300)\n",
        "    weights=[embedding_matrix],  # Pesos pré-treinados\n",
        "    input_length=max_sequence_len, # Tamanho da sequência de entrada (50)\n",
        "    trainable=False              # Congela os pesos do embedding\n",
        "))\n",
        "\n",
        "# Camadas LSTM\n",
        "# return_sequences=True é necessário se a próxima camada for outra LSTM\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100)) # Removed return_sequences=True\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Camada de Saída\n",
        "# A saída é uma probabilidade para cada palavra do vocabulário\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "# Compilação\n",
        "# Usamos 'sparse_categorical_crossentropy' pois nosso 'y' são índices inteiros\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_15w3nLQkOq"
      },
      "source": [
        "## 6. Treinamento do Modelo\n",
        "\n",
        "Vamos treinar o modelo. Com um corpus pequeno como \"Alice\", 20-30 épocas podem ser suficientes para um resultado razoável. Para um modelo de produção, seriam necessárias muito mais épocas e dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKoV5EpyQkOr",
        "outputId": "64823c9a-cfea-4edb-a1f8-394f8b3b085c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando o treinamento...\n",
            "Epoch 1/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5415 - loss: 1.7668\n",
            "Epoch 2/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5398 - loss: 1.7744\n",
            "Epoch 3/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5468 - loss: 1.7568\n",
            "Epoch 4/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5457 - loss: 1.7493\n",
            "Epoch 5/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5503 - loss: 1.7520\n",
            "Epoch 6/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5351 - loss: 1.7756\n",
            "Epoch 7/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5425 - loss: 1.7628\n",
            "Epoch 8/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5402 - loss: 1.7558\n",
            "Epoch 9/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5484 - loss: 1.7433\n",
            "Epoch 10/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5384 - loss: 1.7551\n",
            "Epoch 11/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5474 - loss: 1.7475\n",
            "Epoch 12/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5447 - loss: 1.7480\n",
            "Epoch 13/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5478 - loss: 1.7211\n",
            "Epoch 14/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5461 - loss: 1.7355\n",
            "Epoch 15/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5532 - loss: 1.7133\n",
            "Epoch 16/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5463 - loss: 1.7394\n",
            "Epoch 17/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5476 - loss: 1.7140\n",
            "Epoch 18/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5480 - loss: 1.7314\n",
            "Epoch 19/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5407 - loss: 1.7489\n",
            "Epoch 20/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5476 - loss: 1.7314\n",
            "Epoch 21/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5515 - loss: 1.7112\n",
            "Epoch 22/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5493 - loss: 1.7220\n",
            "Epoch 23/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5585 - loss: 1.6942\n",
            "Epoch 24/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5423 - loss: 1.7201\n",
            "Epoch 25/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5524 - loss: 1.7052\n",
            "Epoch 26/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5504 - loss: 1.7188\n",
            "Epoch 27/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5509 - loss: 1.7016\n",
            "Epoch 28/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5526 - loss: 1.7200\n",
            "Epoch 29/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5506 - loss: 1.6916\n",
            "Epoch 30/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5597 - loss: 1.6800\n",
            "Epoch 31/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5529 - loss: 1.6896\n",
            "Epoch 32/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5499 - loss: 1.7060\n",
            "Epoch 33/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5613 - loss: 1.6865\n",
            "Epoch 34/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5521 - loss: 1.7007\n",
            "Epoch 35/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5554 - loss: 1.6814\n",
            "Epoch 36/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5576 - loss: 1.7049\n",
            "Epoch 37/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5578 - loss: 1.6640\n",
            "Epoch 38/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5567 - loss: 1.6596\n",
            "Epoch 39/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5623 - loss: 1.6748\n",
            "Epoch 40/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5629 - loss: 1.6712\n",
            "Epoch 41/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5542 - loss: 1.6836\n",
            "Epoch 42/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5648 - loss: 1.6625\n",
            "Epoch 43/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5552 - loss: 1.6791\n",
            "Epoch 44/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5535 - loss: 1.6816\n",
            "Epoch 45/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5646 - loss: 1.6423\n",
            "Epoch 46/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5616 - loss: 1.6498\n",
            "Epoch 47/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5619 - loss: 1.6630\n",
            "Epoch 48/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.5718 - loss: 1.6370\n",
            "Epoch 49/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5617 - loss: 1.6538\n",
            "Epoch 50/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5614 - loss: 1.6502\n",
            "Epoch 51/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5667 - loss: 1.6500\n",
            "Epoch 52/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5605 - loss: 1.6469\n",
            "Epoch 53/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5673 - loss: 1.6182\n",
            "Epoch 54/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5646 - loss: 1.6311\n",
            "Epoch 55/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5648 - loss: 1.6387\n",
            "Epoch 56/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5737 - loss: 1.6188\n",
            "Epoch 57/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5720 - loss: 1.6067\n",
            "Epoch 58/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5628 - loss: 1.6417\n",
            "Epoch 59/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5588 - loss: 1.6485\n",
            "Epoch 60/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5680 - loss: 1.6421\n",
            "Epoch 61/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5624 - loss: 1.6470\n",
            "Epoch 62/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5720 - loss: 1.6210\n",
            "Epoch 63/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5693 - loss: 1.6178\n",
            "Epoch 64/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5669 - loss: 1.6218\n",
            "Epoch 65/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5722 - loss: 1.6184\n",
            "Epoch 66/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5632 - loss: 1.6388\n",
            "Epoch 67/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5704 - loss: 1.6063\n",
            "Epoch 68/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5814 - loss: 1.5776\n",
            "Epoch 69/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5683 - loss: 1.6048\n",
            "Epoch 70/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5741 - loss: 1.5906\n",
            "Epoch 71/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5746 - loss: 1.6099\n",
            "Epoch 72/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5756 - loss: 1.5963\n",
            "Epoch 73/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5705 - loss: 1.6040\n",
            "Epoch 74/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5721 - loss: 1.6038\n",
            "Epoch 75/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5670 - loss: 1.6161\n",
            "Epoch 76/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5767 - loss: 1.5963\n",
            "Epoch 77/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5765 - loss: 1.5571\n",
            "Epoch 78/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5756 - loss: 1.5922\n",
            "Epoch 79/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5867 - loss: 1.5718\n",
            "Epoch 80/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5800 - loss: 1.5791\n",
            "Epoch 81/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5798 - loss: 1.5841\n",
            "Epoch 82/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5739 - loss: 1.5886\n",
            "Epoch 83/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5835 - loss: 1.5745\n",
            "Epoch 84/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5808 - loss: 1.5762\n",
            "Epoch 85/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5785 - loss: 1.6024\n",
            "Epoch 86/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5842 - loss: 1.5609\n",
            "Epoch 87/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5811 - loss: 1.5545\n",
            "Epoch 88/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5821 - loss: 1.5531\n",
            "Epoch 89/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5798 - loss: 1.5630\n",
            "Epoch 90/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5843 - loss: 1.5670\n",
            "Epoch 91/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5794 - loss: 1.5821\n",
            "Epoch 92/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5875 - loss: 1.5502\n",
            "Epoch 93/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5860 - loss: 1.5477\n",
            "Epoch 94/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5855 - loss: 1.5519\n",
            "Epoch 95/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5863 - loss: 1.5402\n",
            "Epoch 96/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5861 - loss: 1.5567\n",
            "Epoch 97/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5828 - loss: 1.5611\n",
            "Epoch 98/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5830 - loss: 1.5552\n",
            "Epoch 99/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5764 - loss: 1.5626\n",
            "Epoch 100/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5896 - loss: 1.5363\n",
            "Epoch 101/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5824 - loss: 1.5519\n",
            "Epoch 102/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5803 - loss: 1.5595\n",
            "Epoch 103/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5892 - loss: 1.5311\n",
            "Epoch 104/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5855 - loss: 1.5419\n",
            "Epoch 105/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5904 - loss: 1.5289\n",
            "Epoch 106/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5904 - loss: 1.5282\n",
            "Epoch 107/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5795 - loss: 1.5598\n",
            "Epoch 108/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5905 - loss: 1.5283\n",
            "Epoch 109/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5921 - loss: 1.5270\n",
            "Epoch 110/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5923 - loss: 1.5027\n",
            "Epoch 111/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5900 - loss: 1.5356\n",
            "Epoch 112/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5909 - loss: 1.5221\n",
            "Epoch 113/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5852 - loss: 1.5477\n",
            "Epoch 114/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5919 - loss: 1.5108\n",
            "Epoch 115/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5857 - loss: 1.5291\n",
            "Epoch 116/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5906 - loss: 1.5132\n",
            "Epoch 117/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5888 - loss: 1.5380\n",
            "Epoch 118/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5884 - loss: 1.5181\n",
            "Epoch 119/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5971 - loss: 1.4997\n",
            "Epoch 120/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5961 - loss: 1.4974\n",
            "Epoch 121/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5963 - loss: 1.4975\n",
            "Epoch 122/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6000 - loss: 1.4992\n",
            "Epoch 123/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6017 - loss: 1.4900\n",
            "Epoch 124/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.5930 - loss: 1.5049\n",
            "Epoch 125/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5951 - loss: 1.4933\n",
            "Epoch 126/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5986 - loss: 1.4796\n",
            "Epoch 127/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5923 - loss: 1.5119\n",
            "Epoch 128/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5905 - loss: 1.5303\n",
            "Epoch 129/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5956 - loss: 1.4813\n",
            "Epoch 130/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5953 - loss: 1.4994\n",
            "Epoch 131/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5992 - loss: 1.4903\n",
            "Epoch 132/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6016 - loss: 1.4901\n",
            "Epoch 133/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6070 - loss: 1.4612\n",
            "Epoch 134/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5997 - loss: 1.4873\n",
            "Epoch 135/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6012 - loss: 1.4655\n",
            "Epoch 136/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6027 - loss: 1.4713\n",
            "Epoch 137/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6012 - loss: 1.4715\n",
            "Epoch 138/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5950 - loss: 1.4950\n",
            "Epoch 139/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5980 - loss: 1.4713\n",
            "Epoch 140/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5972 - loss: 1.4847\n",
            "Epoch 141/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6033 - loss: 1.4564\n",
            "Epoch 142/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6050 - loss: 1.4615\n",
            "Epoch 143/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6042 - loss: 1.4672\n",
            "Epoch 144/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5994 - loss: 1.4613\n",
            "Epoch 145/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6115 - loss: 1.4586\n",
            "Epoch 146/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5975 - loss: 1.4710\n",
            "Epoch 147/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6100 - loss: 1.4459\n",
            "Epoch 148/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5997 - loss: 1.4689\n",
            "Epoch 149/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6036 - loss: 1.4629\n",
            "Epoch 150/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6090 - loss: 1.4492\n",
            "Epoch 151/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6016 - loss: 1.4533\n",
            "Epoch 152/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6032 - loss: 1.4701\n",
            "Epoch 153/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6050 - loss: 1.4638\n",
            "Epoch 154/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6134 - loss: 1.4288\n",
            "Epoch 155/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6086 - loss: 1.4161\n",
            "Epoch 156/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5921 - loss: 1.5128\n",
            "Epoch 157/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5996 - loss: 1.4633\n",
            "Epoch 158/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6102 - loss: 1.4251\n",
            "Epoch 159/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6119 - loss: 1.4331\n",
            "Epoch 160/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5999 - loss: 1.4667\n",
            "Epoch 161/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5989 - loss: 1.4613\n",
            "Epoch 162/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6121 - loss: 1.4290\n",
            "Epoch 163/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6109 - loss: 1.4214\n",
            "Epoch 164/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6071 - loss: 1.4341\n",
            "Epoch 165/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6132 - loss: 1.4150\n",
            "Epoch 166/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6074 - loss: 1.4378\n",
            "Epoch 167/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.6138 - loss: 1.4335\n",
            "Epoch 168/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6117 - loss: 1.4241\n",
            "Epoch 169/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6071 - loss: 1.4398\n",
            "Epoch 170/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6112 - loss: 1.4209\n",
            "Epoch 171/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6153 - loss: 1.4089\n",
            "Epoch 172/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.6113 - loss: 1.4141\n",
            "Epoch 173/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6106 - loss: 1.4194\n",
            "Epoch 174/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6088 - loss: 1.4398\n",
            "Epoch 175/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6082 - loss: 1.4229\n",
            "Epoch 176/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6075 - loss: 1.4335\n",
            "Epoch 177/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6220 - loss: 1.3946\n",
            "Epoch 178/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6100 - loss: 1.4258\n",
            "Epoch 179/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6112 - loss: 1.4246\n",
            "Epoch 180/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6186 - loss: 1.4013\n",
            "Epoch 181/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6184 - loss: 1.4170\n",
            "Epoch 182/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.6188 - loss: 1.3922\n",
            "Epoch 183/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6161 - loss: 1.4074\n",
            "Epoch 184/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6110 - loss: 1.4207\n",
            "Epoch 185/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6101 - loss: 1.4036\n",
            "Epoch 186/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6169 - loss: 1.3942\n",
            "Epoch 187/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6160 - loss: 1.4018\n",
            "Epoch 188/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6123 - loss: 1.4067\n",
            "Epoch 189/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6180 - loss: 1.3909\n",
            "Epoch 190/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6147 - loss: 1.4126\n",
            "Epoch 191/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6186 - loss: 1.3894\n",
            "Epoch 192/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6130 - loss: 1.4055\n",
            "Epoch 193/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6095 - loss: 1.4169\n",
            "Epoch 194/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6187 - loss: 1.3729\n",
            "Epoch 195/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6185 - loss: 1.4082\n",
            "Epoch 196/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6235 - loss: 1.3817\n",
            "Epoch 197/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6223 - loss: 1.3777\n",
            "Epoch 198/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.6150 - loss: 1.3941\n",
            "Epoch 199/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6153 - loss: 1.3977\n",
            "Epoch 200/200\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6188 - loss: 1.3820\n"
          ]
        }
      ],
      "source": [
        "print(\"Iniciando o treinamento...\")\n",
        "# Aumente o número de épocas para melhores resultados (ex: 100)\n",
        "history = model.fit(X, y, epochs=200, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQn9G8lfQkOs"
      },
      "source": [
        "## 7. Geração de Texto\n",
        "\n",
        "Agora, a parte divertida. Vamos criar uma função para gerar texto usando o modelo treinado.\n",
        "\n",
        "Usaremos uma técnica chamada **\"Temperature Sampling\"** para controlar a \"criatividade\" do modelo.\n",
        "* `temperature` baixa (ex: 0.2): Texto mais previsível e conservador.\n",
        "* `temperature` alta (ex: 1.0): Texto mais criativo e arriscado (pode gerar mais erros)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_Ei6fHTCQkOt"
      },
      "outputs": [],
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    \"\"\"Helper para re-ponderar a distribuição de probabilidade com 'temperatura'.\"\"\"\n",
        "    # Adiciona 1e-7 para evitar log(0)\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds + 1e-7) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    # Amostra um índice da distribuição multinomial\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len, tokenizer, temperature=0.7):\n",
        "    generated_text = seed_text\n",
        "    current_seed = seed_text\n",
        "\n",
        "    for _ in range(next_words):\n",
        "        # 1. Tokenizar o texto atual (seed)\n",
        "        token_list = tokenizer.texts_to_sequences([current_seed])[0]\n",
        "\n",
        "        # 2. Fazer o padding para ter o tamanho exato de entrada do modelo\n",
        "        token_list_padded = pad_sequences([token_list], maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "        # 3. Obter as predições (probabilidades da próxima palavra)\n",
        "        predicted_probs = model.predict(token_list_padded, verbose=0)[0]\n",
        "\n",
        "        # 4. Aplicar o sampling com temperatura para escolher o índice da próxima palavra\n",
        "        predicted_index = sample(predicted_probs, temperature)\n",
        "\n",
        "        # 5. Converter o índice de volta para uma palavra\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        # 6. Se a palavra for <oov> ou vazia, pula esta iteração\n",
        "        if output_word == \"<oov>\" or not output_word:\n",
        "            continue\n",
        "\n",
        "        # 7. Adicionar a nova palavra ao texto gerado\n",
        "        generated_text += \" \" + output_word\n",
        "\n",
        "        # 8. Atualizar o 'current_seed' para a próxima iteração\n",
        "        # (Remove a primeira palavra e adiciona a nova ao final)\n",
        "        current_seed = ' '.join(current_seed.split()[1:]) + \" \" + output_word\n",
        "\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ynlkz6RZQkOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e98bcc-5f1a-4c9b-e579-7bde75f3b49e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SEED INICIAL (Semente) ---\n",
            "with a sigh “it’s always teatime and we’ve no time to wash the things between whiles” “then you keep moving round i suppose” said alice “exactly so” said the hatter “as the things get used up” “but what happens when you come to the beginning again” alice ventured to ask\n",
            "\n",
            "--- TEXTO GERADO (Temp 0.5 - Mais conservador) ---\n",
            "with a sigh “it’s always teatime and we’ve no time to wash the things between whiles” “then you keep moving round i suppose” said alice “exactly so” said the hatter “as the things get used up” “but what happens when you come to the beginning again” alice ventured to ask “suppose we change the subject” the march hare interrupted yawning “i’m that’s thimble” said the mock turtle “crumbs do you mean of” said the hatter “i didn’t seem it” said the hatter “as you know walk in it” very fine tone though i was now in the gryphon said to itself in the way were on the other side and her as it was peeped and broke in a head in looking at the pigeon as she spoke “either then “what said the mock turtle and he replied in a low voice “and us did you ask the queen this\n",
            "\n",
            "--- TEXTO GERADO (Temp 1.0 - Mais criativo) ---\n",
            "with a sigh “it’s always teatime and we’ve no time to wash the things between whiles” “then you keep moving round i suppose” said alice “exactly so” said the hatter “as the things get used up” “but what happens when you come to the beginning again” alice ventured to ask “suppose i don’t and won’t such a turn out of by the day i used you know you heard the knave and never could to say you or you deserved it would do at the day i don’t know it “and said this then alice had been such a thing of play it then it felt the white rabbit sang his arm on the queen and my friends dinah even what to any use a such much and thing to your thing is quite like now” said the mock turtle “but won’t said the gryphon “you i began “fifteenth” alice\n"
          ]
        }
      ],
      "source": [
        "# Vamos pegar um \"seed\" (semente) aleatório do texto original\n",
        "seed_start_index = np.random.randint(0, len(sequences))\n",
        "raw_seed = sequences[seed_start_index]\n",
        "\n",
        "# O seed text precisa ter exatamente 'seq_length' (50) palavras\n",
        "seed_text = ' '.join(raw_seed.split()[:-1]) # Pega as primeiras 50 palavras da sequência\n",
        "\n",
        "print(f\"--- SEED INICIAL (Semente) ---\\n{seed_text}\")\n",
        "\n",
        "print(\"\\n--- TEXTO GERADO (Temp 0.5 - Mais conservador) ---\")\n",
        "generated_output_05 = generate_text(seed_text, 100, model, max_sequence_len, tokenizer, temperature=0.5)\n",
        "print(generated_output_05)\n",
        "\n",
        "print(\"\\n--- TEXTO GERADO (Temp 1.0 - Mais criativo) ---\")\n",
        "generated_output_10 = generate_text(seed_text, 100, model, max_sequence_len, tokenizer, temperature=1.0)\n",
        "print(generated_output_10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSLP_-yoQkOx"
      },
      "source": [
        "## 8. Conclusão e Próximos Passos\n",
        "\n",
        "Conseguimos treinar uma LSTM que usa Word2Vec para gerar texto! O resultado com 30 épocas e um livro pequeno é apenas razoável, mas o processo está correto.\n",
        "\n",
        "Para melhorar:\n",
        "\n",
        "1.  **Mais Dados:** Use um corpus muito maior (ex: vários livros do Projeto Gutenberg combinados).\n",
        "2.  **Mais Treinamento:** Aumente o número de épocas (ex: 100, 200 ou mais).\n",
        "3.  **Ajuste Fino (Fine-Tuning):** Tente definir `trainable=True` na camada `Embedding` após algumas épocas iniciais, ou desde o início com uma taxa de aprendizado baixa. Isso permite que os pesos do Word2Vec sejam ligeiramente ajustados para o estilo literário do corpus.\n",
        "4.  **Arquitetura:** Experimente com mais camadas LSTM, `Bidirectional(LSTM(...))` ou `GRU`."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}