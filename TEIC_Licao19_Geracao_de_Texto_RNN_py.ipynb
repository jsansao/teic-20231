{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsansao/teic-20231/blob/main/TEIC_Licao19_Geracao_de_Texto_RNN_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geração de Texto em Nível de Caractere com RNN (LSTM)\n",
        "\n",
        " Este notebook demonstra como treinar uma Rede Neural Recorrente (LSTM) para gerar texto, um caractere de cada vez.\n",
        "\n",
        " Usaremos um pequeno dataset (um soneto de Shakespeare) para treinar o modelo. O objetivo é que o modelo aprenda os padrões da linguagem (quais caracteres tendem a seguir quais outros) e, em seguida, gere um novo texto que se assemelhe ao original.\n",
        "\n",
        " O processo é dividido em:\n",
        " 1.  **Carregar e Preparar os Dados**: Carregar o texto e criar um \"vocabulário\" de caracteres únicos.\n",
        " 2.  **Mapeamento**: Criar dicionários para converter caracteres em números inteiros e vice-versa.\n",
        " 3.  **Criar Sequências**: Transformar o texto em sequências de entrada (X) e o próximo caractere alvo (y).\n",
        " 4.  **Construir o Modelo**: Definir a arquitetura da rede com Embedding e LSTM.\n",
        " 5.  **Treinar o Modelo**: Treinar a rede para prever o próximo caractere.\n",
        " 6.  **Gerar Texto**: Usar o modelo treinado para gerar um novo texto a partir de uma \"semente\" inicial.\n",
        "\n",
        "\n",
        "## 1. Importações e Definição do Dataset ---"
      ],
      "metadata": {
        "id": "X1RvVa6xW8fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import sys\n",
        "\n",
        "print(f\"Versão do TensorFlow: {tf.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOx5F4sDWrHN",
        "outputId": "94b720a0-9112-4a9f-e6c1-de9ca5ded7c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versão do TensorFlow: 2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Nosso \"toy dataset\": Soneto 18 de Shakespeare\n",
        "# É pequeno o suficiente para treinar rápido, mas tem\n",
        "# estrutura, pontuação e capitalização para a rede aprender.\n",
        "text = \"\"\"\n",
        "Shall I compare thee to a summer's day?\n",
        "Thou art more lovely and more temperate:\n",
        "Rough winds do shake the darling buds of May,\n",
        "And summer's lease hath all too short a date:\n",
        "Sometime too hot the eye of heaven shines,\n",
        "And often is his gold complexion dimm'd;\n",
        "And every fair from fair sometime declines,\n",
        "By chance, or nature's changing course, untrimm'd;\n",
        "But thy eternal summer shall not fade\n",
        "Nor lose possession of that fair thou ow'st;\n",
        "Nor shall Death brag thou wander'st in his shade,\n",
        "When in eternal lines to time thou grow'st:\n",
        "  So long as men can breathe or eyes can see,\n",
        "  So long lives this, and this gives life to thee.\n",
        "\"\"\"\n",
        "\n",
        "# Limpa o texto (opcional, mas remove o \\n do início)\n",
        "text = text.strip()\n",
        "\n",
        "print(f\"Texto original com {len(text)} caracteres.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YwtLc-RXC8P",
        "outputId": "406dcb43-390b-4304-ec16-f499cae69839"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original com 625 caracteres.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nosso \"toy dataset\": Vários sonetos de Shakespeare.\n",
        "# Este texto é significativamente maior que o anterior,\n",
        "# permitindo ao modelo aprender padrões mais ricos.\n",
        "text = \"\"\"\n",
        "FROM fairest creatures we desire increase,\n",
        "That thereby beauty's rose might never die,\n",
        "But as the riper should by time decease,\n",
        "His tender heir might bear his memory:\n",
        "But thou, contracted to thine own bright eyes,\n",
        "Feed'st thy light's flame with self-substantial fuel,\n",
        "Making a famine where abundance lies,\n",
        "Thyself thy foe, to thy sweet self too cruel.\n",
        "Thou that art now the world's fresh ornament\n",
        "And only herald to the gaudy spring,\n",
        "Within thine own bud buriest thy content\n",
        "And, tender churl, mak'st waste in niggarding.\n",
        "Pity the world, or else this glutton be,\n",
        "To eat the world's due, by the grave and thee.\n",
        "\n",
        "When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery, so gazed on now,\n",
        "Will be a tatter'd weed, of small worth held:\n",
        "Then being ask'd where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days,\n",
        "To say, within thine own deep-sunken eyes,\n",
        "Were an all-eating shame and thriftless praise.\n",
        "How much more praise deserved thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\n",
        "\n",
        "Look in thy glass, and tell the face thou viewest\n",
        "Now is the time that face should form another;\n",
        "Whose fresh repair if now thou not renewest,\n",
        "Thou dost beguile the world, unbless some mother.\n",
        "For where is she so fair whose unear'd womb\n",
        "Disdains the tillage of thy husbandry?\n",
        "Or who is he so fond will be the tomb\n",
        "Of his self-love, to stop posterity?\n",
        "Thou art thy mother's glass, and she in thee\n",
        "Calls back the lovely April of her prime:\n",
        "So thou through windows of thine age shalt see,\n",
        "Despite of wrinkles, this thy golden time.\n",
        "But if thou live, remember'd not to be,\n",
        "Die single, and thine image dies with thee.\n",
        "\n",
        "Unthrifty loveliness, why dost thou spend\n",
        "Upon thyself thy beauty's legacy?\n",
        "Nature's bequest gives nothing, but doth lend,\n",
        "And being frank, she lends to those are free.\n",
        "Then, beauteous niggard, why dost thou abuse\n",
        "The bounteous largess given thee to give?\n",
        "Profitless usurer, why dost thou use\n",
        "So great a sum of sums, yet canst not live?\n",
        "For having traffic with thyself alone,\n",
        "Thou of thyself thy sweet self dost deceive.\n",
        "Then how, when nature calls thee to be gone,\n",
        "What acceptable audit canst thou leave?\n",
        "Thy unused beauty must be tomb'd with thee,\n",
        "Which, used, lives th' executor to be.\n",
        "\n",
        "Those hours, that with gentle work did frame\n",
        "The lovely gaze where every eye doth dwell,\n",
        "Will play the tyrants to the very same\n",
        "And that unfair which fairly doth excel;\n",
        "For never-resting time leads summer on\n",
        "To hideous winter and confounds him there;\n",
        "Sap check'd with frost and lusty leaves quite gone,\n",
        "Beauty o'ersnow'd and bareness every where:\n",
        "Then, were not summer's distillation left,\n",
        "A liquid prisoner pent in walls of glass,\n",
        "Beauty's effect with beauty were bereft,\n",
        "Nor it, nor no remembrance what it was:\n",
        "But flowers distill'd, though they with winter meet,\n",
        "Leese but their show; their substance still lives sweet.\n",
        "\n",
        "Then let not winter's ragged hand deface\n",
        "In thee thy summer, ere thou be distill'd:\n",
        "Make sweet some vial; treasure thou some place\n",
        "With beauty's treasure, ere it be self-kill'd.\n",
        "That use is not forbidden usury,\n",
        "Which happies those that pay the willing loan;\n",
        "That's for thyself to breed another thee,\n",
        "Or ten times happier, be it ten for one;\n",
        "Ten times thyself were happier than thou art,\n",
        "If ten of thine ten times refigured thee:\n",
        "Then what could death do, if thou shouldst depart,\n",
        "Leaving thee living in posterity?\n",
        "Be not self-will'd, for thou art much too fair\n",
        "To be death's conquest and make worms thine heir.\n",
        "\n",
        "Lo, in the orient when the gracious light\n",
        "Lifts up his burning head, each under eye\n",
        "Doth homage to his new-appearing sight,\n",
        "Serving with looks his sacred majesty;\n",
        "And having climb'd the steep-up heavenly hill,\n",
        "Resembling strong youth in his middle age,\n",
        "Yet mortal looks adore his beauty still,\n",
        "Attending on his golden pilgrimage;\n",
        "But when from highmost pitch, with weary car,\n",
        "Like feeble age, he reeleth from the day,\n",
        "The eyes, 'fore duteous, now converted are\n",
        "From his low tract, and look another way:\n",
        "So thou, thyself outgoing in thy noon,\n",
        "Unlook'd on diest, unless thou get a son.\n",
        "\n",
        "Music to hear, why hear'st thou music sadly?\n",
        "Sweets with sweets war not, joy delights in joy.\n",
        "Why lov'st thou that which thou receiv'st not gladly,\n",
        "Or else receiv'st with pleasure thine annoy?\n",
        "If the true concord of well-tuned sounds,\n",
        "By unions married, do offend thine ear,\n",
        "They do but sweetly chide thee, who confounds\n",
        "In singleness the parts that thou shouldst bear.\n",
        "Mark how one string, sweet husband to another,\n",
        "Strikes each in each by mutual ordering;\n",
        "Resembling sire and child and happy mother,\n",
        "Who, all in one, one pleasing note do sing:\n",
        "Whose speechless song, being many, seeming one,\n",
        "Sings this to thee: 'Thou single wilt prove none.'\n",
        "\n",
        "Is it for fear to wet a widow's eye\n",
        "That thou consum'st thyself in single life?\n",
        "Ah! if thou issueless shalt hap to die,\n",
        "The world will wail thee, like a makeless wife;\n",
        "The world will be thy widow, and still weep\n",
        "That thou no form of thee hast left behind,\n",
        "When every private widow well may keep\n",
        "By children's eyes her husband's shape in mind.\n",
        "Look, what an unthrift in the world doth spend\n",
        "Shifts but his place, for still the world enjoys it;\n",
        "But beauty's waste hath in the world an end,\n",
        "And kept unused, the user so destroys it.\n",
        "No love toward others in that bosom sits\n",
        "That on himself such murderous shame commits.\n",
        "\n",
        "For shame! deny that thou bear'st love to any,\n",
        "Who for thyself art so unprovident.\n",
        "Grant, if thou wilt, thou art beloved of many,\n",
        "But that thou none lov'st is most evident;\n",
        "For thou art so possess'd with murderous hate,\n",
        "That 'gainst thyself thou stick'st not to conspire,\n",
        "Seeking that beauteous roof to ruinate\n",
        "Which to repair should be thy chief desire.\n",
        "O, change thy thought, that I may change my mind!\n",
        "Shall hate be fairer lodged than gentle love?\n",
        "Be, as thy presence is, gracious and kind,\n",
        "Or to thyself at least kind-hearted prove:\n",
        "Make thee another self, for love of me,\n",
        "That beauty still may live in thine or thee.\n",
        "\"\"\"\n",
        "\n",
        "# Limpa o texto (remove espaços/quebras de linha extras no início e fim)\n",
        "text = text.strip()\n",
        "\n",
        "print(f\"Texto original com {len(text)} caracteres.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BD1YOCwbXK9",
        "outputId": "e4e079bb-3f8a-4ab0-fd40-b2793aa86ced"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original com 6149 caracteres.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Pré-processamento: Mapeamento de Caracteres\n",
        "\n",
        " A rede neural não entende 'a', 'b', 'c'. Ela entende números.\n",
        "\n",
        " Primeiro, criamos um \"vocabulário\" com todos os caracteres únicos do texto. Em seguida, criamos dois dicionários:\n",
        "\n",
        " 1.  `char_to_int`: Mapeia um caractere (ex: 'h') para um número (ex: 15).\n",
        " 2.  `int_to_char`: Mapeia um número (ex: 15) de volta para um caractere (ex: 'h').\n"
      ],
      "metadata": {
        "id": "dbiH2JNdXIcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encontra todos os caracteres únicos e os ordena\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f\"Total de caracteres únicos (vocabulário): {vocab_size}\")\n",
        "print(f\"Vocabulário: {''.join(chars)}\")\n",
        "\n",
        "# Cria os dicionários de mapeamento\n",
        "char_to_int = {char: i for i, char in enumerate(chars)}\n",
        "int_to_char = {i: char for i, char in enumerate(chars)}\n",
        "\n",
        "# Exemplo de mapeamento\n",
        "print(f\"\\nExemplo: 'S' -> {char_to_int['S']}, 'a' -> {char_to_int['a']}\")\n",
        "print(f\"Exemplo: 10 -> '{int_to_char[10]}', 20 -> '{int_to_char[20]}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaltgQYfXU9l",
        "outputId": "37a843e8-da3c-4582-c41a-56a097e9fc6c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de caracteres únicos (vocabulário): 55\n",
            "Vocabulário: \n",
            " !',-.:;?ABCDFGHILMNOPRSTUWYabcdefghijklmnopqrstuvwxyz\n",
            "\n",
            "Exemplo: 'S' -> 24, 'a' -> 29\n",
            "Exemplo: 10 -> 'A', 20 -> 'N'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Criação das Sequências de Treinamento\n",
        "\n",
        " Agora, transformamos o texto em um problema de aprendizado supervisionado. Usaremos uma \"janela deslizante\" para criar sequências de entrada (X) e um caractere alvo (y).\n",
        "\n",
        " Se `seq_length = 5` e o texto for \"hello\":\n",
        "\n",
        " * `X` = \"hell\", `y` = \"o\""
      ],
      "metadata": {
        "id": "qAxsz-1OXYoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o tamanho da nossa sequência de entrada\n",
        "seq_length = 40\n",
        "step = 1 # Desliza a janela 1 caractere de cada vez\n",
        "\n",
        "sequences = []   # Armazena as sequências de entrada (X)\n",
        "next_chars = []  # Armazena os caracteres alvo (y)\n",
        "\n",
        "# Converte o texto inteiro para inteiros\n",
        "encoded_text = [char_to_int[c] for c in text]\n",
        "\n",
        "# Cria as sequências\n",
        "for i in range(0, len(encoded_text) - seq_length, step):\n",
        "    sequences.append(encoded_text[i : i + seq_length])\n",
        "    next_chars.append(encoded_text[i + seq_length])\n",
        "\n",
        "n_sequences = len(sequences)\n",
        "print(f\"Total de sequências de treinamento: {n_sequences}\")\n",
        "\n",
        "# Converte para arrays numpy\n",
        "X = np.array(sequences)\n",
        "y = np.array(next_chars)\n",
        "\n",
        "print(f\"Formato de X (antes do pad): {X.shape}\")\n",
        "print(f\"Formato de y: {y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksGBBFL_Xl1t",
        "outputId": "ecf99eff-e84c-4242-d34c-0211dd61dea1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de sequências de treinamento: 6109\n",
            "Formato de X (antes do pad): (6109, 40)\n",
            "Formato de y: (6109,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Construção do Modelo RNN (LSTM)\n",
        "\n",
        " Construímos nosso modelo:\n",
        "\n",
        " 1.  **Input**: Define o formato de entrada, que é `(seq_length,)`.\n",
        " 2.  **Embedding**: Esta camada é muito importante. Ela transforma nossos números inteiros (ex: 15) em vetores densos (ex: `[0.1, 0.7, -0.2, ...]`). Isso permite que a rede aprenda *relações* entre caracteres (ex: 'a' é semanticamente mais próximo de 'e' do que de 'z').\n",
        " 3.  **LSTM**: A camada recorrente principal que processa a sequência e mantém a memória.\n",
        " 4.  **Dense (Saída)**: Uma camada densa com `vocab_size` neurônios e ativação `softmax`. Ela produzirá uma distribuição de probabilidade sobre *todos os caracteres possíveis* do vocabulário, indicando qual ela acha que é o próximo.\n",
        "\n"
      ],
      "metadata": {
        "id": "tolhw5iVXrOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperparâmetros\n",
        "EMBEDDING_DIM = 64\n",
        "LSTM_UNITS = 128\n",
        "\n",
        "model = Sequential([\n",
        "    # A entrada é a sequência de `seq_length` inteiros\n",
        "    Input(shape=(seq_length,)),\n",
        "\n",
        "    # Camada de Embedding: transforma inteiros em vetores densos\n",
        "    # input_dim = tamanho do vocabulário\n",
        "    # output_dim = tamanho de cada vetor de embedding\n",
        "    Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM),\n",
        "\n",
        "    # A camada LSTM que processa a sequência de vetores\n",
        "    LSTM(LSTM_UNITS),\n",
        "\n",
        "    # A camada de saída que prevê o próximo caractere\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compila o modelo\n",
        "# Usamos 'sparse_categorical_crossentropy' porque nossos alvos (y)\n",
        "# são inteiros (0, 1, 2...), e não vetores one-hot.\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "kBdHSlDcXwms",
        "outputId": "590fb085-d15b-4d40-93ad-24962b1a8ceb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m3,520\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,816\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m)             │         \u001b[38;5;34m7,095\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,520</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,095</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,431\u001b[0m (427.46 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,431</span> (427.46 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,431\u001b[0m (427.46 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,431</span> (427.46 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Treinamento do Modelo\n",
        "\n",
        "Agora, treinamos o modelo. Como o dataset é muito pequeno, o treinamento será rápido e podemos usar um número maior de épocas.\n"
      ],
      "metadata": {
        "id": "imFXSrOlXx-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Iniciando o treinamento...\")\n",
        "\n",
        "# O dataset é pequeno, então podemos usar um número alto de épocas.\n",
        "# 50-100 épocas devem mostrar um resultado interessante.\n",
        "history = model.fit(\n",
        "    X,\n",
        "    y,\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Treinamento concluído.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4kEY9z8X8GZ",
        "outputId": "b3589004-78ba-48ea-a148-9e0bee8880f4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando o treinamento...\n",
            "Epoch 1/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.1347 - loss: 3.4391\n",
            "Epoch 2/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.1857 - loss: 3.0185\n",
            "Epoch 3/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.2815 - loss: 2.6783\n",
            "Epoch 4/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.3051 - loss: 2.4888\n",
            "Epoch 5/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.3266 - loss: 2.3727\n",
            "Epoch 6/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - accuracy: 0.3452 - loss: 2.2968\n",
            "Epoch 7/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.3289 - loss: 2.2674\n",
            "Epoch 8/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.3520 - loss: 2.2387\n",
            "Epoch 9/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.3437 - loss: 2.2042\n",
            "Epoch 10/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.3613 - loss: 2.1588\n",
            "Epoch 11/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.3817 - loss: 2.1120\n",
            "Epoch 12/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.3885 - loss: 2.0635\n",
            "Epoch 13/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - accuracy: 0.3999 - loss: 2.0387\n",
            "Epoch 14/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.3976 - loss: 2.0428\n",
            "Epoch 15/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.4175 - loss: 1.9715\n",
            "Epoch 16/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.4279 - loss: 1.9521\n",
            "Epoch 17/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 61ms/step - accuracy: 0.4259 - loss: 1.9589\n",
            "Epoch 18/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.4360 - loss: 1.8944\n",
            "Epoch 19/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.4497 - loss: 1.8606\n",
            "Epoch 20/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.4519 - loss: 1.8221\n",
            "Epoch 21/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.4612 - loss: 1.7999\n",
            "Epoch 22/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.4587 - loss: 1.7954\n",
            "Epoch 23/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.4716 - loss: 1.7558\n",
            "Epoch 24/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.4760 - loss: 1.7142\n",
            "Epoch 25/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.4771 - loss: 1.7067\n",
            "Epoch 26/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.4935 - loss: 1.6703\n",
            "Epoch 27/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.4966 - loss: 1.6582\n",
            "Epoch 28/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5201 - loss: 1.5912\n",
            "Epoch 29/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.5257 - loss: 1.5745\n",
            "Epoch 30/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.5297 - loss: 1.5474\n",
            "Epoch 31/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.5437 - loss: 1.5430\n",
            "Epoch 32/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.5363 - loss: 1.5157\n",
            "Epoch 33/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.5634 - loss: 1.4718\n",
            "Epoch 34/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.5477 - loss: 1.4670\n",
            "Epoch 35/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.5599 - loss: 1.4441\n",
            "Epoch 36/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.5754 - loss: 1.4243\n",
            "Epoch 37/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.5804 - loss: 1.3839\n",
            "Epoch 38/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.5974 - loss: 1.3427\n",
            "Epoch 39/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - accuracy: 0.5958 - loss: 1.3344\n",
            "Epoch 40/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.6116 - loss: 1.3068\n",
            "Epoch 41/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.6269 - loss: 1.2628\n",
            "Epoch 42/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.6251 - loss: 1.2455\n",
            "Epoch 43/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.6324 - loss: 1.2196\n",
            "Epoch 44/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 61ms/step - accuracy: 0.6442 - loss: 1.1923\n",
            "Epoch 45/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.6526 - loss: 1.1744\n",
            "Epoch 46/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 59ms/step - accuracy: 0.6625 - loss: 1.1283\n",
            "Epoch 47/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.6646 - loss: 1.1403\n",
            "Epoch 48/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.6771 - loss: 1.0950\n",
            "Epoch 49/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.6848 - loss: 1.0860\n",
            "Epoch 50/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.6799 - loss: 1.0873\n",
            "Epoch 51/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - accuracy: 0.6899 - loss: 1.0485\n",
            "Epoch 52/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.7062 - loss: 1.0147\n",
            "Epoch 53/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - accuracy: 0.7219 - loss: 0.9810\n",
            "Epoch 54/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - accuracy: 0.7311 - loss: 0.9617\n",
            "Epoch 55/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7224 - loss: 0.9661\n",
            "Epoch 56/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.7376 - loss: 0.9225\n",
            "Epoch 57/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.7419 - loss: 0.8953\n",
            "Epoch 58/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7525 - loss: 0.8853\n",
            "Epoch 59/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.7605 - loss: 0.8539\n",
            "Epoch 60/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - accuracy: 0.7657 - loss: 0.8369\n",
            "Epoch 61/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.7781 - loss: 0.8069\n",
            "Epoch 62/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.7981 - loss: 0.7672\n",
            "Epoch 63/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.7984 - loss: 0.7583\n",
            "Epoch 64/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.8087 - loss: 0.7364\n",
            "Epoch 65/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.8022 - loss: 0.7399\n",
            "Epoch 66/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - accuracy: 0.8238 - loss: 0.7062\n",
            "Epoch 67/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.8258 - loss: 0.6765\n",
            "Epoch 68/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.8367 - loss: 0.6450\n",
            "Epoch 69/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.8361 - loss: 0.6374\n",
            "Epoch 70/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.8460 - loss: 0.6240\n",
            "Epoch 71/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.8534 - loss: 0.5955\n",
            "Epoch 72/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.8681 - loss: 0.5757\n",
            "Epoch 73/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.8681 - loss: 0.5561\n",
            "Epoch 74/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - accuracy: 0.8746 - loss: 0.5402\n",
            "Epoch 75/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.8790 - loss: 0.5293\n",
            "Epoch 76/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - accuracy: 0.8881 - loss: 0.5130\n",
            "Epoch 77/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9005 - loss: 0.4687\n",
            "Epoch 78/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - accuracy: 0.9080 - loss: 0.4513\n",
            "Epoch 79/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.9078 - loss: 0.4499\n",
            "Epoch 80/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9112 - loss: 0.4319\n",
            "Epoch 81/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.9190 - loss: 0.4219\n",
            "Epoch 82/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.9257 - loss: 0.3992\n",
            "Epoch 83/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - accuracy: 0.9360 - loss: 0.3768\n",
            "Epoch 84/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.9342 - loss: 0.3800\n",
            "Epoch 85/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.9410 - loss: 0.3534\n",
            "Epoch 86/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.9390 - loss: 0.3466\n",
            "Epoch 87/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.9416 - loss: 0.3412\n",
            "Epoch 88/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.9461 - loss: 0.3266\n",
            "Epoch 89/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9529 - loss: 0.2981\n",
            "Epoch 90/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9547 - loss: 0.2929\n",
            "Epoch 91/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.9663 - loss: 0.2683\n",
            "Epoch 92/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9641 - loss: 0.2602\n",
            "Epoch 93/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.9723 - loss: 0.2399\n",
            "Epoch 94/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9686 - loss: 0.2418\n",
            "Epoch 95/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.9721 - loss: 0.2245\n",
            "Epoch 96/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.9765 - loss: 0.2167\n",
            "Epoch 97/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.9730 - loss: 0.2210\n",
            "Epoch 98/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9746 - loss: 0.2093\n",
            "Epoch 99/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.9773 - loss: 0.2052\n",
            "Epoch 100/100\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.9805 - loss: 0.1953\n",
            "Treinamento concluído.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Geração de Texto\n",
        "\n",
        " Esta é a parte divertida!\n",
        "\n",
        " O processo de geração é iterativo:\n",
        " 1.  Forneça uma \"semente\" (seed) de texto (ex: \"Shall I compare\").\n",
        " 2.  Prepare a semente: converta-a em inteiros e aplique padding para ter o tamanho `seq_length`.\n",
        " 3.  Peça ao modelo para *prever* o próximo caractere.\n",
        " 4.  O modelo retorna probabilidades. Em vez de pegar o caractere mais provável (o que leva a um texto chato e repetitivo), nós *amostramos* a partir dessa distribuição.\n",
        " 5.  Pegamos o caractere amostrado e o adicionamos ao final da nossa semente (e removemos o primeiro caractere da semente).\n",
        " 6.  Repetimos os passos 3-5 para gerar quantos caracteres quisermos.\n",
        "\n"
      ],
      "metadata": {
        "id": "9HvOhzl4X_Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed_text, num_chars_to_gen=200):\n",
        "    \"\"\"\n",
        "    Gera texto usando o modelo treinado.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Gerando texto a partir da semente: '{seed_text}' ---\")\n",
        "\n",
        "    # Armazena o texto gerado\n",
        "    generated_text = seed_text\n",
        "\n",
        "    # 'current_input' é a nossa \"janela deslizante\" de texto\n",
        "    current_input = seed_text\n",
        "\n",
        "    for i in range(num_chars_to_gen):\n",
        "        # 1. Preparar a entrada (seed)\n",
        "        # Converte o texto de entrada para inteiros\n",
        "        x_pred = np.array([char_to_int[c] for c in current_input])\n",
        "\n",
        "        # Adiciona padding para ter o tamanho `seq_length`\n",
        "        # 'pre' padding: [0, 0, ..., id1, id2, id3]\n",
        "        x_pred = pad_sequences([x_pred], maxlen=seq_length, padding='pre', truncating='pre')\n",
        "\n",
        "        # 2. Fazer a previsão (obter probabilidades)\n",
        "        # model.predict retorna (1, vocab_size), então pegamos [0]\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "\n",
        "        # 3. Amostrar o próximo caractere\n",
        "        # Usamos np.random.choice para amostrar com base nas probabilidades 'p'\n",
        "        # Isso torna a geração não-determinística e mais criativa.\n",
        "        next_int = np.random.choice(len(chars), p=preds)\n",
        "\n",
        "        # 4. Converter o inteiro de volta para um caractere\n",
        "        next_char = int_to_char[next_int]\n",
        "\n",
        "        # 5. Adicionar ao resultado e atualizar a entrada\n",
        "        generated_text += next_char\n",
        "        current_input = current_input[1:] + next_char\n",
        "\n",
        "        # Imprime o caractere gerado em tempo real\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    print(\"\\n--- Fim da Geração ---\")\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "yVoBfuvNYVIq"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Vamos gerar!\n",
        "# Pegamos uma semente do início do texto.\n",
        "start_index = np.random.randint(0, len(text) - seq_length - 1)\n",
        "seed_text = text[start_index : start_index + seq_length]\n",
        "# Gera o texto\n",
        "generated_text = generate_text(model, seed_text, num_chars_to_gen=300)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Gerando texto a partir da semente: 'stop posterity?\n",
            "Thou art thy mother's gl' ---\n",
            "ass, and seathid;\n",
            "To eart to be thy winters warone if there;\n",
            "Srace if thy farld wilt ways hear's bearte\n",
            "Or the wiel, from hichming not ent so viede\n",
            "fatr of thou faccelv'st bacuners glive\n",
            "Dof maness the conts torld on time leats\n",
            "Lind to he has loof thyself anst beguith's deft\n",
            "And summ'd with beauty b\n",
            "--- Fim da Geração ---\n"
          ]
        }
      ],
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW4UhxeZVyfj",
        "outputId": "eb708535-83d1-462c-e58e-5c438782f3ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "\n",
        "Like feeble age, he reeleth from the day,\n",
        "The eyes, 'fore duteous, now converted are\n",
        "From his low tract, and look another way:\n",
        "So thou, thyself outgoing in thy noon,\n",
        "Unlook'd on diest, unless thou get a son.\n",
        "\n",
        "\n",
        "--- Gerando texto a partir da semente: 'ch, with weary car,\n",
        "Like feeble age, he ' ---\n",
        "hee anund'st the ares,\n",
        "Or tenger of thou alow a sonf in thy braire.\n",
        "Prout thyself for stoms tont thyself thy eve?\n",
        "To reweir my inow, she gold to though beraith's in thein.\n",
        "Fo consing, of thee happier thou beauty livery And,\n",
        "Reseaptures with tin widow werl, form dom die,\n",
        "Thy entracs, and the ir thy f\n",
        "--- Fim da Geração ---\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "hSv3Q9n8iUsO"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}