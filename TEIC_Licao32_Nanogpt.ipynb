{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsansao/teic-20231/blob/main/TEIC_Licao32_Nanogpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWV6epU0xaWG"
      },
      "source": [
        "# 📖 Demonstração: Treinando um nanoGPT do Zero\n",
        "\n",
        "Este notebook irá demonstrar como treinar um modelo de linguagem (GPT) do zero usando o repositório `nanoGPT` de Andrej Karpathy.\n",
        "\n",
        "Vamos treinar um modelo *baseado em caracteres* no corpus de \"Tiny Shakespeare\" para que ele aprenda a \"falar\" como Shakespeare.\n",
        "\n",
        "### Passo 1: Configurar o Ambiente\n",
        "\n",
        "Primeiro, precisamos garantir que estamos usando uma GPU.\n",
        "\n",
        "1.  Vá em **Ambiente de execução -> Alterar o tipo de ambiente de execução**.\n",
        "2.  Selecione **T4 GPU** (ou qualquer GPU disponível) e clique em `Salvar`.\n",
        "\n",
        "Agora, vamos verificar se o PyTorch consegue detectar a GPU."
      ],
      "id": "uWV6epU0xaWG"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d4RMZrWxaWI",
        "outputId": "c2282e62-126d-475b-ed2e-65bd07da2c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU (CUDA) detectada! 👍\n",
            "Nome da GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU (CUDA) detectada! 👍\")\n",
        "    print(f\"Nome da GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"GPU não detectada. 👎\")\n",
        "    print(\"Por favor, habilite a GPU no 'Ambiente de execução' para continuar.\")"
      ],
      "id": "7d4RMZrWxaWI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mmpe8eKxaWJ"
      },
      "source": [
        "### Passo 2: Clonar o nanoGPT e Instalar Dependências\n",
        "\n",
        "Vamos baixar o código do GitHub e instalar as bibliotecas necessárias."
      ],
      "id": "7Mmpe8eKxaWJ"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoRmih-2xaWJ",
        "outputId": "decc0e1d-bf4a-4b8d-ac5c-573021bc121f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 686, done.\u001b[K\n",
            "remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\n",
            "Receiving objects: 100% (686/686), 974.06 KiB | 3.78 MiB/s, done.\n",
            "Resolving deltas: 100% (380/380), done.\n",
            "/content/nanoGPT\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# 1. Clona o repositório oficial\n",
        "!git clone https://github.com/karpathy/nanoGPT.git\n",
        "\n",
        "# 2. Entra no diretório do projeto\n",
        "# Usamos o comando mágico %cd para mudar o diretório do notebook\n",
        "%cd nanoGPT\n",
        "\n",
        "# 3. Instala as bibliotecas necessárias\n",
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ],
      "id": "CoRmih-2xaWJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFP5l8aUxaWK"
      },
      "source": [
        "### Passo 3: Obter e Preparar os Dados (Corpus de Shakespeare)\n",
        "\n",
        "O repositório já vem com um script que baixa o \"Tiny Shakespeare\" (um arquivo `input.txt`). Este script irá \"tokenizar\" o texto, ou seja, converter todos os caracteres únicos em números."
      ],
      "id": "LFP5l8aUxaWK"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9x1fI0OxaWL",
        "outputId": "b87bae31-8a2b-44c7-ec1b-d48e240f5b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ],
      "source": [
        "# Este script baixa o input.txt e o processa,\n",
        "# criando os arquivos 'train.bin' e 'val.bin' na pasta 'data/shakespeare_char'\n",
        "!python data/shakespeare_char/prepare.py"
      ],
      "id": "-9x1fI0OxaWL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t9e63zzxaWL"
      },
      "source": [
        "### Passo 4: Treinar o Modelo!\n",
        "\n",
        "Este é o momento principal. Vamos iniciar o treinamento usando o arquivo de configuração `train_shakespeare_char.py`.\n",
        "\n",
        "Preste atenção no output: você verá o `iter` (iteração) e o `loss` (erro). O `loss` deve diminuir progressivamente, provando que o modelo está aprendendo."
      ],
      "id": "8t9e63zzxaWL"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyEDdI2GxaWL",
        "outputId": "3f45e0bc-4ae3-4c7a-fe3a-0e1b39741b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "W1030 20:19:53.536000 616 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "step 0: train loss 4.2874, val loss 4.2823\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "iter 0: loss 4.2653, time 75821.67ms, mfu -100.00%\n",
            "iter 10: loss 3.1424, time 530.31ms, mfu 0.70%\n",
            "iter 20: loss 2.7336, time 523.50ms, mfu 0.70%\n",
            "iter 30: loss 2.6169, time 525.87ms, mfu 0.70%\n",
            "iter 40: loss 2.5760, time 526.14ms, mfu 0.70%\n",
            "iter 50: loss 2.5263, time 524.04ms, mfu 0.71%\n",
            "iter 60: loss 2.5081, time 522.70ms, mfu 0.71%\n",
            "iter 70: loss 2.4933, time 522.42ms, mfu 0.71%\n",
            "iter 80: loss 2.4959, time 523.87ms, mfu 0.71%\n",
            "iter 90: loss 2.4690, time 526.28ms, mfu 0.71%\n",
            "iter 100: loss 2.4606, time 524.92ms, mfu 0.71%\n",
            "iter 110: loss 2.4531, time 524.22ms, mfu 0.71%\n",
            "iter 120: loss 2.4255, time 524.95ms, mfu 0.71%\n",
            "iter 130: loss 2.4127, time 525.14ms, mfu 0.71%\n",
            "iter 140: loss 2.4130, time 526.53ms, mfu 0.71%\n",
            "iter 150: loss 2.4114, time 527.47ms, mfu 0.71%\n",
            "iter 160: loss 2.3761, time 525.36ms, mfu 0.71%\n",
            "iter 170: loss 2.3767, time 526.42ms, mfu 0.71%\n",
            "iter 180: loss 2.3191, time 525.30ms, mfu 0.71%\n",
            "iter 190: loss 2.2531, time 524.77ms, mfu 0.71%\n",
            "iter 200: loss 2.2257, time 525.64ms, mfu 0.71%\n",
            "iter 210: loss 2.1454, time 524.64ms, mfu 0.71%\n",
            "iter 220: loss 2.1384, time 525.38ms, mfu 0.71%\n",
            "iter 230: loss 2.0778, time 526.02ms, mfu 0.71%\n",
            "iter 240: loss 2.0855, time 526.25ms, mfu 0.71%\n",
            "step 250: train loss 1.9650, val loss 2.0674\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.0412, time 73798.38ms, mfu 0.64%\n",
            "iter 260: loss 1.9759, time 522.48ms, mfu 0.65%\n",
            "iter 270: loss 1.9719, time 524.66ms, mfu 0.65%\n",
            "iter 280: loss 1.9749, time 524.04ms, mfu 0.66%\n",
            "iter 290: loss 1.9158, time 524.14ms, mfu 0.66%\n",
            "iter 300: loss 1.9019, time 526.05ms, mfu 0.67%\n",
            "iter 310: loss 1.8636, time 526.11ms, mfu 0.67%\n",
            "iter 320: loss 1.8454, time 524.67ms, mfu 0.68%\n",
            "iter 330: loss 1.8111, time 525.77ms, mfu 0.68%\n",
            "iter 340: loss 1.7829, time 525.43ms, mfu 0.68%\n",
            "iter 350: loss 1.8266, time 525.11ms, mfu 0.68%\n",
            "iter 360: loss 1.7644, time 527.45ms, mfu 0.69%\n",
            "iter 370: loss 1.7319, time 526.46ms, mfu 0.69%\n",
            "iter 380: loss 1.7256, time 527.02ms, mfu 0.69%\n",
            "iter 390: loss 1.7236, time 526.61ms, mfu 0.69%\n",
            "iter 400: loss 1.7635, time 526.84ms, mfu 0.69%\n",
            "iter 410: loss 1.6987, time 526.30ms, mfu 0.70%\n",
            "iter 420: loss 1.7165, time 527.39ms, mfu 0.70%\n",
            "iter 430: loss 1.6852, time 525.67ms, mfu 0.70%\n",
            "iter 440: loss 1.6608, time 528.51ms, mfu 0.70%\n",
            "iter 450: loss 1.6398, time 526.06ms, mfu 0.70%\n",
            "iter 460: loss 1.5976, time 525.82ms, mfu 0.70%\n",
            "iter 470: loss 1.6588, time 526.31ms, mfu 0.70%\n",
            "iter 480: loss 1.6229, time 528.27ms, mfu 0.70%\n",
            "iter 490: loss 1.5944, time 527.40ms, mfu 0.70%\n",
            "step 500: train loss 1.5201, val loss 1.7172\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.5953, time 74123.88ms, mfu 0.63%\n",
            "iter 510: loss 1.6015, time 524.24ms, mfu 0.64%\n",
            "iter 520: loss 1.5848, time 527.52ms, mfu 0.65%\n",
            "iter 530: loss 1.5556, time 526.47ms, mfu 0.65%\n",
            "iter 540: loss 1.6126, time 525.77ms, mfu 0.66%\n",
            "iter 550: loss 1.5605, time 526.04ms, mfu 0.66%\n",
            "iter 560: loss 1.5725, time 527.05ms, mfu 0.67%\n",
            "iter 570: loss 1.5572, time 523.95ms, mfu 0.67%\n",
            "iter 580: loss 1.5290, time 525.74ms, mfu 0.68%\n",
            "iter 590: loss 1.4969, time 526.02ms, mfu 0.68%\n",
            "iter 600: loss 1.5167, time 526.22ms, mfu 0.68%\n",
            "iter 610: loss 1.5392, time 526.67ms, mfu 0.68%\n",
            "iter 620: loss 1.5232, time 524.66ms, mfu 0.69%\n",
            "iter 630: loss 1.5087, time 526.62ms, mfu 0.69%\n",
            "iter 640: loss 1.4576, time 525.53ms, mfu 0.69%\n",
            "iter 650: loss 1.4997, time 527.05ms, mfu 0.69%\n",
            "iter 660: loss 1.5003, time 528.43ms, mfu 0.69%\n",
            "iter 670: loss 1.4480, time 524.15ms, mfu 0.70%\n",
            "iter 680: loss 1.5103, time 525.81ms, mfu 0.70%\n",
            "iter 690: loss 1.4562, time 526.37ms, mfu 0.70%\n",
            "iter 700: loss 1.4783, time 527.48ms, mfu 0.70%\n",
            "iter 710: loss 1.4566, time 526.71ms, mfu 0.70%\n",
            "iter 720: loss 1.4360, time 527.02ms, mfu 0.70%\n",
            "iter 730: loss 1.4240, time 526.34ms, mfu 0.70%\n",
            "iter 740: loss 1.4296, time 526.87ms, mfu 0.70%\n",
            "step 750: train loss 1.3589, val loss 1.5821\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4202, time 74196.27ms, mfu 0.63%\n",
            "iter 760: loss 1.4477, time 524.72ms, mfu 0.64%\n",
            "iter 770: loss 1.4173, time 525.46ms, mfu 0.65%\n",
            "iter 780: loss 1.4154, time 526.74ms, mfu 0.65%\n",
            "iter 790: loss 1.4110, time 525.37ms, mfu 0.66%\n",
            "iter 800: loss 1.4307, time 524.61ms, mfu 0.66%\n",
            "iter 810: loss 1.3969, time 525.27ms, mfu 0.67%\n",
            "iter 820: loss 1.3975, time 525.63ms, mfu 0.67%\n",
            "iter 830: loss 1.3857, time 526.79ms, mfu 0.68%\n",
            "iter 840: loss 1.3994, time 523.73ms, mfu 0.68%\n",
            "iter 850: loss 1.3845, time 525.52ms, mfu 0.68%\n",
            "iter 860: loss 1.3888, time 523.46ms, mfu 0.69%\n",
            "iter 870: loss 1.3984, time 523.62ms, mfu 0.69%\n",
            "iter 880: loss 1.3704, time 524.00ms, mfu 0.69%\n",
            "iter 890: loss 1.3829, time 526.05ms, mfu 0.69%\n",
            "iter 900: loss 1.3679, time 525.71ms, mfu 0.69%\n",
            "iter 910: loss 1.3197, time 525.01ms, mfu 0.70%\n",
            "iter 920: loss 1.3563, time 524.78ms, mfu 0.70%\n",
            "iter 930: loss 1.3538, time 524.18ms, mfu 0.70%\n",
            "iter 940: loss 1.3435, time 524.83ms, mfu 0.70%\n",
            "iter 950: loss 1.3506, time 527.28ms, mfu 0.70%\n",
            "iter 960: loss 1.3548, time 524.17ms, mfu 0.70%\n",
            "iter 970: loss 1.3604, time 526.72ms, mfu 0.70%\n",
            "iter 980: loss 1.3456, time 526.15ms, mfu 0.70%\n",
            "iter 990: loss 1.3299, time 524.73ms, mfu 0.70%\n",
            "step 1000: train loss 1.2687, val loss 1.5201\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3343, time 74176.82ms, mfu 0.63%\n",
            "iter 1010: loss 1.3305, time 523.85ms, mfu 0.64%\n",
            "iter 1020: loss 1.3057, time 527.03ms, mfu 0.65%\n",
            "iter 1030: loss 1.3346, time 524.03ms, mfu 0.65%\n",
            "iter 1040: loss 1.3546, time 525.22ms, mfu 0.66%\n",
            "iter 1050: loss 1.2960, time 527.38ms, mfu 0.66%\n",
            "iter 1060: loss 1.3263, time 523.89ms, mfu 0.67%\n",
            "iter 1070: loss 1.3332, time 528.33ms, mfu 0.67%\n",
            "iter 1080: loss 1.3237, time 525.56ms, mfu 0.68%\n",
            "iter 1090: loss 1.3447, time 526.82ms, mfu 0.68%\n",
            "iter 1100: loss 1.3057, time 526.77ms, mfu 0.68%\n",
            "iter 1110: loss 1.2993, time 525.08ms, mfu 0.68%\n",
            "iter 1120: loss 1.2946, time 525.56ms, mfu 0.69%\n",
            "iter 1130: loss 1.2942, time 525.24ms, mfu 0.69%\n",
            "iter 1140: loss 1.2965, time 525.83ms, mfu 0.69%\n",
            "iter 1150: loss 1.2973, time 526.69ms, mfu 0.69%\n",
            "iter 1160: loss 1.3265, time 524.95ms, mfu 0.69%\n",
            "iter 1170: loss 1.2928, time 525.08ms, mfu 0.70%\n",
            "iter 1180: loss 1.3157, time 523.89ms, mfu 0.70%\n",
            "iter 1190: loss 1.2635, time 526.30ms, mfu 0.70%\n",
            "iter 1200: loss 1.2916, time 525.44ms, mfu 0.70%\n",
            "iter 1210: loss 1.2611, time 524.76ms, mfu 0.70%\n",
            "iter 1220: loss 1.3037, time 524.39ms, mfu 0.70%\n",
            "iter 1230: loss 1.2886, time 525.09ms, mfu 0.70%\n",
            "iter 1240: loss 1.2967, time 525.58ms, mfu 0.70%\n",
            "step 1250: train loss 1.1990, val loss 1.4919\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.2733, time 73986.58ms, mfu 0.63%\n",
            "iter 1260: loss 1.2834, time 520.80ms, mfu 0.64%\n",
            "iter 1270: loss 1.2616, time 527.96ms, mfu 0.65%\n",
            "iter 1280: loss 1.2564, time 524.87ms, mfu 0.65%\n",
            "iter 1290: loss 1.2867, time 523.04ms, mfu 0.66%\n",
            "iter 1300: loss 1.2957, time 523.28ms, mfu 0.67%\n",
            "iter 1310: loss 1.2354, time 525.97ms, mfu 0.67%\n",
            "iter 1320: loss 1.2991, time 524.95ms, mfu 0.67%\n",
            "iter 1330: loss 1.2670, time 524.54ms, mfu 0.68%\n",
            "iter 1340: loss 1.2892, time 525.17ms, mfu 0.68%\n",
            "iter 1350: loss 1.2520, time 524.61ms, mfu 0.68%\n",
            "iter 1360: loss 1.2656, time 524.02ms, mfu 0.69%\n",
            "iter 1370: loss 1.2477, time 524.43ms, mfu 0.69%\n",
            "iter 1380: loss 1.2533, time 526.22ms, mfu 0.69%\n",
            "iter 1390: loss 1.2501, time 522.42ms, mfu 0.69%\n",
            "iter 1400: loss 1.2616, time 524.01ms, mfu 0.69%\n",
            "iter 1410: loss 1.2447, time 524.86ms, mfu 0.70%\n",
            "iter 1420: loss 1.2651, time 524.89ms, mfu 0.70%\n",
            "iter 1430: loss 1.2376, time 526.76ms, mfu 0.70%\n",
            "iter 1440: loss 1.2493, time 525.63ms, mfu 0.70%\n",
            "iter 1450: loss 1.2297, time 527.26ms, mfu 0.70%\n",
            "iter 1460: loss 1.2337, time 524.88ms, mfu 0.70%\n",
            "iter 1470: loss 1.2166, time 526.87ms, mfu 0.70%\n",
            "iter 1480: loss 1.1946, time 526.08ms, mfu 0.70%\n",
            "iter 1490: loss 1.2291, time 524.16ms, mfu 0.70%\n",
            "step 1500: train loss 1.1494, val loss 1.4741\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.1855, time 74345.61ms, mfu 0.63%\n",
            "iter 1510: loss 1.2338, time 524.25ms, mfu 0.64%\n",
            "iter 1520: loss 1.2177, time 526.14ms, mfu 0.65%\n",
            "iter 1530: loss 1.2555, time 525.69ms, mfu 0.65%\n",
            "iter 1540: loss 1.1932, time 526.17ms, mfu 0.66%\n",
            "iter 1550: loss 1.2254, time 526.07ms, mfu 0.66%\n",
            "iter 1560: loss 1.2024, time 523.32ms, mfu 0.67%\n",
            "iter 1570: loss 1.2342, time 524.12ms, mfu 0.67%\n",
            "iter 1580: loss 1.2034, time 524.75ms, mfu 0.68%\n",
            "iter 1590: loss 1.1916, time 524.59ms, mfu 0.68%\n",
            "iter 1600: loss 1.1945, time 525.30ms, mfu 0.68%\n",
            "iter 1610: loss 1.2329, time 525.67ms, mfu 0.69%\n",
            "iter 1620: loss 1.1778, time 526.93ms, mfu 0.69%\n",
            "iter 1630: loss 1.2045, time 526.69ms, mfu 0.69%\n",
            "iter 1640: loss 1.1985, time 526.01ms, mfu 0.69%\n",
            "iter 1650: loss 1.1812, time 526.23ms, mfu 0.69%\n",
            "iter 1660: loss 1.2083, time 524.87ms, mfu 0.70%\n",
            "iter 1670: loss 1.1921, time 528.49ms, mfu 0.70%\n",
            "iter 1680: loss 1.1988, time 529.07ms, mfu 0.70%\n",
            "iter 1690: loss 1.1953, time 527.25ms, mfu 0.70%\n",
            "iter 1700: loss 1.1820, time 526.86ms, mfu 0.70%\n",
            "iter 1710: loss 1.1722, time 525.73ms, mfu 0.70%\n",
            "iter 1720: loss 1.1767, time 525.59ms, mfu 0.70%\n",
            "iter 1730: loss 1.1980, time 527.15ms, mfu 0.70%\n",
            "iter 1740: loss 1.1674, time 527.11ms, mfu 0.70%\n",
            "step 1750: train loss 1.0990, val loss 1.4705\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.1859, time 74071.54ms, mfu 0.63%\n",
            "iter 1760: loss 1.1873, time 522.57ms, mfu 0.64%\n",
            "iter 1770: loss 1.1911, time 526.37ms, mfu 0.65%\n",
            "iter 1780: loss 1.1907, time 525.42ms, mfu 0.65%\n",
            "iter 1790: loss 1.1901, time 525.10ms, mfu 0.66%\n",
            "iter 1800: loss 1.1767, time 525.58ms, mfu 0.66%\n",
            "iter 1810: loss 1.1570, time 524.81ms, mfu 0.67%\n",
            "iter 1820: loss 1.1608, time 524.08ms, mfu 0.67%\n",
            "iter 1830: loss 1.1670, time 528.73ms, mfu 0.68%\n",
            "iter 1840: loss 1.1602, time 525.78ms, mfu 0.68%\n",
            "iter 1850: loss 1.1560, time 525.43ms, mfu 0.68%\n",
            "iter 1860: loss 1.1715, time 526.06ms, mfu 0.68%\n",
            "iter 1870: loss 1.1400, time 525.26ms, mfu 0.69%\n",
            "iter 1880: loss 1.1808, time 527.20ms, mfu 0.69%\n",
            "iter 1890: loss 1.1752, time 526.17ms, mfu 0.69%\n",
            "iter 1900: loss 1.1275, time 524.02ms, mfu 0.69%\n",
            "iter 1910: loss 1.1566, time 526.52ms, mfu 0.69%\n",
            "iter 1920: loss 1.1654, time 526.86ms, mfu 0.70%\n",
            "iter 1930: loss 1.1471, time 529.13ms, mfu 0.70%\n",
            "iter 1940: loss 1.1217, time 528.61ms, mfu 0.70%\n",
            "iter 1950: loss 1.1356, time 527.72ms, mfu 0.70%\n",
            "iter 1960: loss 1.1453, time 525.67ms, mfu 0.70%\n",
            "iter 1970: loss 1.1553, time 526.02ms, mfu 0.70%\n",
            "iter 1980: loss 1.1478, time 523.98ms, mfu 0.70%\n",
            "iter 1990: loss 1.1488, time 525.27ms, mfu 0.70%\n",
            "step 2000: train loss 1.0493, val loss 1.4818\n",
            "iter 2000: loss 1.1155, time 73695.33ms, mfu 0.63%\n",
            "iter 2010: loss 1.1198, time 526.49ms, mfu 0.64%\n",
            "iter 2020: loss 1.1244, time 524.21ms, mfu 0.65%\n",
            "iter 2030: loss 1.1473, time 525.43ms, mfu 0.65%\n",
            "iter 2040: loss 1.1323, time 525.91ms, mfu 0.66%\n",
            "iter 2050: loss 1.1070, time 525.14ms, mfu 0.66%\n",
            "iter 2060: loss 1.0901, time 525.05ms, mfu 0.67%\n",
            "iter 2070: loss 1.1268, time 524.25ms, mfu 0.67%\n",
            "iter 2080: loss 1.1184, time 524.28ms, mfu 0.68%\n",
            "iter 2090: loss 1.1211, time 524.98ms, mfu 0.68%\n",
            "iter 2100: loss 1.1270, time 525.31ms, mfu 0.68%\n",
            "iter 2110: loss 1.1305, time 526.55ms, mfu 0.69%\n",
            "iter 2120: loss 1.1219, time 526.05ms, mfu 0.69%\n",
            "iter 2130: loss 1.1314, time 524.94ms, mfu 0.69%\n",
            "iter 2140: loss 1.1378, time 525.53ms, mfu 0.69%\n",
            "iter 2150: loss 1.1143, time 526.06ms, mfu 0.69%\n",
            "iter 2160: loss 1.1376, time 526.56ms, mfu 0.69%\n",
            "iter 2170: loss 1.1281, time 526.62ms, mfu 0.70%\n",
            "iter 2180: loss 1.1066, time 524.71ms, mfu 0.70%\n",
            "iter 2190: loss 1.1071, time 525.18ms, mfu 0.70%\n",
            "iter 2200: loss 1.1255, time 525.20ms, mfu 0.70%\n",
            "iter 2210: loss 1.1070, time 525.54ms, mfu 0.70%\n",
            "iter 2220: loss 1.1222, time 526.71ms, mfu 0.70%\n",
            "iter 2230: loss 1.1083, time 526.40ms, mfu 0.70%\n",
            "iter 2240: loss 1.1212, time 527.71ms, mfu 0.70%\n",
            "step 2250: train loss 1.0027, val loss 1.4901\n",
            "iter 2250: loss 1.0989, time 73983.20ms, mfu 0.63%\n",
            "iter 2260: loss 1.0984, time 527.23ms, mfu 0.64%\n",
            "iter 2270: loss 1.1167, time 523.91ms, mfu 0.65%\n",
            "iter 2280: loss 1.0944, time 524.71ms, mfu 0.65%\n",
            "iter 2290: loss 1.1440, time 527.55ms, mfu 0.66%\n",
            "iter 2300: loss 1.1220, time 526.87ms, mfu 0.66%\n",
            "iter 2310: loss 1.0888, time 526.80ms, mfu 0.67%\n",
            "iter 2320: loss 1.0899, time 524.83ms, mfu 0.67%\n",
            "iter 2330: loss 1.0965, time 526.03ms, mfu 0.68%\n",
            "iter 2340: loss 1.1108, time 524.74ms, mfu 0.68%\n",
            "iter 2350: loss 1.0973, time 525.97ms, mfu 0.68%\n",
            "iter 2360: loss 1.1015, time 525.20ms, mfu 0.68%\n",
            "iter 2370: loss 1.0843, time 524.39ms, mfu 0.69%\n",
            "iter 2380: loss 1.0822, time 523.86ms, mfu 0.69%\n",
            "iter 2390: loss 1.0779, time 527.66ms, mfu 0.69%\n",
            "iter 2400: loss 1.0765, time 525.47ms, mfu 0.69%\n",
            "iter 2410: loss 1.0675, time 525.26ms, mfu 0.69%\n",
            "iter 2420: loss 1.0763, time 527.42ms, mfu 0.70%\n",
            "iter 2430: loss 1.0489, time 525.09ms, mfu 0.70%\n",
            "iter 2440: loss 1.0573, time 526.78ms, mfu 0.70%\n",
            "iter 2450: loss 1.0644, time 524.67ms, mfu 0.70%\n",
            "iter 2460: loss 1.0838, time 523.29ms, mfu 0.70%\n",
            "iter 2470: loss 1.0840, time 522.92ms, mfu 0.70%\n",
            "iter 2480: loss 1.0781, time 526.45ms, mfu 0.70%\n",
            "iter 2490: loss 1.0550, time 526.39ms, mfu 0.70%\n",
            "step 2500: train loss 0.9557, val loss 1.5074\n",
            "iter 2500: loss 1.0775, time 73771.31ms, mfu 0.63%\n",
            "iter 2510: loss 1.0661, time 525.17ms, mfu 0.64%\n",
            "iter 2520: loss 1.0466, time 525.27ms, mfu 0.65%\n",
            "iter 2530: loss 1.0442, time 525.26ms, mfu 0.65%\n",
            "iter 2540: loss 1.0540, time 526.69ms, mfu 0.66%\n",
            "iter 2550: loss 1.0630, time 523.36ms, mfu 0.66%\n",
            "iter 2560: loss 1.0529, time 523.97ms, mfu 0.67%\n",
            "iter 2570: loss 1.0755, time 523.41ms, mfu 0.67%\n",
            "iter 2580: loss 1.0714, time 527.83ms, mfu 0.68%\n",
            "iter 2590: loss 1.0690, time 523.98ms, mfu 0.68%\n",
            "iter 2600: loss 1.0684, time 523.26ms, mfu 0.68%\n",
            "iter 2610: loss 1.0475, time 526.58ms, mfu 0.69%\n",
            "iter 2620: loss 1.0369, time 525.46ms, mfu 0.69%\n",
            "iter 2630: loss 1.0194, time 524.11ms, mfu 0.69%\n",
            "iter 2640: loss 1.0428, time 524.33ms, mfu 0.69%\n",
            "iter 2650: loss 1.0631, time 526.56ms, mfu 0.69%\n",
            "iter 2660: loss 1.0361, time 527.15ms, mfu 0.70%\n",
            "iter 2670: loss 1.0107, time 523.54ms, mfu 0.70%\n",
            "iter 2680: loss 1.0457, time 526.28ms, mfu 0.70%\n",
            "iter 2690: loss 1.0492, time 525.72ms, mfu 0.70%\n",
            "iter 2700: loss 1.0196, time 524.78ms, mfu 0.70%\n",
            "iter 2710: loss 1.0398, time 526.35ms, mfu 0.70%\n",
            "iter 2720: loss 1.0363, time 525.39ms, mfu 0.70%\n",
            "iter 2730: loss 1.0591, time 526.37ms, mfu 0.70%\n",
            "iter 2740: loss 1.0169, time 526.25ms, mfu 0.70%\n",
            "step 2750: train loss 0.9097, val loss 1.5208\n",
            "iter 2750: loss 1.0375, time 73988.27ms, mfu 0.63%\n",
            "iter 2760: loss 1.0256, time 526.59ms, mfu 0.64%\n",
            "iter 2770: loss 1.0220, time 525.17ms, mfu 0.65%\n",
            "iter 2780: loss 1.0169, time 526.19ms, mfu 0.65%\n",
            "iter 2790: loss 1.0286, time 527.48ms, mfu 0.66%\n",
            "iter 2800: loss 1.0037, time 525.68ms, mfu 0.66%\n",
            "iter 2810: loss 1.0322, time 525.76ms, mfu 0.67%\n",
            "iter 2820: loss 1.0181, time 526.15ms, mfu 0.67%\n",
            "iter 2830: loss 1.0283, time 526.57ms, mfu 0.68%\n",
            "iter 2840: loss 0.9900, time 525.60ms, mfu 0.68%\n",
            "iter 2850: loss 1.0205, time 525.17ms, mfu 0.68%\n",
            "iter 2860: loss 1.0143, time 525.01ms, mfu 0.68%\n",
            "iter 2870: loss 1.0012, time 524.54ms, mfu 0.69%\n",
            "iter 2880: loss 1.0321, time 525.11ms, mfu 0.69%\n",
            "iter 2890: loss 0.9956, time 525.01ms, mfu 0.69%\n",
            "iter 2900: loss 0.9851, time 526.95ms, mfu 0.69%\n",
            "iter 2910: loss 1.0295, time 525.12ms, mfu 0.69%\n",
            "iter 2920: loss 1.0025, time 525.17ms, mfu 0.70%\n",
            "iter 2930: loss 0.9942, time 524.99ms, mfu 0.70%\n",
            "iter 2940: loss 0.9912, time 525.00ms, mfu 0.70%\n",
            "iter 2950: loss 1.0205, time 524.79ms, mfu 0.70%\n",
            "iter 2960: loss 0.9979, time 525.59ms, mfu 0.70%\n",
            "iter 2970: loss 0.9891, time 525.21ms, mfu 0.70%\n",
            "iter 2980: loss 0.9921, time 525.45ms, mfu 0.70%\n",
            "iter 2990: loss 0.9717, time 526.00ms, mfu 0.70%\n",
            "step 3000: train loss 0.8618, val loss 1.5341\n",
            "iter 3000: loss 0.9800, time 73777.09ms, mfu 0.63%\n",
            "iter 3010: loss 0.9913, time 527.77ms, mfu 0.64%\n",
            "iter 3020: loss 0.9967, time 525.02ms, mfu 0.65%\n",
            "iter 3030: loss 0.9937, time 524.80ms, mfu 0.65%\n",
            "iter 3040: loss 1.0280, time 523.83ms, mfu 0.66%\n",
            "iter 3050: loss 0.9739, time 524.79ms, mfu 0.66%\n",
            "iter 3060: loss 0.9872, time 525.98ms, mfu 0.67%\n",
            "iter 3070: loss 1.0154, time 525.48ms, mfu 0.67%\n",
            "iter 3080: loss 0.9873, time 524.37ms, mfu 0.68%\n",
            "iter 3090: loss 0.9751, time 527.08ms, mfu 0.68%\n",
            "iter 3100: loss 0.9940, time 525.47ms, mfu 0.68%\n",
            "iter 3110: loss 0.9709, time 523.97ms, mfu 0.69%\n",
            "iter 3120: loss 0.9976, time 523.53ms, mfu 0.69%\n",
            "iter 3130: loss 0.9746, time 526.48ms, mfu 0.69%\n",
            "iter 3140: loss 0.9798, time 525.82ms, mfu 0.69%\n",
            "iter 3150: loss 0.9858, time 525.24ms, mfu 0.69%\n",
            "iter 3160: loss 1.0057, time 524.65ms, mfu 0.70%\n",
            "iter 3170: loss 0.9601, time 525.94ms, mfu 0.70%\n",
            "iter 3180: loss 0.9699, time 525.42ms, mfu 0.70%\n",
            "iter 3190: loss 0.9954, time 525.70ms, mfu 0.70%\n",
            "iter 3200: loss 0.9666, time 525.00ms, mfu 0.70%\n",
            "iter 3210: loss 0.9584, time 525.29ms, mfu 0.70%\n",
            "iter 3220: loss 0.9554, time 525.73ms, mfu 0.70%\n",
            "iter 3230: loss 0.9487, time 527.10ms, mfu 0.70%\n",
            "iter 3240: loss 0.9499, time 521.99ms, mfu 0.70%\n",
            "step 3250: train loss 0.8142, val loss 1.5709\n",
            "iter 3250: loss 0.9708, time 73875.28ms, mfu 0.63%\n",
            "iter 3260: loss 0.9576, time 525.69ms, mfu 0.64%\n",
            "iter 3270: loss 0.9754, time 526.32ms, mfu 0.65%\n",
            "iter 3280: loss 0.9353, time 528.55ms, mfu 0.65%\n",
            "iter 3290: loss 0.9399, time 524.87ms, mfu 0.66%\n",
            "iter 3300: loss 0.9335, time 526.66ms, mfu 0.66%\n",
            "iter 3310: loss 0.9536, time 526.81ms, mfu 0.67%\n",
            "iter 3320: loss 0.9629, time 525.24ms, mfu 0.67%\n",
            "iter 3330: loss 0.9548, time 527.83ms, mfu 0.68%\n",
            "iter 3340: loss 0.9425, time 525.19ms, mfu 0.68%\n",
            "iter 3350: loss 0.9480, time 528.12ms, mfu 0.68%\n",
            "iter 3360: loss 0.9198, time 525.49ms, mfu 0.68%\n",
            "iter 3370: loss 0.9558, time 527.75ms, mfu 0.69%\n",
            "iter 3380: loss 0.9444, time 527.01ms, mfu 0.69%\n",
            "iter 3390: loss 0.9436, time 527.21ms, mfu 0.69%\n",
            "iter 3400: loss 0.9513, time 525.17ms, mfu 0.69%\n",
            "iter 3410: loss 0.9423, time 525.77ms, mfu 0.69%\n",
            "iter 3420: loss 0.9448, time 526.07ms, mfu 0.70%\n",
            "iter 3430: loss 0.9421, time 526.31ms, mfu 0.70%\n",
            "iter 3440: loss 0.9767, time 527.07ms, mfu 0.70%\n",
            "iter 3450: loss 0.9555, time 526.03ms, mfu 0.70%\n",
            "iter 3460: loss 0.9385, time 525.86ms, mfu 0.70%\n",
            "iter 3470: loss 0.9371, time 527.09ms, mfu 0.70%\n",
            "iter 3480: loss 0.9451, time 524.26ms, mfu 0.70%\n",
            "iter 3490: loss 0.9175, time 524.87ms, mfu 0.70%\n",
            "step 3500: train loss 0.7747, val loss 1.5794\n",
            "iter 3500: loss 0.9072, time 73951.94ms, mfu 0.63%\n",
            "iter 3510: loss 0.9035, time 526.49ms, mfu 0.64%\n",
            "iter 3520: loss 0.9186, time 526.65ms, mfu 0.65%\n",
            "iter 3530: loss 0.9460, time 523.21ms, mfu 0.65%\n",
            "iter 3540: loss 0.9206, time 525.64ms, mfu 0.66%\n",
            "iter 3550: loss 0.9219, time 525.04ms, mfu 0.66%\n",
            "iter 3560: loss 0.9466, time 522.13ms, mfu 0.67%\n",
            "iter 3570: loss 0.9296, time 524.82ms, mfu 0.67%\n",
            "iter 3580: loss 0.9254, time 524.62ms, mfu 0.68%\n",
            "iter 3590: loss 0.9125, time 526.13ms, mfu 0.68%\n",
            "iter 3600: loss 0.9189, time 527.43ms, mfu 0.68%\n",
            "iter 3610: loss 0.9138, time 524.11ms, mfu 0.69%\n",
            "iter 3620: loss 0.9011, time 526.55ms, mfu 0.69%\n",
            "iter 3630: loss 0.9196, time 526.51ms, mfu 0.69%\n",
            "iter 3640: loss 0.9115, time 524.81ms, mfu 0.69%\n",
            "iter 3650: loss 0.9096, time 527.70ms, mfu 0.69%\n",
            "iter 3660: loss 0.9288, time 525.32ms, mfu 0.69%\n",
            "iter 3670: loss 0.9276, time 526.14ms, mfu 0.70%\n",
            "iter 3680: loss 0.9008, time 525.82ms, mfu 0.70%\n",
            "iter 3690: loss 0.9350, time 525.78ms, mfu 0.70%\n",
            "iter 3700: loss 0.8705, time 525.02ms, mfu 0.70%\n",
            "iter 3710: loss 0.8778, time 525.74ms, mfu 0.70%\n",
            "iter 3720: loss 0.8982, time 523.33ms, mfu 0.70%\n",
            "iter 3730: loss 0.9037, time 524.38ms, mfu 0.70%\n",
            "iter 3740: loss 0.8882, time 525.70ms, mfu 0.70%\n",
            "step 3750: train loss 0.7361, val loss 1.6081\n",
            "iter 3750: loss 0.8902, time 73841.46ms, mfu 0.63%\n",
            "iter 3760: loss 0.9322, time 522.57ms, mfu 0.64%\n",
            "iter 3770: loss 0.9235, time 526.94ms, mfu 0.65%\n",
            "iter 3780: loss 0.9173, time 526.12ms, mfu 0.65%\n",
            "iter 3790: loss 0.8959, time 524.33ms, mfu 0.66%\n",
            "iter 3800: loss 0.9113, time 524.00ms, mfu 0.66%\n",
            "iter 3810: loss 0.9175, time 526.08ms, mfu 0.67%\n",
            "iter 3820: loss 0.8834, time 526.27ms, mfu 0.67%\n",
            "iter 3830: loss 0.9055, time 522.96ms, mfu 0.68%\n",
            "iter 3840: loss 0.8957, time 524.84ms, mfu 0.68%\n",
            "iter 3850: loss 0.8882, time 527.23ms, mfu 0.68%\n",
            "iter 3860: loss 0.8745, time 526.40ms, mfu 0.69%\n",
            "iter 3870: loss 0.8837, time 524.61ms, mfu 0.69%\n",
            "iter 3880: loss 0.8883, time 526.04ms, mfu 0.69%\n",
            "iter 3890: loss 0.8932, time 525.06ms, mfu 0.69%\n",
            "iter 3900: loss 0.8970, time 525.16ms, mfu 0.69%\n",
            "iter 3910: loss 0.8866, time 526.52ms, mfu 0.70%\n",
            "iter 3920: loss 0.8655, time 525.48ms, mfu 0.70%\n",
            "iter 3930: loss 0.8856, time 524.23ms, mfu 0.70%\n",
            "iter 3940: loss 0.8707, time 525.47ms, mfu 0.70%\n",
            "iter 3950: loss 0.8690, time 524.78ms, mfu 0.70%\n",
            "iter 3960: loss 0.9024, time 525.36ms, mfu 0.70%\n",
            "iter 3970: loss 0.8915, time 526.68ms, mfu 0.70%\n",
            "iter 3980: loss 0.9024, time 524.18ms, mfu 0.70%\n",
            "iter 3990: loss 0.8787, time 524.89ms, mfu 0.70%\n",
            "step 4000: train loss 0.7038, val loss 1.6325\n",
            "iter 4000: loss 0.8541, time 73771.88ms, mfu 0.63%\n",
            "iter 4010: loss 0.8815, time 524.75ms, mfu 0.64%\n",
            "iter 4020: loss 0.8796, time 523.42ms, mfu 0.65%\n",
            "iter 4030: loss 0.8724, time 524.68ms, mfu 0.65%\n",
            "iter 4040: loss 0.8758, time 525.07ms, mfu 0.66%\n",
            "iter 4050: loss 0.8753, time 524.59ms, mfu 0.66%\n",
            "iter 4060: loss 0.8563, time 523.68ms, mfu 0.67%\n",
            "iter 4070: loss 0.8629, time 525.25ms, mfu 0.67%\n",
            "iter 4080: loss 0.8725, time 524.27ms, mfu 0.68%\n",
            "iter 4090: loss 0.8445, time 523.70ms, mfu 0.68%\n",
            "iter 4100: loss 0.8863, time 526.61ms, mfu 0.68%\n",
            "iter 4110: loss 0.8651, time 524.41ms, mfu 0.69%\n",
            "iter 4120: loss 0.8761, time 523.28ms, mfu 0.69%\n",
            "iter 4130: loss 0.8637, time 526.37ms, mfu 0.69%\n",
            "iter 4140: loss 0.8807, time 524.22ms, mfu 0.69%\n",
            "iter 4150: loss 0.8625, time 525.31ms, mfu 0.69%\n",
            "iter 4160: loss 0.8523, time 524.91ms, mfu 0.70%\n",
            "iter 4170: loss 0.8581, time 524.17ms, mfu 0.70%\n",
            "iter 4180: loss 0.8719, time 524.42ms, mfu 0.70%\n",
            "iter 4190: loss 0.8647, time 525.12ms, mfu 0.70%\n",
            "iter 4200: loss 0.8461, time 526.35ms, mfu 0.70%\n",
            "iter 4210: loss 0.8686, time 524.10ms, mfu 0.70%\n",
            "iter 4220: loss 0.8538, time 525.78ms, mfu 0.70%\n",
            "iter 4230: loss 0.8736, time 524.94ms, mfu 0.70%\n",
            "iter 4240: loss 0.8618, time 524.95ms, mfu 0.70%\n",
            "step 4250: train loss 0.6742, val loss 1.6489\n",
            "iter 4250: loss 0.8645, time 73751.74ms, mfu 0.63%\n",
            "iter 4260: loss 0.8566, time 524.67ms, mfu 0.64%\n",
            "iter 4270: loss 0.8665, time 524.76ms, mfu 0.65%\n",
            "iter 4280: loss 0.8546, time 525.75ms, mfu 0.65%\n",
            "iter 4290: loss 0.8257, time 525.80ms, mfu 0.66%\n",
            "iter 4300: loss 0.8301, time 526.80ms, mfu 0.66%\n",
            "iter 4310: loss 0.8516, time 525.87ms, mfu 0.67%\n",
            "iter 4320: loss 0.8333, time 524.26ms, mfu 0.67%\n",
            "iter 4330: loss 0.8617, time 525.12ms, mfu 0.68%\n",
            "iter 4340: loss 0.8236, time 525.98ms, mfu 0.68%\n",
            "iter 4350: loss 0.8378, time 525.77ms, mfu 0.68%\n",
            "iter 4360: loss 0.8568, time 523.81ms, mfu 0.69%\n",
            "iter 4370: loss 0.8433, time 525.88ms, mfu 0.69%\n",
            "iter 4380: loss 0.8318, time 525.12ms, mfu 0.69%\n",
            "iter 4390: loss 0.8538, time 524.49ms, mfu 0.69%\n",
            "iter 4400: loss 0.8376, time 523.96ms, mfu 0.69%\n",
            "iter 4410: loss 0.8629, time 524.81ms, mfu 0.70%\n",
            "iter 4420: loss 0.8607, time 526.41ms, mfu 0.70%\n",
            "iter 4430: loss 0.8402, time 524.68ms, mfu 0.70%\n",
            "iter 4440: loss 0.8462, time 525.34ms, mfu 0.70%\n",
            "iter 4450: loss 0.8491, time 526.16ms, mfu 0.70%\n",
            "iter 4460: loss 0.8283, time 526.51ms, mfu 0.70%\n",
            "iter 4470: loss 0.8618, time 524.91ms, mfu 0.70%\n",
            "iter 4480: loss 0.8291, time 526.89ms, mfu 0.70%\n",
            "iter 4490: loss 0.8425, time 524.21ms, mfu 0.70%\n",
            "step 4500: train loss 0.6483, val loss 1.6775\n",
            "iter 4500: loss 0.8641, time 73909.92ms, mfu 0.63%\n",
            "iter 4510: loss 0.8388, time 525.44ms, mfu 0.64%\n",
            "iter 4520: loss 0.8426, time 528.55ms, mfu 0.65%\n",
            "iter 4530: loss 0.8502, time 526.70ms, mfu 0.65%\n",
            "iter 4540: loss 0.8461, time 525.68ms, mfu 0.66%\n",
            "iter 4550: loss 0.8655, time 525.13ms, mfu 0.66%\n",
            "iter 4560: loss 0.8540, time 525.21ms, mfu 0.67%\n",
            "iter 4570: loss 0.8400, time 524.88ms, mfu 0.67%\n",
            "iter 4580: loss 0.8537, time 526.65ms, mfu 0.68%\n",
            "iter 4590: loss 0.8494, time 524.15ms, mfu 0.68%\n",
            "iter 4600: loss 0.8245, time 524.30ms, mfu 0.68%\n",
            "iter 4610: loss 0.8571, time 524.94ms, mfu 0.69%\n",
            "iter 4620: loss 0.8395, time 524.50ms, mfu 0.69%\n",
            "iter 4630: loss 0.8241, time 525.28ms, mfu 0.69%\n",
            "iter 4640: loss 0.8381, time 524.98ms, mfu 0.69%\n",
            "iter 4650: loss 0.8534, time 525.13ms, mfu 0.69%\n",
            "iter 4660: loss 0.8385, time 527.10ms, mfu 0.70%\n",
            "iter 4670: loss 0.8365, time 524.89ms, mfu 0.70%\n",
            "iter 4680: loss 0.8469, time 525.26ms, mfu 0.70%\n",
            "iter 4690: loss 0.8461, time 525.50ms, mfu 0.70%\n",
            "iter 4700: loss 0.8186, time 523.23ms, mfu 0.70%\n",
            "iter 4710: loss 0.7925, time 524.63ms, mfu 0.70%\n",
            "iter 4720: loss 0.8281, time 523.41ms, mfu 0.70%\n",
            "iter 4730: loss 0.8165, time 527.51ms, mfu 0.70%\n",
            "iter 4740: loss 0.8264, time 524.70ms, mfu 0.70%\n",
            "step 4750: train loss 0.6297, val loss 1.6956\n",
            "iter 4750: loss 0.8000, time 74018.62ms, mfu 0.63%\n",
            "iter 4760: loss 0.8189, time 528.95ms, mfu 0.64%\n",
            "iter 4770: loss 0.7960, time 527.39ms, mfu 0.65%\n",
            "iter 4780: loss 0.8116, time 524.79ms, mfu 0.65%\n",
            "iter 4790: loss 0.8311, time 524.44ms, mfu 0.66%\n",
            "iter 4800: loss 0.8128, time 523.57ms, mfu 0.66%\n",
            "iter 4810: loss 0.8283, time 526.63ms, mfu 0.67%\n",
            "iter 4820: loss 0.8209, time 525.94ms, mfu 0.67%\n",
            "iter 4830: loss 0.8222, time 525.05ms, mfu 0.68%\n",
            "iter 4840: loss 0.8354, time 523.44ms, mfu 0.68%\n",
            "iter 4850: loss 0.8176, time 525.70ms, mfu 0.68%\n",
            "iter 4860: loss 0.8110, time 525.16ms, mfu 0.69%\n",
            "iter 4870: loss 0.8119, time 525.07ms, mfu 0.69%\n",
            "iter 4880: loss 0.8324, time 523.63ms, mfu 0.69%\n",
            "iter 4890: loss 0.8048, time 523.55ms, mfu 0.69%\n",
            "iter 4900: loss 0.8046, time 522.87ms, mfu 0.69%\n",
            "iter 4910: loss 0.8267, time 523.64ms, mfu 0.70%\n",
            "iter 4920: loss 0.8204, time 527.44ms, mfu 0.70%\n",
            "iter 4930: loss 0.8047, time 524.19ms, mfu 0.70%\n",
            "iter 4940: loss 0.7952, time 524.87ms, mfu 0.70%\n",
            "iter 4950: loss 0.8191, time 525.14ms, mfu 0.70%\n",
            "iter 4960: loss 0.8304, time 527.71ms, mfu 0.70%\n",
            "iter 4970: loss 0.7845, time 526.88ms, mfu 0.70%\n",
            "iter 4980: loss 0.7966, time 523.52ms, mfu 0.70%\n",
            "iter 4990: loss 0.8248, time 525.42ms, mfu 0.70%\n",
            "step 5000: train loss 0.6159, val loss 1.7103\n",
            "iter 5000: loss 0.8199, time 73919.17ms, mfu 0.63%\n"
          ]
        }
      ],
      "source": [
        "# Inicia o treinamento!\n",
        "# Isso pode levar de 5 a 15 minutos para mostrar resultados decentes.\n",
        "# O modelo salvará 'checkpoints' na pasta 'out-shakespeare-char'\n",
        "!python train.py config/train_shakespeare_char.py"
      ],
      "id": "JyEDdI2GxaWL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onwrQZNexaWL"
      },
      "source": [
        "**Nota sobre o Treinamento:**\n",
        "\n",
        "O script padrão roda por `max_iters = 5000` iterações. Para uma demonstração, você não precisa esperar até o fim. Você pode **interromper a execução manualmente** (clicando no botão \"Stop\") quando o `val loss` (loss de validação) estiver baixo (algo em torno de 1.5 ou 1.6). O modelo já será capaz de gerar texto reconhecível."
      ],
      "id": "onwrQZNexaWL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNOb46iJxaWM"
      },
      "source": [
        "### Passo 5: Gerar Texto (A Demonstração)\n",
        "\n",
        "Após o modelo ter treinado por um tempo, ele terá salvo um *checkpoint*. Vamos usar o script `sample.py` para carregar esse checkpoint e gerar texto novo."
      ],
      "id": "cNOb46iJxaWM"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR4FfxP4xaWM",
        "outputId": "8b8cf609-f9b3-482f-a4c2-5b94dfb44540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "\n",
            "ANGELO:\n",
            "And cowards it with the present souls.\n",
            "Stirrah, we have heard the fuults of your desires,\n",
            "But were you fall out of the other to the heart\n",
            "Of whose black is ensented to the time\n",
            "Give and quietness in warms. Ah, noble Peter!\n",
            "Thou hast a childishes wonder'd and Romeo,\n",
            "Is a less present to the laoyWing to thy highness\n",
            "Without the gentle crown, and he is\n",
            "To kill thy father and great shall divise on thee\n",
            "Married with thy household obeys:\n",
            "Sad all his grace hath couragainted Rutland,\n",
            "And from t\n",
            "---------------\n",
            "\n",
            "Men pardon, marry, I will give me\n",
            "That I have sent forth and madness; and yet I will sue.\n",
            "\n",
            "ISABELLA:\n",
            "I fear, sir, I would to instruct me.\n",
            "\n",
            "ANGELO:\n",
            "I am not over the part of most grave to time:\n",
            "Here comes me that I shall speak again\n",
            "Whose looks does of the heavens.\n",
            "\n",
            "ANGELO:\n",
            "Think'st thou, why I may make the world?\n",
            "\n",
            "ISABELLA:\n",
            "Well, good my lord, we have not with thee:\n",
            "But to this outward I hear my part, I am now\n",
            "Be thought bought thee, like to the harm of Russia.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Let me leave to e\n",
            "---------------\n",
            "\n",
            "Messenger.\n",
            "\n",
            "Second Servingman:\n",
            "So hear him to be, he is a sad fall'n back.\n",
            "\n",
            "First Servingman:\n",
            "Sir, it is a most grown fellow; therefore we did\n",
            "call him and be call'd as talking: but it is comes begun of\n",
            "himself to his felling breath in the world of\n",
            "his mother.\n",
            "\n",
            "Second Servingman:\n",
            "Blessed him, thou art our best to the truth thirty of\n",
            "A man; a noble woman are but a stander'd by your first\n",
            "That to put by the power.\n",
            "\n",
            "MENENIUS:\n",
            "One more times not on the meatenant.\n",
            "Well, this is this Roman: who is wel\n",
            "---------------\n",
            "\n",
            "The sleep of your kinsmen with tears;\n",
            "And when you have feel'd to me, to prove you\n",
            "To suppliant one what you have most reap to every in\n",
            "'Twixt some more number than you, give me the senators\n",
            "And as yours, man in such as her does:\n",
            "But I know his father's corrupt chaste,\n",
            "And give only the court of your course,\n",
            "But I have but release of your household from his traitor.\n",
            "\n",
            "GREMIO:\n",
            "You have dead, mighty may marvell'd Jamillo.\n",
            "\n",
            "GRUMIO:\n",
            "I have a pair of mine own man's foot.\n",
            "\n",
            "LEONTES:\n",
            "The huntime I am alr\n",
            "---------------\n",
            "\n",
            "That lack our souls and her parts as in the freed,\n",
            "I would flatter it, undertake off disprospersed\n",
            "The senators of my heart. If my liege,\n",
            "It is not bound of your wills, to your grace\n",
            "Against the order which one than he have fited\n",
            "In presently of charge, to pack me your lambs.\n",
            "\n",
            "ANGELO:\n",
            "Ay, sir, we will have made it;\n",
            "So many our men all distractions me with her.\n",
            "\n",
            "ISABELLA:\n",
            "A gentle matter:\n",
            "What a stirribal, here's no other of this here?\n",
            "Take up a true, your jest of sleep pride; but to question\n",
            "The\n",
            "---------------\n",
            "\n",
            "\n",
            "MENENIUS:\n",
            "He's a like to see him the maid: he is a man\n",
            "a shadow, he has been destroy'd.\n",
            "\n",
            "CORIOLANUS:\n",
            "Would the very thing one that harm to understand his\n",
            "own?\n",
            "\n",
            "VIRGILIA:\n",
            "Not the man in the same innocent, nor seeing it, it\n",
            "must come your bears; there's a mistress, and this short, here\n",
            "betwixt your hands or a beggar's and man and laid\n",
            "hours, till we are yourselves; and not your lady, worth\n",
            "her husband, that he did breathe grown and send\n",
            "voices his beauty inhappy. I rather would not leave\n",
            "him in t\n",
            "---------------\n",
            "\n",
            "Shall I seem my left dispatch'd these heavy springs.\n",
            "\n",
            "ESCALUS:\n",
            "My brother, sir: it is a warrant all.\n",
            "\n",
            "ANGELO:\n",
            "I will tell your protectors with this death. I till you\n",
            "Have it your king and right, that your bodies must be yours.\n",
            "Yet not of your great care, yet your rancing marries\n",
            "For bear what is spite.\n",
            "\n",
            "ESCALUS:\n",
            "The more very walls of him. Alas, the maid\n",
            "Love the down withis excellent that against\n",
            "A word without all tears that filling that I have the others\n",
            "Which to infect the world bid my souls\n",
            "---------------\n",
            "\n",
            "ladies hand half him. And, ere't men says,\n",
            "As indeed, as if the winding, to be said,\n",
            "To make him of bowels and vile steeds again:\n",
            "Blood, best thou art not watch'd him dull;\n",
            "And yet I have declined with all of mine ears,\n",
            "To bear me flowers King Henry's son, and her stars\n",
            "Send with the stirrer of Bohemia that strength\n",
            "In the troublous vows of our dead friars\n",
            "That thou hast put the fetch'd off thy held.\n",
            "\n",
            "LEONTES:\n",
            "Wouldst thou art done! This is thus.\n",
            "\n",
            "BALTHASAR:\n",
            "Woe's that the very world the case. C\n",
            "---------------\n",
            "\n",
            "GLOUCESTER:\n",
            "Lord, good friend with foot, and the old your lord,\n",
            "To cruel vanity or foul this seems so flame\n",
            "Hath seen him poured his beauty in the sentence\n",
            "Of the sun, his hands have gates the mercy of Edward,\n",
            "In Christians for man orrow their lives so with charactions;\n",
            "Or else in him, marrying be blind, and as\n",
            "Your father die, as if they grieve the remedy.\n",
            "\n",
            "ANGELO:\n",
            "He seem'd to truth, and within my sheet\n",
            "That he have released it. Which if he was,\n",
            "Have thus. He the rulest of men, and his being w\n",
            "---------------\n",
            "\n",
            "He had been settled and reigns my hands:\n",
            "And I will have given to be true to her.\n",
            "\n",
            "EDWARD:\n",
            "Whither Gloucester of Gloucester, I do not think\n",
            "I know that contracted so far sorrow\n",
            "As thou victors are as I can do well have done\n",
            "To the royal dislike of our deserts.\n",
            "And young Polixenes!\n",
            "\n",
            "RIVERS:\n",
            "Here's Clarence, my lord! how where's Clarence?\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "He hath done to me but and with suppliance?\n",
            "And what I cannot have to thee, to have many tidings?\n",
            "\n",
            "KING RICHARD III:\n",
            "Nay, come, here counsel An\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# Gera 500 caracteres de texto novo\n",
        "# Ele automaticamente carrega o checkpoint mais recente da pasta 'out-shakespeare-char'\n",
        "!python sample.py --out_dir=out-shakespeare-char"
      ],
      "id": "LR4FfxP4xaWM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6oKfXCNxaWM"
      },
      "source": [
        "**O que observar:**\n",
        "\n",
        "O texto gerado deve se parecer com o inglês de Shakespeare. Você verá nomes de personagens (ex: `KING:`, `JULIET:`) e palavras no estilo arcaico. Isso demonstra que o modelo não *decorou* o texto, mas sim *aprendeu os padrões e o estilo* do corpus.\n",
        "\n",
        "---\n",
        "\n",
        "### (Opcional) Passo 6: Treinar com seu Próprio Corpus\n",
        "\n",
        "Se quiser treinar com um corpus diferente (ex: poemas em português):\n",
        "\n",
        "1.  No painel de arquivos à esquerda, navegue até `nanoGPT/data/`.\n",
        "2.  Crie uma nova pasta (ex: `meu_corpus`).\n",
        "3.  Faça o upload do seu arquivo de texto para essa pasta e renomeie-o para `input.txt`.\n",
        "4.  Execute os comandos abaixo."
      ],
      "id": "m6oKfXCNxaWM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlmfRoYoxaWN"
      },
      "outputs": [],
      "source": [
        "# 1. Crie o diretório (apenas para garantir)\n",
        "!mkdir -p data/meu_corpus\n",
        "\n",
        "#\n",
        "# ----------------------------------------------------------------------\n",
        "# (AGORA, FAÇA O UPLOAD DO SEU 'input.txt' PARA A PASTA 'data/meu_corpus')\n",
        "# ----------------------------------------------------------------------\n",
        "#\n",
        "\n",
        "# 2. Copie o script de preparação\n",
        "!cp data/shakespeare_char/prepare.py data/meu_corpus/prepare.py\n",
        "\n",
        "# 3. Rode a preparação (ele irá tokenizar seu 'input.txt')\n",
        "# (Descomente a linha abaixo após fazer o upload)\n",
        "#!python data/meu_corpus/prepare.py"
      ],
      "id": "SlmfRoYoxaWN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYnQqJKixaWN"
      },
      "outputs": [],
      "source": [
        "# 4. Copie o arquivo de configuração\n",
        "!cp config/train_shakespeare_char.py config/train_meu_corpus.py\n",
        "\n",
        "# 5. EDITE O NOVO ARQUIVO DE CONFIG:\n",
        "#   - Abra 'config/train_meu_corpus.py' no editor de arquivos\n",
        "#   - Mude a linha: dataset = 'shakespeare_char' PARA dataset = 'meu_corpus'\n",
        "#   - Mude a linha: out_dir = 'out-shakespeare-char' PARA out_dir = 'out-meu-corpus'\n",
        "\n",
        "# 6. TREINE com seu corpus\n",
        "# (Descomente a linha abaixo após editar o arquivo de config)\n",
        "#!python train.py config/train_meu_corpus.py"
      ],
      "id": "MYnQqJKixaWN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWCtaDefxaWN"
      },
      "outputs": [],
      "source": [
        "# 7. GERE TEXTO a partir do seu modelo\n",
        "# (Descomente a linha abaixo após o treinamento)\n",
        "#!python sample.py --out_dir=out-meu_corpus --n=500"
      ],
      "id": "GWCtaDefxaWN"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}