{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsansao/teic-20231/blob/main/TEIC_Licao31_bleu_bertscore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHOi4Qdpq35Y"
      },
      "source": [
        "# ðŸ¤– Comparando MÃ©tricas: BLEU vs. BERTScore\n",
        "\n",
        "Este notebook demonstra a diferenÃ§a fundamental entre a mÃ©trica **BLEU** (baseada em *n-grams*) e a mÃ©trica **BERTScore** (baseada em *semÃ¢ntica*).\n",
        "\n",
        "Vamos avaliar 4 sentenÃ§as \"candidatas\" contra 1 sentenÃ§a de \"referÃªncia\" para ver como cada mÃ©trica reage.\n",
        "\n",
        "### O CenÃ¡rio\n",
        "* **BLEU:** Esperamos que pontue bem apenas quando as palavras *exatas* sÃ£o usadas.\n",
        "* **BERTScore:** Esperamos que pontue bem quando o *significado* Ã© semelhante, mesmo que as palavras sejam diferentes."
      ],
      "id": "AHOi4Qdpq35Y"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baUEV4_Mq35a",
        "outputId": "818c9e30-6516-4ab6-bae0-c979804be65a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bert-score in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.8.0+cu126)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2025.10.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "# @title 1. InstalaÃ§Ã£o das Bibliotecas\n",
        "# Vamos instalar a biblioteca bert-score e a nltk (para o BLEU)\n",
        "\n",
        "!pip install bert-score\n",
        "!pip install nltk"
      ],
      "id": "baUEV4_Mq35a"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "br6JPVvhq35a"
      },
      "outputs": [],
      "source": [
        "# @title 2. ImportaÃ§Ãµes\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from bert_score import score\n",
        "import pandas as pd\n",
        "\n",
        "# Usaremos uma funÃ§Ã£o de suavizaÃ§Ã£o para o BLEU,\n",
        "# pois ele nÃ£o funciona bem em sentenÃ§as curtas sem isso.\n",
        "chen_smoothing = SmoothingFunction().method4"
      ],
      "id": "br6JPVvhq35a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxtDjSqsq35c"
      },
      "source": [
        "## 3. DefiniÃ§Ã£o das SentenÃ§as de Teste\n",
        "\n",
        "Este Ã© o nosso caso de teste. Temos uma referÃªncia (a traduÃ§Ã£o \"correta\") e quatro candidatos com caracterÃ­sticas diferentes:\n",
        "\n",
        "1.  **Candidato 1 (CorrespondÃªncia Exata):** IdÃªntico Ã  referÃªncia.\n",
        "2.  **Candidato 2 (ParÃ¡frase/SemÃ¢ntico):** Significado idÃªntico, mas com sinÃ´nimos.\n",
        "3.  **Candidato 3 (SobreposiÃ§Ã£o, Sentido Oposto):** Usa as mesmas palavras, mas o significado Ã© o oposto.\n",
        "4.  **Candidato 4 (Diferente):** Uma sentenÃ§a completamente aleatÃ³ria."
      ],
      "id": "SxtDjSqsq35c"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JH0bW4u1q35c"
      },
      "outputs": [],
      "source": [
        "# @title 4. Nossas sentenÃ§as (em PortuguÃªs)\n",
        "\n",
        "# A sentenÃ§a que consideramos \"correta\"\n",
        "referencia = \"O gato rÃ¡pido pulou sobre o cachorro preguiÃ§oso.\"\n",
        "\n",
        "# Lista de sentenÃ§as geradas para comparar\n",
        "candidatos = [\n",
        "    # 1. CorrespondÃªncia Exata\n",
        "    \"O gato rÃ¡pido pulou sobre o cachorro preguiÃ§oso.\",\n",
        "\n",
        "    # 2. ParÃ¡frase (SinÃ´nimos, mesmo significado)\n",
        "    \"O felino Ã¡gil saltou por cima do cÃ£o sonolento.\",\n",
        "\n",
        "    # 3. SobreposiÃ§Ã£o de palavras (Significado oposto)\n",
        "    \"O cachorro preguiÃ§oso pulou sobre o gato rÃ¡pido.\",\n",
        "\n",
        "    # 4. Totalmente diferente\n",
        "    \"O cÃ©u estÃ¡ azul hoje.\"\n",
        "]\n",
        "\n",
        "# Vamos tokenizar (dividir as palavras) para o BLEU\n",
        "ref_tokenized = [referencia.split()] # BLEU espera uma lista de referÃªncias\n",
        "cands_tokenized = [c.split() for c in candidatos]\n",
        "\n",
        "# BERTScore usa as sentenÃ§as brutas (strings)\n",
        "# Precisamos de uma lista de referÃªncias com o mesmo tamanho da lista de candidatos\n",
        "refs_bert = [referencia] * len(candidatos)"
      ],
      "id": "JH0bW4u1q35c"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0JgTm7uUq35c"
      },
      "outputs": [],
      "source": [
        "# @title 5. CÃ¡lculo das MÃ©tricas\n",
        "\n",
        "# ----- 5.1. Calcular o BLEU -----\n",
        "# BLEU Ã© uma mÃ©trica a nÃ­vel de corpus, mas podemos usÃ¡-la\n",
        "# a nÃ­vel de sentenÃ§a com suavizaÃ§Ã£o.\n",
        "scores_bleu = []\n",
        "for cand in cands_tokenized:\n",
        "    score_b = sentence_bleu(ref_tokenized, cand, smoothing_function=chen_smoothing)\n",
        "    scores_bleu.append(score_b)\n",
        "\n",
        "\n",
        "# ----- 5.2. Calcular o BERTScore -----\n",
        "# Usaremos um modelo BERT treinado para o portuguÃªs para melhores resultados\n",
        "# lang=\"pt\" automaticamente usa \"neuralmind/bert-base-portuguese-cased\"\n",
        "P, R, F1_bert = score(candidatos, refs_bert, lang=\"pt\", verbose=False)\n",
        "\n",
        "# Vamos extrair apenas o F1-score (a mÃ©trica principal)\n",
        "scores_bert = F1_bert.tolist()"
      ],
      "id": "0JgTm7uUq35c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "811jDlKtq35d"
      },
      "source": [
        "## 6. AnÃ¡lise dos Resultados\n",
        "\n",
        "Vamos colocar tudo em uma tabela para comparar lado a lado.\n",
        "\n",
        "* **Score_BLEU:** Varia de 0.0 a 1.0. Foca na sobreposiÃ§Ã£o de palavras.\n",
        "* **Score_BERT (F1):** Varia (aprox.) de 0.0 a 1.0. Foca na similaridade de significado."
      ],
      "id": "811jDlKtq35d"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtmBniYlq35d",
        "outputId": "06257b73-4f41-4840-c79d-83db4d21cfa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReferÃªncia: O gato rÃ¡pido pulou sobre o cachorro preguiÃ§oso.\n",
            "--------------------------------------------------\n",
            "| Candidato                | SentenÃ§a                                         |   Score_BLEU |   Score_BERT (F1) |\n",
            "|:-------------------------|:-------------------------------------------------|-------------:|------------------:|\n",
            "| 1. Exato                 | O gato rÃ¡pido pulou sobre o cachorro preguiÃ§oso. |       1      |            1      |\n",
            "| 2. ParÃ¡frase (SinÃ´nimos) | O felino Ã¡gil saltou por cima do cÃ£o sonolento.  |       0.0257 |            0.836  |\n",
            "| 3. Oposto (SobreposiÃ§Ã£o) | O cachorro preguiÃ§oso pulou sobre o gato rÃ¡pido. |       0.1963 |            0.9605 |\n",
            "| 4. Diferente             | O cÃ©u estÃ¡ azul hoje.                            |       0.0251 |            0.6774 |\n"
          ]
        }
      ],
      "source": [
        "# @title 7. Tabela Comparativa\n",
        "\n",
        "data = {\n",
        "    \"Candidato\": [\n",
        "        \"1. Exato\",\n",
        "        \"2. ParÃ¡frase (SinÃ´nimos)\",\n",
        "        \"3. Oposto (SobreposiÃ§Ã£o)\",\n",
        "        \"4. Diferente\"\n",
        "    ],\n",
        "    \"SentenÃ§a\": candidatos,\n",
        "    \"Score_BLEU\": scores_bleu,\n",
        "    \"Score_BERT (F1)\": scores_bert\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Vamos formatar os nÃºmeros para melhor leitura\n",
        "df[\"Score_BLEU\"] = df[\"Score_BLEU\"].round(4)\n",
        "df[\"Score_BERT (F1)\"] = df[\"Score_BERT (F1)\"].round(4)\n",
        "\n",
        "print(\"ReferÃªncia:\", referencia)\n",
        "print(\"-\" * 50)\n",
        "print(df.to_markdown(index=False))"
      ],
      "id": "rtmBniYlq35d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHq0dCT2q35e"
      },
      "source": [
        "## 8. ConclusÃ£o da AnÃ¡lise\n",
        "\n",
        "Veja o que a tabela nos mostra:\n",
        "\n",
        "1.  **Candidato 1 (Exato):**\n",
        "    * **BLEU:** 1.00 (Perfeito).\n",
        "    * **BERTScore:** 1.00 (Perfeito).\n",
        "    * *Ambas as mÃ©tricas concordam que uma correspondÃªncia exata Ã© perfeita.*\n",
        "\n",
        "2.  **Candidato 2 (ParÃ¡frase/SinÃ´nimos):**\n",
        "    * **BLEU:** ~0.0 (Muito baixo). NÃ£o hÃ¡ quase nenhuma palavra em comum (apenas \"O\" e \"do\").\n",
        "    * **BERTScore:** ~0.84 (Muito alto). O BERT entende que \"gato\" Ã© sinÃ´nimo de \"felino\", \"rÃ¡pido\" de \"Ã¡gil\", \"pulou\" de \"saltou\", etc.\n",
        "    * *Esta Ã© a maior falha do BLEU e a maior forÃ§a do BERTScore.*\n",
        "\n",
        "3.  **Candidato 3 (Oposto):**\n",
        "    * **BLEU:** ~0.20 (MÃ©dio-Alto). O BLEU vÃª que *todas* as palavras corretas estÃ£o presentes, ele sÃ³ nÃ£o entende a ordem e, portanto, a mudanÃ§a de significado (quem pulou em quem).\n",
        "    * **BERTScore:** ~0.96 (Alto). O BERTScore tambÃ©m Ã© \"enganado\" pela alta sobreposiÃ§Ã£o de palavras, mas geralmente dÃ¡ uma pontuaÃ§Ã£o ligeiramente menor que uma parÃ¡frase correta, pois o contexto dos embeddings muda.\n",
        "    * *(Nota: A robustez do BERTScore a mudanÃ§as de ordem pode variar com o modelo).*\n",
        "\n",
        "4.  **Candidato 4 (Diferente):**\n",
        "    * **BLEU:** ~0.0 (Muito baixo).\n",
        "    * **BERTScore:** ~0.67 (Baixo).\n",
        "    * *Ambas as mÃ©tricas concordam que esta Ã© uma pÃ©ssima correspondÃªncia, mas o BERTScore raramente dÃ¡ 0.0, pois sempre hÃ¡ alguma similaridade semÃ¢ntica residual (palavras como \"O\", \"estÃ¡\", etc.).\n",
        "  "
      ],
      "id": "tHq0dCT2q35e"
    }
  ]
}