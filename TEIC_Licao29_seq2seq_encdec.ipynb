{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsansao/teic-20231/blob/main/TEIC_Licao29_seq2seq_encdec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUvFeu7p3tPn"
      },
      "source": [
        "# Treinamento de Transformer para Tradução PT -> EN\n",
        "\n",
        "Notebook completo que implementa um modelo Transformer \"do zero\" (usando as camadas de atenção do Keras) para treinar uma tradução de Português para Inglês, utilizando o conjunto de dados do Anki.\n",
        "\n",
        "Etapas:\n",
        "1.  **Configuração e Imports**\n",
        "2.  **Download e Preparação dos Dados**\n",
        "3.  **Vetorização e Tokenização**\n",
        "4.  **Criação do Pipeline `tf.data`**\n",
        "5.  **Construção do Modelo Transformer**\n",
        "6.  **Treinamento**\n",
        "7.  **Função de Inferência (Tradução)**"
      ],
      "id": "CUvFeu7p3tPn"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "U1MKt2xv3tPo"
      },
      "outputs": [],
      "source": [
        "# Célula 1: Instalações e Imports\n",
        "# (Nenhuma instalação extra é necessária no Colab para isso)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import random"
      ],
      "id": "U1MKt2xv3tPo"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_hR6VIAn3tPp"
      },
      "outputs": [],
      "source": [
        "## -----------------------------------------------------------------\n",
        "## Célula 2: Configuração dos Hiperparâmetros\n",
        "## -----------------------------------------------------------------\n",
        "# Para um demo rápido no Colab, usamos um subconjunto dos dados\n",
        "# e hiperparâmetros menores.\n",
        "# O dataset completo tem ~190.000 pares.\n",
        "NUM_SAMPLES = 100000\n",
        "# Hiperparâmetros do Modelo\n",
        "EMBED_DIM = 256        # Dimensão dos embeddings\n",
        "NUM_HEADS = 4          # Número de cabeças de atenção\n",
        "FF_DIM = 512           # Dimensão da camada Feed-Forward interna\n",
        "NUM_ENCODER_LAYERS = 2 # Número de camadas do Encoder\n",
        "NUM_DECODER_LAYERS = 2 # Número de camadas do Decoder\n",
        "VOCAB_SIZE = 15000     # Tamanho do vocabulário\n",
        "MAX_SEQ_LENGTH = 30    # Comprimento máximo das sentenças\n",
        "\n",
        "# Hiperparâmetros de Treinamento\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30  # Para um treinamento real, aumente para 30-50"
      ],
      "id": "_hR6VIAn3tPp"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz8DmliK3tPp",
        "outputId": "7354bf8b-2350-4cca-dd70-a9ab868c2533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baixando e extraindo o dataset do Anki (por-eng)...\n",
            "replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "Dataset baixado.\n",
            "Total de pares de sentenças: 196620\n",
            "Usando 100000 amostras para este demo.\n",
            "\n",
            "Exemplo de par (Input, Target):\n",
            "Input (PT): Tom disse que ele não entendeu nada.\n",
            "Target (EN): Tom said he didn't understand anything.\n"
          ]
        }
      ],
      "source": [
        "## -----------------------------------------------------------------\n",
        "## Célula 3: Download e Preparação dos Dados\n",
        "## -----------------------------------------------------------------\n",
        "\n",
        "print(\"Baixando e extraindo o dataset do Anki (por-eng)...\")\n",
        "!wget -q http://www.manythings.org/anki/por-eng.zip\n",
        "!unzip -q por-eng.zip\n",
        "\n",
        "data_path = \"por.txt\"\n",
        "print(\"Dataset baixado.\")\n",
        "\n",
        "# Leitura do arquivo\n",
        "# O formato é: Inglês \\t Português \\t Atribuição\n",
        "# Queremos: Português (entrada) -> Inglês (saída)\n",
        "text_pairs = []\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[:-1]:  # O último é vazio\n",
        "    if \"\\t\" not in line:\n",
        "        continue\n",
        "    parts = line.split(\"\\t\")\n",
        "    if len(parts) < 2:\n",
        "        continue\n",
        "\n",
        "    # Nosso objetivo é Português -> Inglês\n",
        "    target_text = parts[0] # Inglês\n",
        "    input_text = parts[1]  # Português\n",
        "\n",
        "    text_pairs.append((input_text, target_text))\n",
        "\n",
        "print(f\"Total de pares de sentenças: {len(text_pairs)}\")\n",
        "\n",
        "# Embaralhar e selecionar um subconjunto\n",
        "random.shuffle(text_pairs)\n",
        "print(f\"Usando {NUM_SAMPLES} amostras para este demo.\")\n",
        "train_pairs = text_pairs[: int(NUM_SAMPLES * 0.9)]\n",
        "val_pairs = text_pairs[int(NUM_SAMPLES * 0.9) : NUM_SAMPLES]\n",
        "\n",
        "print(f\"\\nExemplo de par (Input, Target):\")\n",
        "print(f\"Input (PT): {train_pairs[0][0]}\")\n",
        "print(f\"Target (EN): {train_pairs[0][1]}\")"
      ],
      "id": "Uz8DmliK3tPp"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HluHtSde3tPq",
        "outputId": "643a33cb-345b-4e7f-d193-d21815472c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Adaptando vocabulários...\n",
            "Vocabulários adaptados.\n",
            "\n",
            "Vocabulário PT (Fonte): ['', '[UNK]', np.str_('tom'), np.str_('que'), np.str_('o'), np.str_('não'), np.str_('eu'), np.str_('de'), np.str_('a'), np.str_('você')]...\n",
            "Vocabulário EN (Alvo): ['', '[UNK]', np.str_('[START]'), np.str_('[END]'), np.str_('tom'), np.str_('i'), np.str_('to'), np.str_('you'), np.str_('the'), np.str_('a')]...\n"
          ]
        }
      ],
      "source": [
        "## -----------------------------------------------------------------\n",
        "## Célula 4: Pré-processamento e Vetorização do Texto\n",
        "## -----------------------------------------------------------------\n",
        "\n",
        "# Precisamos de uma função de padronização customizada para:\n",
        "# 1. Colocar em minúsculas\n",
        "# 2. Remover pontuação\n",
        "# 3. Adicionar tokens [START] e [END] (APENAS para o alvo/target)\n",
        "\n",
        "strip_chars = string.punctuation.replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    # Minúsculas e remoção de HTML (embora não deva ter aqui)\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "\n",
        "    # Remove pontuação\n",
        "    no_punctuation = tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\"\n",
        "    )\n",
        "\n",
        "    # Adiciona tokens de início e fim\n",
        "    # Isso é crucial para o decoder\n",
        "    return tf.strings.join([\"[START]\", no_punctuation, \"[END]\"], separator=\" \")\n",
        "\n",
        "# Camada de Vetorização para a ENTRADA (Português)\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_SEQ_LENGTH,\n",
        "    # Não adicionamos [START]/[END] na entrada\n",
        ")\n",
        "\n",
        "# Camada de Vetorização para o ALVO (Inglês)\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_SEQ_LENGTH + 1, # +1 para acomodar [START] ou [END]\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "# \"Adaptar\" (treinar) os vocabulários\n",
        "print(\"\\nAdaptando vocabulários...\")\n",
        "source_texts = [pair[0] for pair in train_pairs]\n",
        "target_texts = [pair[1] for pair in train_pairs]\n",
        "\n",
        "source_vectorization.adapt(source_texts)\n",
        "target_vectorization.adapt(target_texts)\n",
        "print(\"Vocabulários adaptados.\")\n",
        "\n",
        "print(f\"\\nVocabulário PT (Fonte): {source_vectorization.get_vocabulary()[:10]}...\")\n",
        "print(f\"Vocabulário EN (Alvo): {target_vectorization.get_vocabulary()[:10]}...\")"
      ],
      "id": "HluHtSde3tPq"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF9UBMgE3tPq",
        "outputId": "04db0f68-0f2c-43ce-d3a6-839ee8289ec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Formato do Lote de Dados (X, Y):\n",
            "X['encoder_inputs'] shape: (64, 30)\n",
            "X['decoder_inputs'] shape: (64, 30)\n",
            "Y (decoder_outputs) shape: (64, 30)\n"
          ]
        }
      ],
      "source": [
        "## -----------------------------------------------------------------\n",
        "## Célula 5: Criação do Pipeline `tf.data`\n",
        "## -----------------------------------------------------------------\n",
        "\n",
        "# Esta função prepara os dados para o formato que o Transformer espera:\n",
        "# X = (encoder_inputs, decoder_inputs)\n",
        "# Y = decoder_outputs (que é o decoder_inputs deslocado em 1)\n",
        "\n",
        "def format_dataset(source, target):\n",
        "    source_vec = source_vectorization(source)\n",
        "    target_vec = target_vectorization(target)\n",
        "\n",
        "    # target_vec é \"[START] ... [END]\"\n",
        "    # decoder_inputs é \"[START] ...\" (sem o [END])\n",
        "    decoder_inputs = target_vec[:, :-1]\n",
        "\n",
        "    # decoder_outputs é \"... [END]\" (sem o [START])\n",
        "    decoder_outputs = target_vec[:, 1:]\n",
        "\n",
        "    # O modelo receberá dois inputs e tentará prever o output\n",
        "    return (\n",
        "        {\"encoder_inputs\": source_vec, \"decoder_inputs\": decoder_inputs},\n",
        "        decoder_outputs\n",
        "    )\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    source_texts, target_texts = zip(*pairs)\n",
        "    source_texts = list(source_texts)\n",
        "    target_texts = list(target_texts)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((source_texts, target_texts))\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.shuffle(2048).prefetch(tf.data.AUTOTUNE).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "\n",
        "print(\"\\nFormato do Lote de Dados (X, Y):\")\n",
        "for batch in train_ds.take(1):\n",
        "    X, Y = batch\n",
        "    print(\"X['encoder_inputs'] shape:\", X['encoder_inputs'].shape)\n",
        "    print(\"X['decoder_inputs'] shape:\", X['decoder_inputs'].shape)\n",
        "    print(\"Y (decoder_outputs) shape:\", Y.shape)"
      ],
      "id": "DF9UBMgE3tPq"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "USbiQqG03tPr"
      },
      "outputs": [],
      "source": [
        "## -----------------------------------------------------------------\n",
        "## Célula 6: Construção dos Blocos do Transformer\n",
        "## -----------------------------------------------------------------\n",
        "\n",
        "# O Transformer é composto por um Encoder e um Decoder.\n",
        "# Ambos usam Atenção Multi-Cabeça (Multi-Head Attention).\n",
        "\n",
        "### 6.1 - Embedding de Posição\n",
        "# O Transformer não tem recorrência (como RNNs), então precisamos\n",
        "# injetar informação sobre a posição dos tokens.\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Embedding dos tokens\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        # Embedding da posição (um vetor para cada posição)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.maxlen = maxlen\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, x):\n",
        "        # Cria um vetor de posições (0, 1, 2, ..., maxlen-1)\n",
        "        # tf.shape(x)[1] é o comprimento da sequência\n",
        "        positions = tf.range(start=0, limit=tf.shape(x)[1], delta=1)\n",
        "\n",
        "        # Obtém os embeddings de posição\n",
        "        position_embeddings = self.pos_emb(positions)\n",
        "\n",
        "        # Obtém os embeddings de token\n",
        "        token_embeddings = self.token_emb(x)\n",
        "\n",
        "        # Soma os dois\n",
        "        return token_embeddings + position_embeddings\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"maxlen\": self.maxlen,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "### 6.2 - Camada do Encoder Transformer\n",
        "class TransformerEncoderLayer(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "\n",
        "        # Camada de Atenção Multi-Cabeça (Self-Attention)\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "\n",
        "        # Rede Feed-Forward\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Camadas de Normalização (Layer Normalization)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Camadas de Dropout\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # 1. Self-Attention + Conexão Residual\n",
        "        # (query=inputs, key=inputs, value=inputs)\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        # Conexão residual e normalização\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        # 2. Feed-Forward + Conexão Residual\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        # Conexão residual e normalização\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "### 6.3 - Camada do Decoder Transformer\n",
        "class TransformerDecoderLayer(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "\n",
        "        # 1. Atenção Multi-Cabeça MASCARADA (Self-Attention)\n",
        "        # O decoder só pode \"ver\" os tokens anteriores.\n",
        "        self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "\n",
        "        # 2. Atenção Multi-Cabeça (Cross-Attention)\n",
        "        # O decoder \"vê\" a saída do encoder.\n",
        "        self.att2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "\n",
        "        # Rede Feed-Forward\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "        self.dropout3 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, training=False):\n",
        "        # `inputs` é o output do decoder até agora\n",
        "        # `encoder_outputs` é o output final do encoder\n",
        "\n",
        "        # 1. Self-Attention MASCARADA\n",
        "        # use_causal_mask=True garante que a posição 'i' só atenda\n",
        "        # às posições < 'i'.\n",
        "        attn1 = self.att1(inputs, inputs, use_causal_mask=True)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn1)\n",
        "\n",
        "        # 2. Cross-Attention (Encoder-Decoder Attention)\n",
        "        # Query = out1 (do decoder)\n",
        "        # Key/Value = encoder_outputs (do encoder)\n",
        "        attn2 = self.att2(query=out1, value=encoder_outputs, key=encoder_outputs)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(out1 + attn2)\n",
        "\n",
        "        # 3. Feed-Forward\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        return self.layernorm3(out2 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "        })\n",
        "        return config"
      ],
      "id": "USbiQqG03tPr"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "GiEUBXx83tPr",
        "outputId": "aeaf5ece-d087-4569-edde-b3bbecd50241"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_18\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_18\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_and_position… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m3,847,680\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mTokenAndPositionE…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m1,315,840\u001b[0m │ token_and_positi… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_and_position… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m3,847,936\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mTokenAndPositionE…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m1,315,840\u001b[0m │ transformer_enco… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m2,368,256\u001b[0m │ token_and_positi… │\n",
              "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m2,368,256\u001b[0m │ transformer_deco… │\n",
              "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m3,855,000\u001b[0m │ transformer_deco… │\n",
              "│                     │ \u001b[38;5;34m15000\u001b[0m)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_and_position… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,847,680</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionE…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,840</span> │ token_and_positi… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_and_position… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,847,936</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionE…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,840</span> │ transformer_enco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,368,256</span> │ token_and_positi… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,368,256</span> │ transformer_deco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,855,000</span> │ transformer_deco… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,918,808\u001b[0m (72.17 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,918,808</span> (72.17 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,918,808\u001b[0m (72.17 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,918,808</span> (72.17 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "## -----------------------------------------------------------------\n",
        "## Célula 7: Montagem do Modelo Transformer Completo\n",
        "## -----------------------------------------------------------------\n",
        "\n",
        "def build_transformer_model():\n",
        "    # --- ENCODER ---\n",
        "    # Input do Encoder (sentenças em Português)\n",
        "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "\n",
        "    # Embedding de Posição do Encoder\n",
        "    # Usamos MAX_SEQ_LENGTH aqui\n",
        "    x = TokenAndPositionEmbedding(MAX_SEQ_LENGTH, VOCAB_SIZE, EMBED_DIM)(encoder_inputs)\n",
        "\n",
        "    # Pilha de Camadas do Encoder\n",
        "    # O output do encoder será a memória de \"contexto\"\n",
        "    for _ in range(NUM_ENCODER_LAYERS):\n",
        "        x = TransformerEncoderLayer(EMBED_DIM, NUM_HEADS, FF_DIM)(x)\n",
        "    encoder_outputs = x # Saída final do Encoder\n",
        "\n",
        "    # --- DECODER ---\n",
        "    # Input do Decoder (sentenças em Inglês, deslocadas)\n",
        "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "\n",
        "    # Embedding de Posição do Decoder\n",
        "    # Usamos MAX_SEQ_LENGTH + 1 aqui (por causa do [START]/[END])\n",
        "    x = TokenAndPositionEmbedding(MAX_SEQ_LENGTH + 1, VOCAB_SIZE, EMBED_DIM)(decoder_inputs)\n",
        "\n",
        "    # Pilha de Camadas do Decoder\n",
        "    # O decoder recebe seu próprio input (x) e a saída do encoder\n",
        "    for _ in range(NUM_DECODER_LAYERS):\n",
        "        x = TransformerDecoderLayer(EMBED_DIM, NUM_HEADS, FF_DIM)(x, encoder_outputs)\n",
        "\n",
        "    # --- SAÍDA FINAL ---\n",
        "    # Camada Dense final para prever a próxima palavra no vocabulário\n",
        "    # A saída terá shape (batch_size, seq_len, vocab_size)\n",
        "    outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "\n",
        "    # Cria o modelo Keras\n",
        "    model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Instancia o modelo\n",
        "transformer = build_transformer_model()\n",
        "\n",
        "transformer.summary()\n",
        "\n",
        "# Plotar o modelo (útil no Colab)\n",
        "# keras.utils.plot_model(transformer, show_shapes=True, dpi=64)"
      ],
      "id": "GiEUBXx83tPr"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5YCKiLA3tPs",
        "outputId": "2b37899f-39fb-46d6-d70d-2a3bde4a341a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando o treinamento...\n",
            "Epoch 1/30\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 95ms/step - accuracy: 0.7920 - loss: 2.6965 - val_accuracy: 0.8745 - val_loss: 0.7879\n",
            "Epoch 2/30\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 82ms/step - accuracy: 0.8854 - loss: 0.7137 - val_accuracy: 0.9115 - val_loss: 0.5313\n",
            "Epoch 3/30\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 81ms/step - accuracy: 0.9188 - loss: 0.4840 - val_accuracy: 0.9308 - val_loss: 0.4100\n",
            "Epoch 4/30\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 81ms/step - accuracy: 0.9362 - loss: 0.3626 - val_accuracy: 0.9390 - val_loss: 0.3497\n",
            "Epoch 5/30\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 81ms/step - accuracy: 0.9462 - loss: 0.2887 - val_accuracy: 0.9438 - val_loss: 0.3144\n",
            "Epoch 6/30\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 81ms/step - accuracy: 0.9534 - loss: 0.2378 - val_accuracy: 0.9467 - val_loss: 0.2932\n",
            "Epoch 7/30\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 82ms/step - accuracy: 0.9587 - loss: 0.2005 - val_accuracy: 0.9489 - val_loss: 0.2810\n",
            "Epoch 8/30\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 81ms/step - accuracy: 0.9630 - loss: 0.1715 - val_accuracy: 0.9502 - val_loss: 0.2726\n",
            "Epoch 9/30\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 82ms/step - accuracy: 0.9667 - loss: 0.1481 - val_accuracy: 0.9509 - val_loss: 0.2721\n",
            "Epoch 10/30\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 81ms/step - accuracy: 0.9701 - loss: 0.1290 - val_accuracy: 0.9516 - val_loss: 0.2699\n",
            "Epoch 11/30\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 82ms/step - accuracy: 0.9727 - loss: 0.1129 - val_accuracy: 0.9516 - val_loss: 0.2719\n",
            "Epoch 12/30\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 83ms/step - accuracy: 0.9752 - loss: 0.0992 - val_accuracy: 0.9520 - val_loss: 0.2732\n",
            "Epoch 13/30\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 83ms/step - accuracy: 0.9774 - loss: 0.0873 - val_accuracy: 0.9519 - val_loss: 0.2773\n",
            "\n",
            "Treinamento concluído.\n"
          ]
        }
      ],
      "source": [
        "## -----------------------------------------------------------------\n",
        "## Célula 8: Treinamento do Modelo\n",
        "## -----------------------------------------------------------------\n",
        "\n",
        "print(\"\\nIniciando o treinamento...\")\n",
        "\n",
        "# Compilamos o modelo\n",
        "# Usamos \"sparse_categorical_crossentropy\" porque nossos alvos (Y)\n",
        "# são inteiros (IDs de tokens), e não one-hot encoded.\n",
        "transformer.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"], # A acurácia aqui é \"por token\"\n",
        ")\n",
        "\n",
        "# Treina o modelo\n",
        "# Usamos o EarlyStopping para parar se a validação não melhorar\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "history = transformer.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"\\nTreinamento concluído.\")"
      ],
      "id": "p5YCKiLA3tPs"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "LPnb4OBj3tPs"
      },
      "outputs": [],
      "source": [
        "## -----------------------------------------------------------------\n",
        "## Célula 9: Inferência (Tradução)\n",
        "## -----------------------------------------------------------------\n",
        "\n",
        "# Para traduzir, precisamos de um loop auto-regressivo:\n",
        "# 1. Enviar a sentença PT para o Encoder -> obter `encoder_outputs`\n",
        "# 2. Iniciar o Decoder com o token [START]\n",
        "# 3. Loop:\n",
        "#    a. Prever o próximo token\n",
        "#    b. Pegar o token com maior prob. (argmax)\n",
        "#    c. Se for [END], parar.\n",
        "#    d. Senão, adicionar o token à sequência do decoder e repetir.\n",
        "\n",
        "# Mapeamentos de ID -> Palavra para o vocabulário de Inglês (Alvo)\n",
        "target_vocab = target_vectorization.get_vocabulary()\n",
        "index_to_word = dict(zip(range(len(target_vocab)), target_vocab))\n",
        "word_to_index = dict(zip(target_vocab, range(len(target_vocab))))\n",
        "\n",
        "# Índices dos tokens especiais\n",
        "START_TOKEN_INDEX = word_to_index[\"[START]\"]\n",
        "END_TOKEN_INDEX = word_to_index[\"[END]\"]\n",
        "\n",
        "def translate(input_sentence):\n",
        "    \"\"\"Traduza uma sentença em Português para Inglês.\"\"\"\n",
        "\n",
        "    print(f\"Input: {input_sentence}\")\n",
        "\n",
        "    # 1. Vetorizar a sentença de entrada (PT)\n",
        "    tokenized_input = source_vectorization([input_sentence])\n",
        "\n",
        "    # 2. Iniciar a sequência do decoder. Começa apenas com [START].\n",
        "    # Shape é (batch_size, seq_len) -> (1, 1)\n",
        "    decoder_input_tokens = tf.convert_to_tensor([[START_TOKEN_INDEX]], dtype=tf.int64)\n",
        "\n",
        "    # 3. Loop auto-regressivo\n",
        "    for i in range(MAX_SEQ_LENGTH):\n",
        "        # Obter as predições do modelo\n",
        "        # O modelo é chamado com [encoder_inputs, decoder_inputs]\n",
        "        preds = transformer([tokenized_input, decoder_input_tokens])\n",
        "\n",
        "        # Pegar os logits apenas do ÚLTIMO token previsto\n",
        "        # preds shape: (1, seq_len, vocab_size) -> (1, vocab_size)\n",
        "        last_token_pred = preds[:, -1, :]\n",
        "\n",
        "        # Encontrar o ID do token com a maior probabilidade (argmax)\n",
        "        sampled_token_index = tf.argmax(last_token_pred, axis=1)[0].numpy()\n",
        "\n",
        "        # Se for o token [END], paramos a tradução\n",
        "        if sampled_token_index == END_TOKEN_INDEX:\n",
        "            break\n",
        "\n",
        "        # Adicionar o novo token previsto à sequência de entrada do decoder\n",
        "        # para a próxima iteração\n",
        "        decoder_input_tokens = tf.concat(\n",
        "            [decoder_input_tokens, [[sampled_token_index]]], axis=1\n",
        "        )\n",
        "\n",
        "    # 4. Decodificar a sequência de tokens de saída\n",
        "    output_tokens = decoder_input_tokens[0].numpy()\n",
        "\n",
        "    # Converte os IDs de volta para palavras, ignorando [START]\n",
        "    translated_text = \" \".join(\n",
        "        [index_to_word[token] for token in output_tokens\n",
        "         if token not in [START_TOKEN_INDEX, END_TOKEN_INDEX]]\n",
        "    )\n",
        "\n",
        "    print(f\"Output: {translated_text}\\n\")\n",
        "    return translated_text"
      ],
      "id": "LPnb4OBj3tPs"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kOPpXHR33tPt",
        "outputId": "58b99666-692a-468b-9b98-05fcc6b6e566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testando Traduções (Exemplos de Validação) ---\n",
            "Input: Os cavalos se assustam facilmente.\n",
            "Output: horses scare yourself\n",
            "\n",
            "(Referência: Horses are easily spooked.)\n",
            "------------------------------\n",
            "Input: É quase impossível aprender uma língua estrangeira em pouco tempo.\n",
            "Output: its almost impossible to learn a foreign language in a while\n",
            "\n",
            "(Referência: It is almost impossible to learn a foreign language in a short time.)\n",
            "------------------------------\n",
            "Input: É assim que nós fazemos as coisas.\n",
            "Output: thats how we do things\n",
            "\n",
            "(Referência: That's how we do things.)\n",
            "------------------------------\n",
            "Input: Quanto tempo você vai ficar em Boston?\n",
            "Output: how long are you going to stay in boston\n",
            "\n",
            "(Referência: How much time will you be in Boston?)\n",
            "------------------------------\n",
            "Input: Tente ter uma mente aberta.\n",
            "Output: try to have a open open\n",
            "\n",
            "(Referência: Try to have an open mind.)\n",
            "------------------------------\n",
            "Input: Diga tchau aos seus amigos.\n",
            "Output: tell your friends to your friends\n",
            "\n",
            "(Referência: Say goodbye to your friends.)\n",
            "------------------------------\n",
            "Input: Todo mundo deveria ser submetido a um exame físico periodicamente.\n",
            "Output: everyone should be allowed the exam for a sheet of steel\n",
            "\n",
            "(Referência: Everyone should periodically receive a physical examination.)\n",
            "------------------------------\n",
            "Input: Eu escaparei.\n",
            "Output: i will find it\n",
            "\n",
            "(Referência: I'm going to escape.)\n",
            "------------------------------\n",
            "Input: Todos os rapazes estavam rindo.\n",
            "Output: all the boys were laughing\n",
            "\n",
            "(Referência: All the guys were laughing.)\n",
            "------------------------------\n",
            "Input: Tom nunca me enganou.\n",
            "Output: tom never fooled me\n",
            "\n",
            "(Referência: Tom never fooled me.)\n",
            "------------------------------\n",
            "\n",
            "--- Teste com Novas Sentenças ---\n",
            "Input: Eu gosto de gatos.\n",
            "Output: i like cats\n",
            "\n",
            "Input: Este é um teste.\n",
            "Output: is this a test\n",
            "\n",
            "Input: Onde fica o banheiro?\n",
            "Output: wheres the toilet\n",
            "\n",
            "Input: Quantos anos você tem?\n",
            "Output: how old are you\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'how old are you'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "## -----------------------------------------------------------------\n",
        "## Célula 10: Teste do Modelo\n",
        "## -----------------------------------------------------------------\n",
        "\n",
        "print(\"\\n--- Testando Traduções (Exemplos de Validação) ---\")\n",
        "\n",
        "for i in range(10):\n",
        "    pair = val_pairs[i]\n",
        "    pt_sentence = pair[0]\n",
        "    en_sentence = pair[1]\n",
        "\n",
        "    translate(pt_sentence)\n",
        "    print(f\"(Referência: {en_sentence})\\n\" + (\"-\"*30))\n",
        "\n",
        "print(\"\\n--- Teste com Novas Sentenças ---\")\n",
        "translate(\"Eu gosto de gatos.\")\n",
        "translate(\"Este é um teste.\")\n",
        "translate(\"Onde fica o banheiro?\")\n",
        "translate(\"Quantos anos você tem?\")"
      ],
      "id": "kOPpXHR33tPt"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}