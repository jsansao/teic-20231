{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsansao/teic-20231/blob/main/TEIC_Licao22_FastTextLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLXxBrCTAVm-"
      },
      "source": [
        "# Abordagem 1: Classificação com FastText (Embeddings) e LSTM\n",
        "\n",
        "Este notebook demonstra como usar o **FastText** como um gerador de *embeddings* (vetores de palavras) que são então usados para alimentar uma rede LSTM no Keras.\n",
        "\n",
        "A principal vantagem do FastText sobre o Word2Vec é o uso de **n-gramas de caracteres** (informação de subpalavras). Isso permite que ele crie vetores para palavras que não estavam no vocabulário de treino (OOV - Out-of-Vocabulary).\n",
        "\n",
        "O fluxo é quase idêntico ao do notebook Word2Vec:\n",
        "1. Carregar dados.\n",
        "2. Treinar um modelo FastText (via `gensim`) nos textos.\n",
        "3. Preparar dados para o Keras (Tokenizer, Padding).\n",
        "4. Criar a Matriz de Embedding usando os vetores do FastText.\n",
        "5. Construir, treinar e avaliar o modelo LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5Mz7Ve8AVnH",
        "outputId": "734aab8d-d618-4941-917b-603cd941067f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ],
      "source": [
        "# Célula 1: Instalação e Importações\n",
        "!pip install gensim\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import FastText # <-- A MUDANÇA ESTÁ AQUI\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaXWqcKEAVnP"
      },
      "source": [
        "## Passo 1: Carregar e Preparar Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWmpRNI2AVnU",
        "outputId": "0657778c-4dda-4094-e5ff-124b592f1352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amostras de treino: 1571\n",
            "Amostras de teste: 393\n"
          ]
        }
      ],
      "source": [
        "# Célula 2: Carregar os dados\n",
        "categorias = ['comp.graphics', 'sci.crypt']\n",
        "dados = fetch_20newsgroups(subset='all', categories=categorias, shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(dados.data, dados.target, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Amostras de treino: {len(X_train_text)}\")\n",
        "print(f\"Amostras de teste: {len(X_test_text)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C25-UX8JAVnY"
      },
      "source": [
        "## Passo 2: Treinar o Modelo FastText (Gensim)\n",
        "\n",
        "O processo é o mesmo do Word2Vec, mas trocamos a classe `Word2Vec` por `FastText`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm4m7hXlAVna",
        "outputId": "a3033bf8-e9da-4f0e-8a82-b91784f15910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeiro documento tokenizado (para Gensim):\n",
            "[]...\n"
          ]
        }
      ],
      "source": [
        "# Célula 3: Pré-processamento para o Gensim\n",
        "def preprocess_text_gensim(text):\n",
        "    text = re.sub(r'\\W+', ' ', text) # Remove caracteres não-alfanuméricos\n",
        "    text = text.lower()\n",
        "    return text.split()\n",
        "\n",
        "textos_completos = X_train_text + X_test_text\n",
        "textos_tokenizados = [preprocess_text_gensim(doc) for doc in textos_completos]\n",
        "\n",
        "print(f\"Primeiro documento tokenizado (para Gensim):\\n{textos_tokenizados[0][:20]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77p27rHXAVnd",
        "outputId": "4946f3fd-cbd5-42e4-fa72-a617c0d348c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando modelo FastText...\n",
            "Treino do FastText concluído.\n",
            "Tamanho do vocabulário do FastText (com min_count=3): 9871 palavras\n"
          ]
        }
      ],
      "source": [
        "# Célula 4: Treinar o modelo FastText\n",
        "\n",
        "DIM_EMBEDDING = 100\n",
        "WINDOW_SIZE = 5\n",
        "MIN_COUNT = 3\n",
        "WORKERS = 4\n",
        "\n",
        "print(\"Treinando modelo FastText...\")\n",
        "ft_model = FastText( # <-- A MUDANÇA ESTÁ AQUI\n",
        "    sentences=textos_tokenizados,\n",
        "    vector_size=DIM_EMBEDDING,\n",
        "    window=WINDOW_SIZE,\n",
        "    min_count=MIN_COUNT,\n",
        "    workers=WORKERS,\n",
        "    min_n=3, # Tamanho mínimo dos n-gramas de caracteres\n",
        "    max_n=6  # Tamanho máximo dos n-gramas de caracteres\n",
        ")\n",
        "\n",
        "print(\"Treino do FastText concluído.\")\n",
        "vocab_size_ft = len(ft_model.wv.key_to_index)\n",
        "print(f\"Tamanho do vocabulário do FastText (com min_count={MIN_COUNT}): {vocab_size_ft} palavras\")\n",
        "\n",
        "# A grande vantagem: gerar vetor para palavra OOV\n",
        "palavra_teste_oov = 'ultramegasupercomputador'\n",
        "if palavra_teste_oov not in ft_model.wv:\n",
        "    print(f\"'{palavra_teste_oov}' NÃO está no vocabulário treinado.\")\n",
        "    # Mas ainda podemos obter um vetor!\n",
        "    vetor_oov = ft_model.wv[palavra_teste_oov]\n",
        "    print(f\"Mas o FastText gerou um vetor para ela: {vetor_oov.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amOh3nINAVnh"
      },
      "source": [
        "## Passo 3 & 4: Preparar Keras e Matriz de Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqRsNTizAVnl",
        "outputId": "ebaa80c4-17e7-4381-99bc-629c5c067510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho do vocabulário do Keras: 10000\n"
          ]
        }
      ],
      "source": [
        "# Célula 5: Tokenização e Padding do Keras\n",
        "MAX_PALAVRAS_VOCAB = 10000\n",
        "MAX_COMPRIMENTO_SEQ = 250\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_PALAVRAS_VOCAB, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train_text)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_COMPRIMENTO_SEQ, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_COMPRIMENTO_SEQ, padding='post', truncating='post')\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size_keras = min(MAX_PALAVRAS_VOCAB, len(word_index) + 1)\n",
        "\n",
        "print(f\"Tamanho do vocabulário do Keras: {vocab_size_keras}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYjcfYmaAVnt",
        "outputId": "c9a99694-048d-4509-f6bb-c5248c286aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Embedding criada com formato: (10000, 100)\n",
            "9999 palavras do Keras mapeadas.\n",
            "0 palavras não mapeadas (improvável com FastText).\n"
          ]
        }
      ],
      "source": [
        "# Célula 6: Construção da Matriz de Embedding (usando ft_model)\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size_keras, DIM_EMBEDDING))\n",
        "palavras_encontradas = 0\n",
        "palavras_nao_encontradas = 0\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i >= MAX_PALAVRAS_VOCAB:\n",
        "        continue\n",
        "\n",
        "    # Usamos o ft_model.wv. A vantagem é que mesmo que 'word'\n",
        "    # não esteja no vocabulário treinado (por min_count),\n",
        "    # o FastText pode inferir um vetor para ela.\n",
        "    try:\n",
        "        embedding_matrix[i] = ft_model.wv[word]\n",
        "        palavras_encontradas += 1\n",
        "    except KeyError:\n",
        "        # No FastText (gensim >= 4.0), isso é raro, mas tratamos por segurança\n",
        "        embedding_matrix[i] = np.zeros(DIM_EMBEDDING)\n",
        "        palavras_nao_encontradas += 1\n",
        "\n",
        "print(f\"Matriz de Embedding criada com formato: {embedding_matrix.shape}\")\n",
        "print(f\"{palavras_encontradas} palavras do Keras mapeadas.\")\n",
        "print(f\"{palavras_nao_encontradas} palavras não mapeadas (improvável com FastText).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQPXSo-JAVnw"
      },
      "source": [
        "## Passo 5 & 6: Construir e Treinar o Modelo LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "_BsAO9woAVn1",
        "outputId": "1753759c-23a6-44c7-ee8e-601a131e8e3d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Modelo_FastText_LSTM\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Modelo_FastText_LSTM\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │     \u001b[38;5;34m1,000,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ spatial_dropout1d_1             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ spatial_dropout1d_1             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,000,000\u001b[0m (3.81 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> (3.81 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,000,000\u001b[0m (3.81 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> (3.81 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Célula 7: Definindo o modelo LSTM\n",
        "modelo_ft_lstm = Sequential(name=\"Modelo_FastText_LSTM\")\n",
        "\n",
        "modelo_ft_lstm.add(Embedding(\n",
        "    input_dim=vocab_size_keras,\n",
        "    output_dim=DIM_EMBEDDING,\n",
        "    input_length=MAX_COMPRIMENTO_SEQ,\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=False # Congelar a camada\n",
        "))\n",
        "\n",
        "modelo_ft_lstm.add(SpatialDropout1D(0.2))\n",
        "modelo_ft_lstm.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "modelo_ft_lstm.add(Dense(16, activation='relu'))\n",
        "modelo_ft_lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "modelo_ft_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "modelo_ft_lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGRySRisAVn2",
        "outputId": "add149fe-776a-4e74-9cee-53c8b7cc445b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando o modelo FT+LSTM...\n",
            "Epoch 1/100\n",
            "50/50 - 13s - 254ms/step - accuracy: 0.8663 - loss: 0.3195 - val_accuracy: 0.8524 - val_loss: 0.3317\n",
            "Epoch 2/100\n",
            "50/50 - 20s - 409ms/step - accuracy: 0.8714 - loss: 0.3210 - val_accuracy: 0.8473 - val_loss: 0.3435\n",
            "Epoch 3/100\n",
            "50/50 - 20s - 407ms/step - accuracy: 0.8644 - loss: 0.3302 - val_accuracy: 0.8677 - val_loss: 0.3332\n",
            "Epoch 4/100\n",
            "50/50 - 13s - 252ms/step - accuracy: 0.8701 - loss: 0.3176 - val_accuracy: 0.8575 - val_loss: 0.3259\n",
            "Epoch 5/100\n",
            "50/50 - 12s - 248ms/step - accuracy: 0.8701 - loss: 0.3160 - val_accuracy: 0.8702 - val_loss: 0.3344\n",
            "Epoch 6/100\n",
            "50/50 - 21s - 410ms/step - accuracy: 0.8663 - loss: 0.3089 - val_accuracy: 0.8601 - val_loss: 0.3459\n",
            "Epoch 7/100\n",
            "50/50 - 13s - 252ms/step - accuracy: 0.8759 - loss: 0.3000 - val_accuracy: 0.8651 - val_loss: 0.3273\n",
            "Epoch 8/100\n",
            "50/50 - 20s - 410ms/step - accuracy: 0.8822 - loss: 0.2928 - val_accuracy: 0.8651 - val_loss: 0.3272\n",
            "Epoch 9/100\n",
            "50/50 - 12s - 250ms/step - accuracy: 0.8733 - loss: 0.2889 - val_accuracy: 0.8397 - val_loss: 0.3459\n",
            "Epoch 10/100\n",
            "50/50 - 20s - 398ms/step - accuracy: 0.8682 - loss: 0.3078 - val_accuracy: 0.8550 - val_loss: 0.3257\n",
            "Epoch 11/100\n",
            "50/50 - 14s - 273ms/step - accuracy: 0.8714 - loss: 0.3043 - val_accuracy: 0.8448 - val_loss: 0.3482\n",
            "Epoch 12/100\n",
            "50/50 - 12s - 250ms/step - accuracy: 0.8771 - loss: 0.2933 - val_accuracy: 0.8575 - val_loss: 0.3131\n",
            "Epoch 13/100\n",
            "50/50 - 12s - 248ms/step - accuracy: 0.8752 - loss: 0.3011 - val_accuracy: 0.8448 - val_loss: 0.3203\n",
            "Epoch 14/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.8752 - loss: 0.2875 - val_accuracy: 0.8601 - val_loss: 0.3293\n",
            "Epoch 15/100\n",
            "50/50 - 12s - 246ms/step - accuracy: 0.8797 - loss: 0.2984 - val_accuracy: 0.8779 - val_loss: 0.3148\n",
            "Epoch 16/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.8708 - loss: 0.2953 - val_accuracy: 0.8677 - val_loss: 0.3174\n",
            "Epoch 17/100\n",
            "50/50 - 20s - 407ms/step - accuracy: 0.8848 - loss: 0.2721 - val_accuracy: 0.8550 - val_loss: 0.3343\n",
            "Epoch 18/100\n",
            "50/50 - 20s - 406ms/step - accuracy: 0.8829 - loss: 0.2779 - val_accuracy: 0.8550 - val_loss: 0.3232\n",
            "Epoch 19/100\n",
            "50/50 - 12s - 244ms/step - accuracy: 0.8848 - loss: 0.2688 - val_accuracy: 0.8601 - val_loss: 0.3230\n",
            "Epoch 20/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.8822 - loss: 0.2843 - val_accuracy: 0.8651 - val_loss: 0.3102\n",
            "Epoch 21/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.8873 - loss: 0.2726 - val_accuracy: 0.8601 - val_loss: 0.3090\n",
            "Epoch 22/100\n",
            "50/50 - 12s - 245ms/step - accuracy: 0.8797 - loss: 0.2840 - val_accuracy: 0.8677 - val_loss: 0.3086\n",
            "Epoch 23/100\n",
            "50/50 - 12s - 249ms/step - accuracy: 0.8924 - loss: 0.2582 - val_accuracy: 0.8524 - val_loss: 0.3002\n",
            "Epoch 24/100\n",
            "50/50 - 12s - 244ms/step - accuracy: 0.8905 - loss: 0.2741 - val_accuracy: 0.8728 - val_loss: 0.3040\n",
            "Epoch 25/100\n",
            "50/50 - 12s - 249ms/step - accuracy: 0.8816 - loss: 0.2669 - val_accuracy: 0.8702 - val_loss: 0.3075\n",
            "Epoch 26/100\n",
            "50/50 - 20s - 409ms/step - accuracy: 0.8810 - loss: 0.2616 - val_accuracy: 0.8702 - val_loss: 0.3045\n",
            "Epoch 27/100\n",
            "50/50 - 14s - 287ms/step - accuracy: 0.8867 - loss: 0.2692 - val_accuracy: 0.8702 - val_loss: 0.3059\n",
            "Epoch 28/100\n",
            "50/50 - 12s - 249ms/step - accuracy: 0.8842 - loss: 0.2732 - val_accuracy: 0.8575 - val_loss: 0.3234\n",
            "Epoch 29/100\n",
            "50/50 - 13s - 256ms/step - accuracy: 0.8899 - loss: 0.2514 - val_accuracy: 0.8550 - val_loss: 0.3250\n",
            "Epoch 30/100\n",
            "50/50 - 20s - 392ms/step - accuracy: 0.8886 - loss: 0.2735 - val_accuracy: 0.8626 - val_loss: 0.3078\n",
            "Epoch 31/100\n",
            "50/50 - 21s - 415ms/step - accuracy: 0.8931 - loss: 0.2552 - val_accuracy: 0.8422 - val_loss: 0.3220\n",
            "Epoch 32/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.8905 - loss: 0.2638 - val_accuracy: 0.8728 - val_loss: 0.3105\n",
            "Epoch 33/100\n",
            "50/50 - 12s - 246ms/step - accuracy: 0.9052 - loss: 0.2454 - val_accuracy: 0.8626 - val_loss: 0.3155\n",
            "Epoch 34/100\n",
            "50/50 - 20s - 406ms/step - accuracy: 0.8956 - loss: 0.2523 - val_accuracy: 0.8651 - val_loss: 0.3128\n",
            "Epoch 35/100\n",
            "50/50 - 12s - 249ms/step - accuracy: 0.8791 - loss: 0.2480 - val_accuracy: 0.8499 - val_loss: 0.3303\n",
            "Epoch 36/100\n",
            "50/50 - 12s - 240ms/step - accuracy: 0.8969 - loss: 0.2576 - val_accuracy: 0.8702 - val_loss: 0.3094\n",
            "Epoch 37/100\n",
            "50/50 - 21s - 419ms/step - accuracy: 0.8924 - loss: 0.2625 - val_accuracy: 0.8626 - val_loss: 0.3368\n",
            "Epoch 38/100\n",
            "50/50 - 20s - 404ms/step - accuracy: 0.8905 - loss: 0.2568 - val_accuracy: 0.8601 - val_loss: 0.3000\n",
            "Epoch 39/100\n",
            "50/50 - 12s - 247ms/step - accuracy: 0.9007 - loss: 0.2397 - val_accuracy: 0.8626 - val_loss: 0.3177\n",
            "Epoch 40/100\n",
            "50/50 - 12s - 248ms/step - accuracy: 0.9077 - loss: 0.2394 - val_accuracy: 0.8626 - val_loss: 0.3273\n",
            "Epoch 41/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.8962 - loss: 0.2371 - val_accuracy: 0.8626 - val_loss: 0.3074\n",
            "Epoch 42/100\n",
            "50/50 - 21s - 429ms/step - accuracy: 0.8962 - loss: 0.2403 - val_accuracy: 0.8601 - val_loss: 0.3075\n",
            "Epoch 43/100\n",
            "50/50 - 12s - 249ms/step - accuracy: 0.8988 - loss: 0.2437 - val_accuracy: 0.8651 - val_loss: 0.2971\n",
            "Epoch 44/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.9039 - loss: 0.2216 - val_accuracy: 0.8728 - val_loss: 0.2807\n",
            "Epoch 45/100\n",
            "50/50 - 12s - 242ms/step - accuracy: 0.8937 - loss: 0.2475 - val_accuracy: 0.8601 - val_loss: 0.2970\n",
            "Epoch 46/100\n",
            "50/50 - 12s - 246ms/step - accuracy: 0.9090 - loss: 0.2284 - val_accuracy: 0.8779 - val_loss: 0.2837\n",
            "Epoch 47/100\n",
            "50/50 - 20s - 408ms/step - accuracy: 0.8931 - loss: 0.2492 - val_accuracy: 0.8626 - val_loss: 0.2975\n",
            "Epoch 48/100\n",
            "50/50 - 12s - 250ms/step - accuracy: 0.9141 - loss: 0.2230 - val_accuracy: 0.8575 - val_loss: 0.2919\n",
            "Epoch 49/100\n",
            "50/50 - 20s - 408ms/step - accuracy: 0.9007 - loss: 0.2176 - val_accuracy: 0.8651 - val_loss: 0.3259\n",
            "Epoch 50/100\n",
            "50/50 - 20s - 405ms/step - accuracy: 0.8918 - loss: 0.2386 - val_accuracy: 0.8550 - val_loss: 0.3071\n",
            "Epoch 51/100\n",
            "50/50 - 13s - 250ms/step - accuracy: 0.9045 - loss: 0.2293 - val_accuracy: 0.8753 - val_loss: 0.2958\n",
            "Epoch 52/100\n",
            "50/50 - 13s - 256ms/step - accuracy: 0.9013 - loss: 0.2306 - val_accuracy: 0.8626 - val_loss: 0.2934\n",
            "Epoch 53/100\n",
            "50/50 - 20s - 404ms/step - accuracy: 0.9077 - loss: 0.2300 - val_accuracy: 0.8728 - val_loss: 0.2735\n",
            "Epoch 54/100\n",
            "50/50 - 21s - 416ms/step - accuracy: 0.9058 - loss: 0.2212 - val_accuracy: 0.8651 - val_loss: 0.2790\n",
            "Epoch 55/100\n",
            "50/50 - 21s - 413ms/step - accuracy: 0.9077 - loss: 0.2173 - val_accuracy: 0.8651 - val_loss: 0.3008\n",
            "Epoch 56/100\n",
            "50/50 - 22s - 433ms/step - accuracy: 0.9109 - loss: 0.2114 - val_accuracy: 0.8651 - val_loss: 0.3148\n",
            "Epoch 57/100\n",
            "50/50 - 19s - 384ms/step - accuracy: 0.9045 - loss: 0.2181 - val_accuracy: 0.8651 - val_loss: 0.3073\n",
            "Epoch 58/100\n",
            "50/50 - 20s - 406ms/step - accuracy: 0.9064 - loss: 0.2190 - val_accuracy: 0.8702 - val_loss: 0.2859\n",
            "Epoch 59/100\n",
            "50/50 - 12s - 250ms/step - accuracy: 0.9128 - loss: 0.2210 - val_accuracy: 0.8677 - val_loss: 0.2734\n",
            "Epoch 60/100\n",
            "50/50 - 21s - 412ms/step - accuracy: 0.9045 - loss: 0.2093 - val_accuracy: 0.8702 - val_loss: 0.2827\n",
            "Epoch 61/100\n",
            "50/50 - 13s - 254ms/step - accuracy: 0.9122 - loss: 0.1985 - val_accuracy: 0.8651 - val_loss: 0.2734\n",
            "Epoch 62/100\n",
            "50/50 - 20s - 410ms/step - accuracy: 0.9026 - loss: 0.2163 - val_accuracy: 0.8601 - val_loss: 0.3356\n",
            "Epoch 63/100\n",
            "50/50 - 20s - 408ms/step - accuracy: 0.9115 - loss: 0.2133 - val_accuracy: 0.8677 - val_loss: 0.2713\n",
            "Epoch 64/100\n",
            "50/50 - 13s - 254ms/step - accuracy: 0.9102 - loss: 0.2086 - val_accuracy: 0.8499 - val_loss: 0.3218\n",
            "Epoch 65/100\n",
            "50/50 - 13s - 255ms/step - accuracy: 0.9211 - loss: 0.1966 - val_accuracy: 0.8830 - val_loss: 0.2623\n",
            "Epoch 66/100\n",
            "50/50 - 13s - 252ms/step - accuracy: 0.9211 - loss: 0.1930 - val_accuracy: 0.8804 - val_loss: 0.2945\n",
            "Epoch 67/100\n",
            "50/50 - 13s - 253ms/step - accuracy: 0.9153 - loss: 0.2150 - val_accuracy: 0.8753 - val_loss: 0.2731\n",
            "Epoch 68/100\n",
            "50/50 - 12s - 248ms/step - accuracy: 0.9204 - loss: 0.1917 - val_accuracy: 0.8830 - val_loss: 0.2846\n",
            "Epoch 69/100\n",
            "50/50 - 13s - 252ms/step - accuracy: 0.9204 - loss: 0.2105 - val_accuracy: 0.8677 - val_loss: 0.2677\n",
            "Epoch 70/100\n",
            "50/50 - 13s - 257ms/step - accuracy: 0.9147 - loss: 0.2036 - val_accuracy: 0.8677 - val_loss: 0.2652\n",
            "Epoch 71/100\n",
            "50/50 - 14s - 278ms/step - accuracy: 0.9236 - loss: 0.1979 - val_accuracy: 0.8753 - val_loss: 0.2734\n",
            "Epoch 72/100\n",
            "50/50 - 13s - 256ms/step - accuracy: 0.9173 - loss: 0.1916 - val_accuracy: 0.8804 - val_loss: 0.2858\n",
            "Epoch 73/100\n",
            "50/50 - 13s - 253ms/step - accuracy: 0.9281 - loss: 0.1824 - val_accuracy: 0.8626 - val_loss: 0.2675\n",
            "Epoch 74/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.9313 - loss: 0.1736 - val_accuracy: 0.8677 - val_loss: 0.2706\n",
            "Epoch 75/100\n",
            "50/50 - 21s - 411ms/step - accuracy: 0.9160 - loss: 0.1984 - val_accuracy: 0.8779 - val_loss: 0.2844\n",
            "Epoch 76/100\n",
            "50/50 - 13s - 253ms/step - accuracy: 0.9274 - loss: 0.1865 - val_accuracy: 0.8855 - val_loss: 0.2728\n",
            "Epoch 77/100\n",
            "50/50 - 20s - 400ms/step - accuracy: 0.9236 - loss: 0.1792 - val_accuracy: 0.8702 - val_loss: 0.2798\n",
            "Epoch 78/100\n",
            "50/50 - 13s - 252ms/step - accuracy: 0.9236 - loss: 0.1778 - val_accuracy: 0.8702 - val_loss: 0.2667\n",
            "Epoch 79/100\n",
            "50/50 - 13s - 253ms/step - accuracy: 0.9198 - loss: 0.1845 - val_accuracy: 0.8728 - val_loss: 0.2731\n",
            "Epoch 80/100\n",
            "50/50 - 20s - 408ms/step - accuracy: 0.9306 - loss: 0.1747 - val_accuracy: 0.8575 - val_loss: 0.3110\n",
            "Epoch 81/100\n",
            "50/50 - 13s - 253ms/step - accuracy: 0.9204 - loss: 0.1809 - val_accuracy: 0.8753 - val_loss: 0.2811\n",
            "Epoch 82/100\n",
            "50/50 - 13s - 255ms/step - accuracy: 0.9236 - loss: 0.1797 - val_accuracy: 0.8830 - val_loss: 0.2675\n",
            "Epoch 83/100\n",
            "50/50 - 13s - 256ms/step - accuracy: 0.9268 - loss: 0.1742 - val_accuracy: 0.8779 - val_loss: 0.2725\n",
            "Epoch 84/100\n",
            "50/50 - 20s - 405ms/step - accuracy: 0.9338 - loss: 0.1628 - val_accuracy: 0.8728 - val_loss: 0.2955\n",
            "Epoch 85/100\n",
            "50/50 - 13s - 260ms/step - accuracy: 0.9319 - loss: 0.1725 - val_accuracy: 0.8753 - val_loss: 0.2779\n",
            "Epoch 86/100\n",
            "50/50 - 12s - 245ms/step - accuracy: 0.9319 - loss: 0.1672 - val_accuracy: 0.8753 - val_loss: 0.2882\n",
            "Epoch 87/100\n",
            "50/50 - 14s - 278ms/step - accuracy: 0.9313 - loss: 0.1565 - val_accuracy: 0.8728 - val_loss: 0.2987\n",
            "Epoch 88/100\n",
            "50/50 - 13s - 254ms/step - accuracy: 0.9313 - loss: 0.1642 - val_accuracy: 0.8702 - val_loss: 0.2926\n",
            "Epoch 89/100\n",
            "50/50 - 21s - 414ms/step - accuracy: 0.9293 - loss: 0.1720 - val_accuracy: 0.8779 - val_loss: 0.3032\n",
            "Epoch 90/100\n",
            "50/50 - 20s - 405ms/step - accuracy: 0.9319 - loss: 0.1667 - val_accuracy: 0.8728 - val_loss: 0.3012\n",
            "Epoch 91/100\n",
            "50/50 - 13s - 255ms/step - accuracy: 0.9357 - loss: 0.1518 - val_accuracy: 0.8702 - val_loss: 0.3132\n",
            "Epoch 92/100\n",
            "50/50 - 13s - 257ms/step - accuracy: 0.9383 - loss: 0.1531 - val_accuracy: 0.8651 - val_loss: 0.3016\n",
            "Epoch 93/100\n",
            "50/50 - 13s - 253ms/step - accuracy: 0.9313 - loss: 0.1550 - val_accuracy: 0.8728 - val_loss: 0.2903\n",
            "Epoch 94/100\n",
            "50/50 - 13s - 259ms/step - accuracy: 0.9319 - loss: 0.1710 - val_accuracy: 0.8728 - val_loss: 0.2897\n",
            "Epoch 95/100\n",
            "50/50 - 13s - 260ms/step - accuracy: 0.9344 - loss: 0.1558 - val_accuracy: 0.8880 - val_loss: 0.2674\n",
            "Epoch 96/100\n",
            "50/50 - 13s - 255ms/step - accuracy: 0.9376 - loss: 0.1541 - val_accuracy: 0.8804 - val_loss: 0.2825\n",
            "Epoch 97/100\n",
            "50/50 - 20s - 409ms/step - accuracy: 0.9313 - loss: 0.1631 - val_accuracy: 0.8524 - val_loss: 0.3598\n",
            "Epoch 98/100\n",
            "50/50 - 13s - 254ms/step - accuracy: 0.9287 - loss: 0.1665 - val_accuracy: 0.8779 - val_loss: 0.2786\n",
            "Epoch 99/100\n",
            "50/50 - 13s - 257ms/step - accuracy: 0.9319 - loss: 0.1491 - val_accuracy: 0.8830 - val_loss: 0.2847\n",
            "Epoch 100/100\n",
            "50/50 - 20s - 401ms/step - accuracy: 0.9408 - loss: 0.1434 - val_accuracy: 0.8651 - val_loss: 0.3061\n",
            "\n",
            "Acurácia (FastText Embeddings + LSTM): 0.8651\n"
          ]
        }
      ],
      "source": [
        "# Célula 8: Treinamento e Avaliação\n",
        "y_train_np = np.array(y_train)\n",
        "y_test_np = np.array(y_test)\n",
        "\n",
        "print(\"\\nTreinando o modelo FT+LSTM...\")\n",
        "\n",
        "historico = modelo_ft_lstm.fit(\n",
        "    X_train_pad,\n",
        "    y_train_np,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_pad, y_test_np),\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "loss, acuracia_ft_lstm = modelo_ft_lstm.evaluate(X_test_pad, y_test_np, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia (FastText Embeddings + LSTM): {acuracia_ft_lstm:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}