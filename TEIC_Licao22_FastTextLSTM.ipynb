{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsansao/teic-20231/blob/main/TEIC_Licao22_FastTextLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLXxBrCTAVm-"
      },
      "source": [
        "# Abordagem 1: Classificação com FastText (Embeddings) e LSTM\n",
        "\n",
        "Este notebook demonstra como usar o **FastText** como um gerador de *embeddings* (vetores de palavras) que são então usados para alimentar uma rede LSTM no Keras.\n",
        "\n",
        "A principal vantagem do FastText sobre o Word2Vec é o uso de **n-gramas de caracteres** (informação de subpalavras). Isso permite que ele crie vetores para palavras que não estavam no vocabulário de treino (OOV - Out-of-Vocabulary).\n",
        "\n",
        "O fluxo é quase idêntico ao do notebook Word2Vec:\n",
        "1. Carregar dados.\n",
        "2. Treinar um modelo FastText (via `gensim`) nos textos.\n",
        "3. Preparar dados para o Keras (Tokenizer, Padding).\n",
        "4. Criar a Matriz de Embedding usando os vetores do FastText.\n",
        "5. Construir, treinar e avaliar o modelo LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5Mz7Ve8AVnH",
        "outputId": "734aab8d-d618-4941-917b-603cd941067f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ],
      "source": [
        "# Célula 1: Instalação e Importações\n",
        "!pip install gensim\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import FastText # <-- A MUDANÇA ESTÁ AQUI\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaXWqcKEAVnP"
      },
      "source": [
        "## Passo 1: Carregar e Preparar Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWmpRNI2AVnU",
        "outputId": "0657778c-4dda-4094-e5ff-124b592f1352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amostras de treino: 1571\n",
            "Amostras de teste: 393\n"
          ]
        }
      ],
      "source": [
        "# Célula 2: Carregar os dados\n",
        "categorias = ['comp.graphics', 'sci.crypt']\n",
        "dados = fetch_20newsgroups(subset='all', categories=categorias, shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(dados.data, dados.target, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Amostras de treino: {len(X_train_text)}\")\n",
        "print(f\"Amostras de teste: {len(X_test_text)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C25-UX8JAVnY"
      },
      "source": [
        "## Passo 2: Treinar o Modelo FastText (Gensim)\n",
        "\n",
        "O processo é o mesmo do Word2Vec, mas trocamos a classe `Word2Vec` por `FastText`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm4m7hXlAVna",
        "outputId": "a3033bf8-e9da-4f0e-8a82-b91784f15910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeiro documento tokenizado (para Gensim):\n",
            "[]...\n"
          ]
        }
      ],
      "source": [
        "# Célula 3: Pré-processamento para o Gensim\n",
        "def preprocess_text_gensim(text):\n",
        "    text = re.sub(r'\\W+', ' ', text) # Remove caracteres não-alfanuméricos\n",
        "    text = text.lower()\n",
        "    return text.split()\n",
        "\n",
        "textos_completos = X_train_text + X_test_text\n",
        "textos_tokenizados = [preprocess_text_gensim(doc) for doc in textos_completos]\n",
        "\n",
        "print(f\"Primeiro documento tokenizado (para Gensim):\\n{textos_tokenizados[0][:20]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77p27rHXAVnd",
        "outputId": "4946f3fd-cbd5-42e4-fa72-a617c0d348c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando modelo FastText...\n",
            "Treino do FastText concluído.\n",
            "Tamanho do vocabulário do FastText (com min_count=3): 9871 palavras\n"
          ]
        }
      ],
      "source": [
        "# Célula 4: Treinar o modelo FastText\n",
        "\n",
        "DIM_EMBEDDING = 100\n",
        "WINDOW_SIZE = 5\n",
        "MIN_COUNT = 3\n",
        "WORKERS = 4\n",
        "\n",
        "print(\"Treinando modelo FastText...\")\n",
        "ft_model = FastText( # <-- A MUDANÇA ESTÁ AQUI\n",
        "    sentences=textos_tokenizados,\n",
        "    vector_size=DIM_EMBEDDING,\n",
        "    window=WINDOW_SIZE,\n",
        "    min_count=MIN_COUNT,\n",
        "    workers=WORKERS,\n",
        "    min_n=3, # Tamanho mínimo dos n-gramas de caracteres\n",
        "    max_n=6  # Tamanho máximo dos n-gramas de caracteres\n",
        ")\n",
        "\n",
        "print(\"Treino do FastText concluído.\")\n",
        "vocab_size_ft = len(ft_model.wv.key_to_index)\n",
        "print(f\"Tamanho do vocabulário do FastText (com min_count={MIN_COUNT}): {vocab_size_ft} palavras\")\n",
        "\n",
        "# A grande vantagem: gerar vetor para palavra OOV\n",
        "palavra_teste_oov = 'ultramegasupercomputador'\n",
        "if palavra_teste_oov not in ft_model.wv:\n",
        "    print(f\"'{palavra_teste_oov}' NÃO está no vocabulário treinado.\")\n",
        "    # Mas ainda podemos obter um vetor!\n",
        "    vetor_oov = ft_model.wv[palavra_teste_oov]\n",
        "    print(f\"Mas o FastText gerou um vetor para ela: {vetor_oov.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amOh3nINAVnh"
      },
      "source": [
        "## Passo 3 & 4: Preparar Keras e Matriz de Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqRsNTizAVnl",
        "outputId": "ebaa80c4-17e7-4381-99bc-629c5c067510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho do vocabulário do Keras: 10000\n"
          ]
        }
      ],
      "source": [
        "# Célula 5: Tokenização e Padding do Keras\n",
        "MAX_PALAVRAS_VOCAB = 10000\n",
        "MAX_COMPRIMENTO_SEQ = 250\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_PALAVRAS_VOCAB, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train_text)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_COMPRIMENTO_SEQ, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_COMPRIMENTO_SEQ, padding='post', truncating='post')\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size_keras = min(MAX_PALAVRAS_VOCAB, len(word_index) + 1)\n",
        "\n",
        "print(f\"Tamanho do vocabulário do Keras: {vocab_size_keras}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYjcfYmaAVnt",
        "outputId": "c9a99694-048d-4509-f6bb-c5248c286aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Embedding criada com formato: (10000, 100)\n",
            "9999 palavras do Keras mapeadas.\n",
            "0 palavras não mapeadas (improvável com FastText).\n"
          ]
        }
      ],
      "source": [
        "# Célula 6: Construção da Matriz de Embedding (usando ft_model)\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size_keras, DIM_EMBEDDING))\n",
        "palavras_encontradas = 0\n",
        "palavras_nao_encontradas = 0\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i >= MAX_PALAVRAS_VOCAB:\n",
        "        continue\n",
        "\n",
        "    # Usamos o ft_model.wv. A vantagem é que mesmo que 'word'\n",
        "    # não esteja no vocabulário treinado (por min_count),\n",
        "    # o FastText pode inferir um vetor para ela.\n",
        "    try:\n",
        "        embedding_matrix[i] = ft_model.wv[word]\n",
        "        palavras_encontradas += 1\n",
        "    except KeyError:\n",
        "        # No FastText (gensim >= 4.0), isso é raro, mas tratamos por segurança\n",
        "        embedding_matrix[i] = np.zeros(DIM_EMBEDDING)\n",
        "        palavras_nao_encontradas += 1\n",
        "\n",
        "print(f\"Matriz de Embedding criada com formato: {embedding_matrix.shape}\")\n",
        "print(f\"{palavras_encontradas} palavras do Keras mapeadas.\")\n",
        "print(f\"{palavras_nao_encontradas} palavras não mapeadas (improvável com FastText).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQPXSo-JAVnw"
      },
      "source": [
        "## Passo 5 & 6: Construir e Treinar o Modelo LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "_BsAO9woAVn1",
        "outputId": "1753759c-23a6-44c7-ee8e-601a131e8e3d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Modelo_FastText_LSTM\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Modelo_FastText_LSTM\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │     \u001b[38;5;34m1,000,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ spatial_dropout1d_1             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ spatial_dropout1d_1             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,000,000\u001b[0m (3.81 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> (3.81 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,000,000\u001b[0m (3.81 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> (3.81 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Célula 7: Definindo o modelo LSTM\n",
        "modelo_ft_lstm = Sequential(name=\"Modelo_FastText_LSTM\")\n",
        "\n",
        "modelo_ft_lstm.add(Embedding(\n",
        "    input_dim=vocab_size_keras,\n",
        "    output_dim=DIM_EMBEDDING,\n",
        "    input_length=MAX_COMPRIMENTO_SEQ,\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=False # Congelar a camada\n",
        "))\n",
        "\n",
        "modelo_ft_lstm.add(SpatialDropout1D(0.2))\n",
        "modelo_ft_lstm.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "modelo_ft_lstm.add(Dense(16, activation='relu'))\n",
        "modelo_ft_lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "modelo_ft_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "modelo_ft_lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGRySRisAVn2",
        "outputId": "2623881a-90f3-4af1-b553-07834f7192f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treinando o modelo FT+LSTM...\n",
            "Epoch 1/100\n",
            "50/50 - 18s - 354ms/step - accuracy: 0.5188 - loss: 0.6860 - val_accuracy: 0.5471 - val_loss: 0.6789\n",
            "Epoch 2/100\n",
            "50/50 - 14s - 278ms/step - accuracy: 0.5398 - loss: 0.6787 - val_accuracy: 0.5547 - val_loss: 0.6766\n",
            "Epoch 3/100\n",
            "50/50 - 19s - 389ms/step - accuracy: 0.6416 - loss: 0.6491 - val_accuracy: 0.5598 - val_loss: 0.7477\n",
            "Epoch 4/100\n",
            "50/50 - 13s - 263ms/step - accuracy: 0.5773 - loss: 0.6640 - val_accuracy: 0.5369 - val_loss: 0.6758\n",
            "Epoch 5/100\n",
            "50/50 - 20s - 399ms/step - accuracy: 0.5856 - loss: 0.6424 - val_accuracy: 0.6921 - val_loss: 0.5926\n",
            "Epoch 6/100\n",
            "50/50 - 21s - 423ms/step - accuracy: 0.6620 - loss: 0.6276 - val_accuracy: 0.7023 - val_loss: 0.5669\n",
            "Epoch 7/100\n",
            "50/50 - 13s - 255ms/step - accuracy: 0.7015 - loss: 0.6061 - val_accuracy: 0.7557 - val_loss: 0.5460\n",
            "Epoch 8/100\n",
            "50/50 - 13s - 257ms/step - accuracy: 0.7377 - loss: 0.5672 - val_accuracy: 0.7863 - val_loss: 0.5206\n",
            "Epoch 9/100\n",
            "50/50 - 13s - 257ms/step - accuracy: 0.6875 - loss: 0.5992 - val_accuracy: 0.6692 - val_loss: 0.5955\n",
            "Epoch 10/100\n",
            "50/50 - 20s - 407ms/step - accuracy: 0.7498 - loss: 0.5620 - val_accuracy: 0.7506 - val_loss: 0.5579\n",
            "Epoch 11/100\n",
            "50/50 - 21s - 410ms/step - accuracy: 0.7486 - loss: 0.5485 - val_accuracy: 0.5394 - val_loss: 0.7479\n",
            "Epoch 12/100\n",
            "50/50 - 13s - 257ms/step - accuracy: 0.5697 - loss: 0.6631 - val_accuracy: 0.5573 - val_loss: 0.6735\n",
            "Epoch 13/100\n",
            "50/50 - 13s - 256ms/step - accuracy: 0.5691 - loss: 0.6593 - val_accuracy: 0.5242 - val_loss: 0.6620\n",
            "Epoch 14/100\n",
            "50/50 - 20s - 408ms/step - accuracy: 0.6104 - loss: 0.6219 - val_accuracy: 0.7201 - val_loss: 0.5676\n",
            "Epoch 15/100\n",
            "50/50 - 21s - 411ms/step - accuracy: 0.7282 - loss: 0.5747 - val_accuracy: 0.6896 - val_loss: 0.6141\n",
            "Epoch 16/100\n",
            "50/50 - 14s - 280ms/step - accuracy: 0.7740 - loss: 0.5238 - val_accuracy: 0.7863 - val_loss: 0.5035\n",
            "Epoch 17/100\n",
            "50/50 - 13s - 255ms/step - accuracy: 0.7632 - loss: 0.5305 - val_accuracy: 0.7659 - val_loss: 0.5139\n",
            "Epoch 18/100\n",
            "50/50 - 13s - 256ms/step - accuracy: 0.7842 - loss: 0.5180 - val_accuracy: 0.8142 - val_loss: 0.4788\n",
            "Epoch 19/100\n",
            "50/50 - 20s - 405ms/step - accuracy: 0.7899 - loss: 0.5064 - val_accuracy: 0.7888 - val_loss: 0.5142\n",
            "Epoch 20/100\n",
            "50/50 - 21s - 412ms/step - accuracy: 0.7906 - loss: 0.4952 - val_accuracy: 0.7990 - val_loss: 0.4897\n",
            "Epoch 21/100\n",
            "50/50 - 13s - 255ms/step - accuracy: 0.7753 - loss: 0.5060 - val_accuracy: 0.7990 - val_loss: 0.4750\n",
            "Epoch 22/100\n",
            "50/50 - 20s - 409ms/step - accuracy: 0.7549 - loss: 0.5433 - val_accuracy: 0.5420 - val_loss: 0.7465\n",
            "Epoch 23/100\n",
            "50/50 - 20s - 409ms/step - accuracy: 0.7110 - loss: 0.5710 - val_accuracy: 0.7608 - val_loss: 0.5094\n",
            "Epoch 24/100\n",
            "50/50 - 13s - 254ms/step - accuracy: 0.7276 - loss: 0.5567 - val_accuracy: 0.6005 - val_loss: 0.6594\n",
            "Epoch 25/100\n",
            "50/50 - 21s - 412ms/step - accuracy: 0.6836 - loss: 0.5971 - val_accuracy: 0.5649 - val_loss: 0.6907\n",
            "Epoch 26/100\n",
            "50/50 - 13s - 255ms/step - accuracy: 0.6550 - loss: 0.6085 - val_accuracy: 0.6743 - val_loss: 0.5802\n",
            "Epoch 27/100\n",
            "50/50 - 21s - 411ms/step - accuracy: 0.7428 - loss: 0.5460 - val_accuracy: 0.7990 - val_loss: 0.4763\n",
            "Epoch 28/100\n",
            "50/50 - 13s - 260ms/step - accuracy: 0.7428 - loss: 0.5271 - val_accuracy: 0.7863 - val_loss: 0.5192\n",
            "Epoch 29/100\n",
            "50/50 - 13s - 258ms/step - accuracy: 0.8160 - loss: 0.4669 - val_accuracy: 0.8117 - val_loss: 0.4679\n",
            "Epoch 30/100\n",
            "50/50 - 14s - 279ms/step - accuracy: 0.7976 - loss: 0.4770 - val_accuracy: 0.7557 - val_loss: 0.5289\n",
            "Epoch 31/100\n",
            "50/50 - 19s - 383ms/step - accuracy: 0.7218 - loss: 0.5385 - val_accuracy: 0.5445 - val_loss: 0.5803\n",
            "Epoch 32/100\n",
            "50/50 - 13s - 254ms/step - accuracy: 0.7530 - loss: 0.5031 - val_accuracy: 0.7964 - val_loss: 0.4496\n",
            "Epoch 33/100\n",
            "50/50 - 21s - 412ms/step - accuracy: 0.7931 - loss: 0.4559 - val_accuracy: 0.8270 - val_loss: 0.4260\n",
            "Epoch 34/100\n",
            "50/50 - 13s - 256ms/step - accuracy: 0.7919 - loss: 0.4584 - val_accuracy: 0.8295 - val_loss: 0.4315\n",
            "Epoch 35/100\n",
            "50/50 - 20s - 406ms/step - accuracy: 0.8014 - loss: 0.4511 - val_accuracy: 0.7888 - val_loss: 0.4556\n",
            "Epoch 36/100\n",
            "50/50 - 13s - 253ms/step - accuracy: 0.8180 - loss: 0.4348 - val_accuracy: 0.8066 - val_loss: 0.4602\n",
            "Epoch 37/100\n",
            "50/50 - 12s - 250ms/step - accuracy: 0.6665 - loss: 0.6325 - val_accuracy: 0.6056 - val_loss: 0.6554\n",
            "Epoch 38/100\n",
            "50/50 - 13s - 253ms/step - accuracy: 0.6334 - loss: 0.6456 - val_accuracy: 0.6005 - val_loss: 0.6468\n",
            "Epoch 39/100\n",
            "50/50 - 13s - 252ms/step - accuracy: 0.6677 - loss: 0.6092 - val_accuracy: 0.6565 - val_loss: 0.6027\n",
            "Epoch 40/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.7008 - loss: 0.5820 - val_accuracy: 0.7252 - val_loss: 0.5653\n",
            "Epoch 41/100\n",
            "50/50 - 13s - 253ms/step - accuracy: 0.7263 - loss: 0.5545 - val_accuracy: 0.7252 - val_loss: 0.5605\n",
            "Epoch 42/100\n",
            "50/50 - 13s - 250ms/step - accuracy: 0.7613 - loss: 0.5363 - val_accuracy: 0.7812 - val_loss: 0.5075\n",
            "Epoch 43/100\n",
            "50/50 - 20s - 409ms/step - accuracy: 0.7747 - loss: 0.5136 - val_accuracy: 0.7379 - val_loss: 0.5607\n",
            "Epoch 44/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.8084 - loss: 0.4709 - val_accuracy: 0.8142 - val_loss: 0.4687\n",
            "Epoch 45/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.8173 - loss: 0.4526 - val_accuracy: 0.8193 - val_loss: 0.4301\n",
            "Epoch 46/100\n",
            "50/50 - 20s - 409ms/step - accuracy: 0.8167 - loss: 0.4468 - val_accuracy: 0.7863 - val_loss: 0.4730\n",
            "Epoch 47/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.8046 - loss: 0.4565 - val_accuracy: 0.7913 - val_loss: 0.4882\n",
            "Epoch 48/100\n",
            "50/50 - 20s - 408ms/step - accuracy: 0.8167 - loss: 0.4382 - val_accuracy: 0.8015 - val_loss: 0.4197\n",
            "Epoch 49/100\n",
            "50/50 - 21s - 411ms/step - accuracy: 0.7969 - loss: 0.4384 - val_accuracy: 0.8015 - val_loss: 0.4486\n",
            "Epoch 50/100\n",
            "50/50 - 13s - 252ms/step - accuracy: 0.7938 - loss: 0.4514 - val_accuracy: 0.8295 - val_loss: 0.4087\n",
            "Epoch 51/100\n",
            "50/50 - 12s - 250ms/step - accuracy: 0.8129 - loss: 0.4363 - val_accuracy: 0.8117 - val_loss: 0.4398\n",
            "Epoch 52/100\n",
            "50/50 - 21s - 415ms/step - accuracy: 0.8243 - loss: 0.4239 - val_accuracy: 0.8346 - val_loss: 0.4202\n",
            "Epoch 53/100\n",
            "50/50 - 20s - 405ms/step - accuracy: 0.8205 - loss: 0.4337 - val_accuracy: 0.8168 - val_loss: 0.4560\n",
            "Epoch 54/100\n",
            "50/50 - 20s - 409ms/step - accuracy: 0.8269 - loss: 0.4237 - val_accuracy: 0.8448 - val_loss: 0.4160\n",
            "Epoch 55/100\n",
            "50/50 - 20s - 409ms/step - accuracy: 0.8262 - loss: 0.4157 - val_accuracy: 0.8168 - val_loss: 0.4190\n",
            "Epoch 56/100\n",
            "50/50 - 12s - 248ms/step - accuracy: 0.8218 - loss: 0.4110 - val_accuracy: 0.8397 - val_loss: 0.4044\n",
            "Epoch 57/100\n",
            "50/50 - 21s - 411ms/step - accuracy: 0.8300 - loss: 0.4029 - val_accuracy: 0.8244 - val_loss: 0.4133\n",
            "Epoch 58/100\n",
            "50/50 - 12s - 247ms/step - accuracy: 0.8345 - loss: 0.4028 - val_accuracy: 0.8346 - val_loss: 0.3891\n",
            "Epoch 59/100\n",
            "50/50 - 14s - 272ms/step - accuracy: 0.8199 - loss: 0.4197 - val_accuracy: 0.8397 - val_loss: 0.3879\n",
            "Epoch 60/100\n",
            "50/50 - 12s - 244ms/step - accuracy: 0.8364 - loss: 0.4036 - val_accuracy: 0.8372 - val_loss: 0.3925\n",
            "Epoch 61/100\n",
            "50/50 - 21s - 415ms/step - accuracy: 0.8453 - loss: 0.4012 - val_accuracy: 0.8346 - val_loss: 0.4274\n",
            "Epoch 62/100\n",
            "50/50 - 20s - 403ms/step - accuracy: 0.8421 - loss: 0.3975 - val_accuracy: 0.8397 - val_loss: 0.3941\n",
            "Epoch 63/100\n",
            "50/50 - 21s - 411ms/step - accuracy: 0.8396 - loss: 0.3937 - val_accuracy: 0.8321 - val_loss: 0.3962\n",
            "Epoch 64/100\n",
            "50/50 - 12s - 250ms/step - accuracy: 0.8313 - loss: 0.4024 - val_accuracy: 0.8295 - val_loss: 0.4079\n",
            "Epoch 65/100\n",
            "50/50 - 20s - 406ms/step - accuracy: 0.8027 - loss: 0.4356 - val_accuracy: 0.8244 - val_loss: 0.3954\n",
            "Epoch 66/100\n",
            "50/50 - 21s - 412ms/step - accuracy: 0.8256 - loss: 0.3929 - val_accuracy: 0.8270 - val_loss: 0.3888\n",
            "Epoch 67/100\n",
            "50/50 - 20s - 406ms/step - accuracy: 0.8396 - loss: 0.3967 - val_accuracy: 0.8372 - val_loss: 0.4054\n",
            "Epoch 68/100\n",
            "50/50 - 12s - 245ms/step - accuracy: 0.8428 - loss: 0.3955 - val_accuracy: 0.8321 - val_loss: 0.4079\n",
            "Epoch 69/100\n",
            "50/50 - 21s - 415ms/step - accuracy: 0.8447 - loss: 0.3946 - val_accuracy: 0.8346 - val_loss: 0.4209\n",
            "Epoch 70/100\n",
            "50/50 - 12s - 248ms/step - accuracy: 0.8440 - loss: 0.3828 - val_accuracy: 0.8372 - val_loss: 0.3936\n",
            "Epoch 71/100\n",
            "50/50 - 20s - 409ms/step - accuracy: 0.8447 - loss: 0.3880 - val_accuracy: 0.8346 - val_loss: 0.3773\n",
            "Epoch 72/100\n",
            "50/50 - 21s - 425ms/step - accuracy: 0.8370 - loss: 0.3807 - val_accuracy: 0.8372 - val_loss: 0.3925\n",
            "Epoch 73/100\n",
            "50/50 - 13s - 250ms/step - accuracy: 0.8390 - loss: 0.3791 - val_accuracy: 0.8499 - val_loss: 0.3731\n",
            "Epoch 74/100\n",
            "50/50 - 20s - 408ms/step - accuracy: 0.8294 - loss: 0.3971 - val_accuracy: 0.8473 - val_loss: 0.4069\n",
            "Epoch 75/100\n",
            "50/50 - 20s - 406ms/step - accuracy: 0.8351 - loss: 0.3953 - val_accuracy: 0.8142 - val_loss: 0.3991\n",
            "Epoch 76/100\n",
            "50/50 - 21s - 412ms/step - accuracy: 0.8307 - loss: 0.3977 - val_accuracy: 0.8499 - val_loss: 0.3840\n",
            "Epoch 77/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.8587 - loss: 0.3576 - val_accuracy: 0.8422 - val_loss: 0.3848\n",
            "Epoch 78/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.8402 - loss: 0.3699 - val_accuracy: 0.8422 - val_loss: 0.3693\n",
            "Epoch 79/100\n",
            "50/50 - 12s - 248ms/step - accuracy: 0.8440 - loss: 0.3785 - val_accuracy: 0.8397 - val_loss: 0.3863\n",
            "Epoch 80/100\n",
            "50/50 - 12s - 249ms/step - accuracy: 0.8530 - loss: 0.3620 - val_accuracy: 0.8372 - val_loss: 0.3800\n",
            "Epoch 81/100\n",
            "50/50 - 12s - 248ms/step - accuracy: 0.8396 - loss: 0.3769 - val_accuracy: 0.8499 - val_loss: 0.3622\n",
            "Epoch 82/100\n",
            "50/50 - 12s - 244ms/step - accuracy: 0.8383 - loss: 0.3871 - val_accuracy: 0.8244 - val_loss: 0.4295\n",
            "Epoch 83/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.8199 - loss: 0.4127 - val_accuracy: 0.7863 - val_loss: 0.4555\n",
            "Epoch 84/100\n",
            "50/50 - 12s - 245ms/step - accuracy: 0.8523 - loss: 0.3755 - val_accuracy: 0.8397 - val_loss: 0.3774\n",
            "Epoch 85/100\n",
            "50/50 - 12s - 240ms/step - accuracy: 0.8568 - loss: 0.3486 - val_accuracy: 0.8041 - val_loss: 0.4742\n",
            "Epoch 86/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.8173 - loss: 0.4190 - val_accuracy: 0.8066 - val_loss: 0.4219\n",
            "Epoch 87/100\n",
            "50/50 - 22s - 435ms/step - accuracy: 0.8300 - loss: 0.3955 - val_accuracy: 0.8448 - val_loss: 0.3851\n",
            "Epoch 88/100\n",
            "50/50 - 12s - 248ms/step - accuracy: 0.8523 - loss: 0.3706 - val_accuracy: 0.8550 - val_loss: 0.3949\n",
            "Epoch 89/100\n",
            "50/50 - 20s - 404ms/step - accuracy: 0.8606 - loss: 0.3517 - val_accuracy: 0.8372 - val_loss: 0.3831\n",
            "Epoch 90/100\n",
            "50/50 - 21s - 420ms/step - accuracy: 0.8523 - loss: 0.3458 - val_accuracy: 0.8550 - val_loss: 0.3716\n",
            "Epoch 91/100\n",
            "50/50 - 20s - 406ms/step - accuracy: 0.8511 - loss: 0.3582 - val_accuracy: 0.8524 - val_loss: 0.3632\n",
            "Epoch 92/100\n",
            "50/50 - 20s - 408ms/step - accuracy: 0.8504 - loss: 0.3554 - val_accuracy: 0.8550 - val_loss: 0.3509\n",
            "Epoch 93/100\n",
            "50/50 - 20s - 407ms/step - accuracy: 0.8491 - loss: 0.3342 - val_accuracy: 0.8524 - val_loss: 0.3469\n",
            "Epoch 94/100\n",
            "50/50 - 13s - 251ms/step - accuracy: 0.8593 - loss: 0.3375 - val_accuracy: 0.8346 - val_loss: 0.3866\n",
            "Epoch 95/100\n",
            "50/50 - 13s - 253ms/step - accuracy: 0.8619 - loss: 0.3398 - val_accuracy: 0.8397 - val_loss: 0.3853\n",
            "Epoch 96/100\n",
            "50/50 - 20s - 399ms/step - accuracy: 0.8466 - loss: 0.3513 - val_accuracy: 0.8448 - val_loss: 0.3668\n",
            "Epoch 97/100\n",
            "50/50 - 12s - 245ms/step - accuracy: 0.8517 - loss: 0.3400 - val_accuracy: 0.8473 - val_loss: 0.3513\n",
            "Epoch 98/100\n",
            "50/50 - 12s - 248ms/step - accuracy: 0.8752 - loss: 0.3146 - val_accuracy: 0.8499 - val_loss: 0.3354\n",
            "Epoch 99/100\n",
            "50/50 - 20s - 405ms/step - accuracy: 0.8568 - loss: 0.3256 - val_accuracy: 0.8499 - val_loss: 0.3371\n",
            "Epoch 100/100\n",
            "50/50 - 21s - 425ms/step - accuracy: 0.8663 - loss: 0.3139 - val_accuracy: 0.8473 - val_loss: 0.3322\n",
            "\n",
            "Acurácia (FastText Embeddings + LSTM): 0.8473\n"
          ]
        }
      ],
      "source": [
        "# Célula 8: Treinamento e Avaliação\n",
        "y_train_np = np.array(y_train)\n",
        "y_test_np = np.array(y_test)\n",
        "\n",
        "print(\"\\nTreinando o modelo FT+LSTM...\")\n",
        "\n",
        "historico = modelo_ft_lstm.fit(\n",
        "    X_train_pad,\n",
        "    y_train_np,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_pad, y_test_np),\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "loss, acuracia_ft_lstm = modelo_ft_lstm.evaluate(X_test_pad, y_test_np, verbose=0)\n",
        "\n",
        "print(f\"\\nAcurácia (FastText Embeddings + LSTM): {acuracia_ft_lstm:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}