{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsansao/teic-20231/blob/main/TEIC_Licao32bis_Nanogpt_pt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWV6epU0xaWG"
      },
      "source": [
        "# 📖 Demonstração: Treinando um nanoGPT do Zero\n",
        "\n",
        "Este notebook irá demonstrar como treinar um modelo de linguagem (GPT) do zero usando o repositório `nanoGPT` de Andrej Karpathy.\n",
        "\n",
        "Vamos treinar um modelo *baseado em caracteres* no corpus de \"Tiny Shakespeare\" para que ele aprenda a \"falar\" como Shakespeare.\n",
        "\n",
        "### Passo 1: Configurar o Ambiente\n",
        "\n",
        "Primeiro, precisamos garantir que estamos usando uma GPU.\n",
        "\n",
        "1.  Vá em **Ambiente de execução -> Alterar o tipo de ambiente de execução**.\n",
        "2.  Selecione **T4 GPU** (ou qualquer GPU disponível) e clique em `Salvar`.\n",
        "\n",
        "Agora, vamos verificar se o PyTorch consegue detectar a GPU."
      ],
      "id": "uWV6epU0xaWG"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d4RMZrWxaWI",
        "outputId": "d9f2ca7d-cd79-49b6-f172-387fb07ad19d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU (CUDA) detectada! 👍\n",
            "Nome da GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU (CUDA) detectada! 👍\")\n",
        "    print(f\"Nome da GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"GPU não detectada. 👎\")\n",
        "    print(\"Por favor, habilite a GPU no 'Ambiente de execução' para continuar.\")"
      ],
      "id": "7d4RMZrWxaWI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mmpe8eKxaWJ"
      },
      "source": [
        "### Passo 2: Clonar o nanoGPT e Instalar Dependências\n",
        "\n",
        "Vamos baixar o código do GitHub e instalar as bibliotecas necessárias."
      ],
      "id": "7Mmpe8eKxaWJ"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoRmih-2xaWJ",
        "outputId": "a053b973-106d-4018-f9bd-6f2b98e1ee2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 686, done.\u001b[K\n",
            "remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\n",
            "Receiving objects: 100% (686/686), 974.06 KiB | 2.65 MiB/s, done.\n",
            "Resolving deltas: 100% (380/380), done.\n",
            "/content/nanoGPT/nanoGPT\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# 1. Clona o repositório oficial\n",
        "!git clone https://github.com/karpathy/nanoGPT.git\n",
        "\n",
        "# 2. Entra no diretório do projeto\n",
        "# Usamos o comando mágico %cd para mudar o diretório do notebook\n",
        "%cd nanoGPT\n",
        "\n",
        "# 3. Instala as bibliotecas necessárias\n",
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ],
      "id": "CoRmih-2xaWJ"
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L https://raw.githubusercontent.com/jsansao/corpus_ptbr/main/cetenfolha/cetenfolha_aa --output \"dump.txt\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-_8u_6_Lweo",
        "outputId": "046988c3-ed86-47af-8187-718c360f8536"
      },
      "id": "E-_8u_6_Lweo",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 3724k  100 3724k    0     0  6519k      0 --:--:-- --:--:-- --:--:-- 6522k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp dump.txt data/shakespeare_char/input.txt"
      ],
      "metadata": {
        "id": "MVVQZdArMGwO"
      },
      "id": "MVVQZdArMGwO",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFP5l8aUxaWK"
      },
      "source": [
        "### Passo 3: Obter e Preparar os Dados (Corpus de Shakespeare)\n",
        "\n",
        "O repositório já vem com um script que baixa o \"Tiny Shakespeare\" (um arquivo `input.txt`). Este script irá \"tokenizar\" o texto, ou seja, converter todos os caracteres únicos em números."
      ],
      "id": "LFP5l8aUxaWK"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9x1fI0OxaWL",
        "outputId": "f86a9a6e-ac3e-4a13-a446-97f797b05bbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 3,700,016\n",
            "all the unique characters: \n",
            " !\"$%&'()*+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]`abcdefghijklmnopqrstuvwxyz}ª«±º»ÀÁÂÃÇÉÊÍÑÓÔÕÚàáâãåçèéêëíîñóôõùúûü€\n",
            "vocab size: 125\n",
            "train has 3,330,014 tokens\n",
            "val has 370,002 tokens\n"
          ]
        }
      ],
      "source": [
        "# Este script baixa o input.txt e o processa,\n",
        "# criando os arquivos 'train.bin' e 'val.bin' na pasta 'data/shakespeare_char'\n",
        "!python data/shakespeare_char/prepare.py"
      ],
      "id": "-9x1fI0OxaWL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t9e63zzxaWL"
      },
      "source": [
        "### Passo 4: Treinar o Modelo!\n",
        "\n",
        "Este é o momento principal. Vamos iniciar o treinamento usando o arquivo de configuração `train_shakespeare_char.py`.\n",
        "\n",
        "Preste atenção no output: você verá o `iter` (iteração) e o `loss` (erro). O `loss` deve diminuir progressivamente, provando que o modelo está aprendendo."
      ],
      "id": "8t9e63zzxaWL"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyEDdI2GxaWL",
        "outputId": "66bec358-97de-4212-fe8a-9216ae8b1300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 125 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.67M\n",
            "/content/nanoGPT/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 26, with 10,763,136 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "W1030 22:15:52.322000 30031 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "step 0: train loss 4.8960, val loss 4.8977\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "iter 0: loss 4.8961, time 65520.26ms, mfu -100.00%\n",
            "iter 10: loss 3.3787, time 495.60ms, mfu 0.75%\n",
            "iter 20: loss 2.9834, time 492.99ms, mfu 0.75%\n",
            "iter 30: loss 2.7044, time 497.17ms, mfu 0.75%\n",
            "iter 40: loss 2.6088, time 497.94ms, mfu 0.75%\n",
            "iter 50: loss 2.4899, time 498.62ms, mfu 0.75%\n",
            "iter 60: loss 2.5280, time 502.08ms, mfu 0.75%\n",
            "iter 70: loss 2.4874, time 501.93ms, mfu 0.75%\n",
            "iter 80: loss 2.5078, time 503.06ms, mfu 0.75%\n",
            "iter 90: loss 2.4840, time 502.07ms, mfu 0.75%\n",
            "iter 100: loss 2.4612, time 505.65ms, mfu 0.75%\n",
            "iter 110: loss 2.4669, time 504.75ms, mfu 0.75%\n",
            "iter 120: loss 2.4495, time 503.92ms, mfu 0.75%\n",
            "iter 130: loss 2.4621, time 503.61ms, mfu 0.75%\n",
            "iter 140: loss 2.4357, time 506.06ms, mfu 0.75%\n",
            "iter 150: loss 2.4265, time 505.61ms, mfu 0.74%\n",
            "iter 160: loss 2.4198, time 510.39ms, mfu 0.74%\n",
            "iter 170: loss 2.3822, time 508.59ms, mfu 0.74%\n",
            "iter 180: loss 2.3719, time 511.66ms, mfu 0.74%\n",
            "iter 190: loss 2.3488, time 511.49ms, mfu 0.74%\n",
            "iter 200: loss 2.3090, time 512.15ms, mfu 0.74%\n",
            "iter 210: loss 2.2771, time 512.95ms, mfu 0.74%\n",
            "iter 220: loss 2.2565, time 514.34ms, mfu 0.74%\n",
            "iter 230: loss 2.2243, time 513.06ms, mfu 0.74%\n",
            "iter 240: loss 2.1807, time 514.00ms, mfu 0.73%\n",
            "step 250: train loss 2.1201, val loss 2.1206\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.1755, time 73476.80ms, mfu 0.66%\n",
            "iter 260: loss 2.1569, time 518.88ms, mfu 0.67%\n",
            "iter 270: loss 2.1186, time 522.83ms, mfu 0.67%\n",
            "iter 280: loss 2.1012, time 523.00ms, mfu 0.68%\n",
            "iter 290: loss 2.0812, time 520.57ms, mfu 0.68%\n",
            "iter 300: loss 2.0635, time 521.80ms, mfu 0.68%\n",
            "iter 310: loss 2.0400, time 522.96ms, mfu 0.69%\n",
            "iter 320: loss 2.0400, time 523.08ms, mfu 0.69%\n",
            "iter 330: loss 2.0077, time 523.80ms, mfu 0.69%\n",
            "iter 340: loss 2.0123, time 523.18ms, mfu 0.69%\n",
            "iter 350: loss 1.9984, time 523.33ms, mfu 0.70%\n",
            "iter 360: loss 1.9425, time 524.35ms, mfu 0.70%\n",
            "iter 370: loss 1.9751, time 524.33ms, mfu 0.70%\n",
            "iter 380: loss 1.9594, time 525.72ms, mfu 0.70%\n",
            "iter 390: loss 1.8737, time 524.12ms, mfu 0.70%\n",
            "iter 400: loss 1.9226, time 523.86ms, mfu 0.70%\n",
            "iter 410: loss 1.9137, time 524.05ms, mfu 0.70%\n",
            "iter 420: loss 1.8683, time 525.20ms, mfu 0.70%\n",
            "iter 430: loss 1.8655, time 524.93ms, mfu 0.70%\n",
            "iter 440: loss 1.8376, time 525.94ms, mfu 0.71%\n",
            "iter 450: loss 1.8295, time 526.33ms, mfu 0.71%\n",
            "iter 460: loss 1.8400, time 523.10ms, mfu 0.71%\n",
            "iter 470: loss 1.8109, time 524.10ms, mfu 0.71%\n",
            "iter 480: loss 1.8226, time 525.86ms, mfu 0.71%\n",
            "iter 490: loss 1.8339, time 524.11ms, mfu 0.71%\n",
            "step 500: train loss 1.7008, val loss 1.7078\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.7962, time 74056.25ms, mfu 0.64%\n",
            "iter 510: loss 1.7818, time 524.02ms, mfu 0.65%\n",
            "iter 520: loss 1.7680, time 525.71ms, mfu 0.65%\n",
            "iter 530: loss 1.7832, time 525.27ms, mfu 0.66%\n",
            "iter 540: loss 1.7745, time 525.40ms, mfu 0.66%\n",
            "iter 550: loss 1.7514, time 524.66ms, mfu 0.67%\n",
            "iter 560: loss 1.7566, time 524.57ms, mfu 0.67%\n",
            "iter 570: loss 1.6904, time 526.23ms, mfu 0.68%\n",
            "iter 580: loss 1.7036, time 523.95ms, mfu 0.68%\n",
            "iter 590: loss 1.7347, time 524.59ms, mfu 0.68%\n",
            "iter 600: loss 1.6505, time 524.51ms, mfu 0.69%\n",
            "iter 610: loss 1.6865, time 524.95ms, mfu 0.69%\n",
            "iter 620: loss 1.6784, time 523.78ms, mfu 0.69%\n",
            "iter 630: loss 1.6774, time 525.30ms, mfu 0.69%\n",
            "iter 640: loss 1.6650, time 525.00ms, mfu 0.69%\n",
            "iter 650: loss 1.6555, time 526.89ms, mfu 0.70%\n",
            "iter 660: loss 1.6593, time 526.42ms, mfu 0.70%\n",
            "iter 670: loss 1.6220, time 527.58ms, mfu 0.70%\n",
            "iter 680: loss 1.6100, time 525.23ms, mfu 0.70%\n",
            "iter 690: loss 1.6183, time 527.66ms, mfu 0.70%\n",
            "iter 700: loss 1.5902, time 528.34ms, mfu 0.70%\n",
            "iter 710: loss 1.6188, time 527.61ms, mfu 0.70%\n",
            "iter 720: loss 1.6050, time 527.53ms, mfu 0.70%\n",
            "iter 730: loss 1.5718, time 527.35ms, mfu 0.70%\n",
            "iter 740: loss 1.6046, time 527.05ms, mfu 0.70%\n",
            "step 750: train loss 1.4870, val loss 1.5050\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.6042, time 74132.99ms, mfu 0.63%\n",
            "iter 760: loss 1.5982, time 524.74ms, mfu 0.64%\n",
            "iter 770: loss 1.5184, time 526.38ms, mfu 0.65%\n",
            "iter 780: loss 1.6023, time 523.29ms, mfu 0.65%\n",
            "iter 790: loss 1.5737, time 526.24ms, mfu 0.66%\n",
            "iter 800: loss 1.5603, time 526.14ms, mfu 0.67%\n",
            "iter 810: loss 1.5475, time 524.10ms, mfu 0.67%\n",
            "iter 820: loss 1.5694, time 524.56ms, mfu 0.67%\n",
            "iter 830: loss 1.5487, time 526.11ms, mfu 0.68%\n",
            "iter 840: loss 1.4802, time 525.93ms, mfu 0.68%\n",
            "iter 850: loss 1.5299, time 524.60ms, mfu 0.68%\n",
            "iter 860: loss 1.5317, time 524.90ms, mfu 0.69%\n",
            "iter 870: loss 1.5152, time 527.31ms, mfu 0.69%\n",
            "iter 880: loss 1.5105, time 525.36ms, mfu 0.69%\n",
            "iter 890: loss 1.4825, time 527.26ms, mfu 0.69%\n",
            "iter 900: loss 1.4848, time 525.94ms, mfu 0.69%\n",
            "iter 910: loss 1.4895, time 524.84ms, mfu 0.70%\n",
            "iter 920: loss 1.5151, time 527.66ms, mfu 0.70%\n",
            "iter 930: loss 1.5081, time 524.22ms, mfu 0.70%\n",
            "iter 940: loss 1.5214, time 525.20ms, mfu 0.70%\n",
            "iter 950: loss 1.4212, time 525.97ms, mfu 0.70%\n",
            "iter 960: loss 1.4854, time 525.79ms, mfu 0.70%\n",
            "iter 970: loss 1.4915, time 526.75ms, mfu 0.70%\n",
            "iter 980: loss 1.4439, time 525.68ms, mfu 0.70%\n",
            "iter 990: loss 1.4288, time 525.71ms, mfu 0.70%\n",
            "step 1000: train loss 1.3792, val loss 1.4053\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.4608, time 74266.78ms, mfu 0.63%\n",
            "iter 1010: loss 1.4327, time 522.82ms, mfu 0.64%\n",
            "iter 1020: loss 1.4524, time 526.17ms, mfu 0.65%\n",
            "iter 1030: loss 1.4842, time 528.15ms, mfu 0.65%\n",
            "iter 1040: loss 1.4532, time 525.72ms, mfu 0.66%\n",
            "iter 1050: loss 1.4567, time 527.65ms, mfu 0.66%\n",
            "iter 1060: loss 1.4432, time 524.47ms, mfu 0.67%\n",
            "iter 1070: loss 1.4432, time 526.12ms, mfu 0.67%\n",
            "iter 1080: loss 1.3818, time 525.65ms, mfu 0.68%\n",
            "iter 1090: loss 1.4604, time 526.77ms, mfu 0.68%\n",
            "iter 1100: loss 1.4301, time 525.08ms, mfu 0.68%\n",
            "iter 1110: loss 1.4266, time 524.68ms, mfu 0.69%\n",
            "iter 1120: loss 1.4077, time 526.10ms, mfu 0.69%\n",
            "iter 1130: loss 1.4453, time 525.68ms, mfu 0.69%\n",
            "iter 1140: loss 1.4627, time 526.50ms, mfu 0.69%\n",
            "iter 1150: loss 1.4549, time 525.62ms, mfu 0.69%\n",
            "iter 1160: loss 1.4549, time 526.22ms, mfu 0.70%\n",
            "iter 1170: loss 1.4268, time 524.18ms, mfu 0.70%\n",
            "iter 1180: loss 1.3958, time 527.77ms, mfu 0.70%\n",
            "iter 1190: loss 1.4088, time 526.51ms, mfu 0.70%\n",
            "iter 1200: loss 1.3933, time 527.50ms, mfu 0.70%\n",
            "iter 1210: loss 1.4128, time 524.30ms, mfu 0.70%\n",
            "iter 1220: loss 1.3869, time 526.19ms, mfu 0.70%\n",
            "iter 1230: loss 1.3726, time 525.52ms, mfu 0.70%\n",
            "iter 1240: loss 1.3947, time 524.13ms, mfu 0.70%\n",
            "step 1250: train loss 1.3106, val loss 1.3437\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.3720, time 74151.77ms, mfu 0.63%\n",
            "iter 1260: loss 1.3606, time 525.36ms, mfu 0.64%\n",
            "iter 1270: loss 1.3769, time 526.13ms, mfu 0.65%\n",
            "iter 1280: loss 1.3791, time 525.64ms, mfu 0.65%\n",
            "iter 1290: loss 1.3962, time 524.61ms, mfu 0.66%\n",
            "iter 1300: loss 1.4188, time 523.59ms, mfu 0.67%\n",
            "iter 1310: loss 1.4100, time 526.97ms, mfu 0.67%\n",
            "iter 1320: loss 1.3364, time 524.56ms, mfu 0.67%\n",
            "iter 1330: loss 1.3481, time 525.90ms, mfu 0.68%\n",
            "iter 1340: loss 1.4392, time 524.42ms, mfu 0.68%\n",
            "iter 1350: loss 1.4199, time 527.22ms, mfu 0.68%\n",
            "iter 1360: loss 1.3792, time 524.33ms, mfu 0.69%\n",
            "iter 1370: loss 1.3546, time 525.33ms, mfu 0.69%\n",
            "iter 1380: loss 1.3629, time 525.80ms, mfu 0.69%\n",
            "iter 1390: loss 1.3611, time 524.35ms, mfu 0.69%\n",
            "iter 1400: loss 1.3650, time 524.37ms, mfu 0.70%\n",
            "iter 1410: loss 1.4062, time 523.78ms, mfu 0.70%\n",
            "iter 1420: loss 1.3746, time 525.12ms, mfu 0.70%\n",
            "iter 1430: loss 1.3790, time 523.72ms, mfu 0.70%\n",
            "iter 1440: loss 1.3933, time 524.47ms, mfu 0.70%\n",
            "iter 1450: loss 1.3574, time 525.68ms, mfu 0.70%\n",
            "iter 1460: loss 1.4183, time 525.08ms, mfu 0.70%\n",
            "iter 1470: loss 1.3542, time 525.24ms, mfu 0.70%\n",
            "iter 1480: loss 1.3618, time 524.98ms, mfu 0.70%\n",
            "iter 1490: loss 1.3233, time 524.72ms, mfu 0.71%\n",
            "step 1500: train loss 1.2666, val loss 1.3108\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.3653, time 74151.70ms, mfu 0.64%\n",
            "iter 1510: loss 1.3077, time 524.69ms, mfu 0.64%\n",
            "iter 1520: loss 1.3921, time 525.63ms, mfu 0.65%\n",
            "iter 1530: loss 1.3236, time 526.30ms, mfu 0.66%\n",
            "iter 1540: loss 1.3568, time 525.78ms, mfu 0.66%\n",
            "iter 1550: loss 1.3222, time 526.63ms, mfu 0.67%\n",
            "iter 1560: loss 1.3108, time 526.94ms, mfu 0.67%\n",
            "iter 1570: loss 1.3315, time 527.93ms, mfu 0.67%\n",
            "iter 1580: loss 1.3375, time 524.68ms, mfu 0.68%\n",
            "iter 1590: loss 1.3187, time 524.07ms, mfu 0.68%\n",
            "iter 1600: loss 1.3181, time 526.22ms, mfu 0.68%\n",
            "iter 1610: loss 1.3175, time 524.20ms, mfu 0.69%\n",
            "iter 1620: loss 1.2983, time 524.99ms, mfu 0.69%\n",
            "iter 1630: loss 1.3108, time 525.42ms, mfu 0.69%\n",
            "iter 1640: loss 1.3279, time 525.80ms, mfu 0.69%\n",
            "iter 1650: loss 1.3485, time 525.93ms, mfu 0.69%\n",
            "iter 1660: loss 1.3328, time 525.95ms, mfu 0.70%\n",
            "iter 1670: loss 1.3333, time 526.98ms, mfu 0.70%\n",
            "iter 1680: loss 1.3143, time 527.42ms, mfu 0.70%\n",
            "iter 1690: loss 1.3541, time 525.48ms, mfu 0.70%\n",
            "iter 1700: loss 1.3209, time 524.69ms, mfu 0.70%\n",
            "iter 1710: loss 1.3219, time 528.00ms, mfu 0.70%\n",
            "iter 1720: loss 1.2758, time 525.29ms, mfu 0.70%\n",
            "iter 1730: loss 1.3046, time 525.31ms, mfu 0.70%\n",
            "iter 1740: loss 1.3337, time 524.78ms, mfu 0.70%\n",
            "step 1750: train loss 1.2264, val loss 1.2768\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.2938, time 74175.05ms, mfu 0.63%\n",
            "iter 1760: loss 1.3121, time 522.80ms, mfu 0.64%\n",
            "iter 1770: loss 1.3322, time 528.57ms, mfu 0.65%\n",
            "iter 1780: loss 1.2594, time 526.70ms, mfu 0.65%\n",
            "iter 1790: loss 1.3266, time 527.63ms, mfu 0.66%\n",
            "iter 1800: loss 1.3082, time 524.76ms, mfu 0.67%\n",
            "iter 1810: loss 1.2951, time 526.28ms, mfu 0.67%\n",
            "iter 1820: loss 1.2606, time 524.63ms, mfu 0.67%\n",
            "iter 1830: loss 1.2779, time 525.49ms, mfu 0.68%\n",
            "iter 1840: loss 1.2832, time 526.28ms, mfu 0.68%\n",
            "iter 1850: loss 1.2898, time 525.85ms, mfu 0.68%\n",
            "iter 1860: loss 1.2797, time 525.52ms, mfu 0.69%\n",
            "iter 1870: loss 1.2819, time 525.90ms, mfu 0.69%\n",
            "iter 1880: loss 1.2994, time 528.41ms, mfu 0.69%\n",
            "iter 1890: loss 1.2912, time 526.51ms, mfu 0.69%\n",
            "iter 1900: loss 1.3086, time 526.18ms, mfu 0.69%\n",
            "iter 1910: loss 1.3103, time 523.72ms, mfu 0.70%\n",
            "iter 1920: loss 1.2751, time 528.48ms, mfu 0.70%\n",
            "iter 1930: loss 1.2733, time 525.64ms, mfu 0.70%\n",
            "iter 1940: loss 1.3341, time 525.08ms, mfu 0.70%\n",
            "iter 1950: loss 1.3251, time 526.11ms, mfu 0.70%\n",
            "iter 1960: loss 1.2789, time 525.59ms, mfu 0.70%\n",
            "iter 1970: loss 1.2477, time 527.20ms, mfu 0.70%\n",
            "iter 1980: loss 1.2920, time 525.71ms, mfu 0.70%\n",
            "iter 1990: loss 1.2602, time 525.69ms, mfu 0.70%\n",
            "step 2000: train loss 1.1944, val loss 1.2538\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.2361, time 74283.10ms, mfu 0.63%\n",
            "iter 2010: loss 1.2800, time 522.76ms, mfu 0.64%\n",
            "iter 2020: loss 1.2559, time 525.79ms, mfu 0.65%\n",
            "iter 2030: loss 1.2576, time 527.60ms, mfu 0.65%\n",
            "iter 2040: loss 1.2559, time 525.00ms, mfu 0.66%\n",
            "iter 2050: loss 1.2967, time 527.20ms, mfu 0.67%\n",
            "iter 2060: loss 1.2860, time 523.49ms, mfu 0.67%\n",
            "iter 2070: loss 1.2527, time 527.03ms, mfu 0.67%\n",
            "iter 2080: loss 1.2377, time 524.63ms, mfu 0.68%\n",
            "iter 2090: loss 1.2218, time 524.07ms, mfu 0.68%\n",
            "iter 2100: loss 1.3008, time 525.42ms, mfu 0.68%\n",
            "iter 2110: loss 1.2805, time 526.82ms, mfu 0.69%\n",
            "iter 2120: loss 1.2663, time 525.26ms, mfu 0.69%\n",
            "iter 2130: loss 1.2336, time 524.80ms, mfu 0.69%\n",
            "iter 2140: loss 1.2655, time 527.44ms, mfu 0.69%\n",
            "iter 2150: loss 1.2457, time 524.59ms, mfu 0.69%\n",
            "iter 2160: loss 1.2285, time 527.86ms, mfu 0.70%\n",
            "iter 2170: loss 1.2653, time 524.13ms, mfu 0.70%\n",
            "iter 2180: loss 1.2375, time 527.41ms, mfu 0.70%\n",
            "iter 2190: loss 1.2371, time 524.65ms, mfu 0.70%\n",
            "iter 2200: loss 1.2332, time 524.92ms, mfu 0.70%\n",
            "iter 2210: loss 1.2501, time 527.87ms, mfu 0.70%\n",
            "iter 2220: loss 1.2620, time 525.26ms, mfu 0.70%\n",
            "iter 2230: loss 1.2836, time 523.96ms, mfu 0.70%\n",
            "iter 2240: loss 1.2200, time 523.15ms, mfu 0.70%\n",
            "step 2250: train loss 1.1706, val loss 1.2386\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2250: loss 1.2220, time 74186.76ms, mfu 0.63%\n",
            "iter 2260: loss 1.2576, time 523.89ms, mfu 0.64%\n",
            "iter 2270: loss 1.2270, time 526.63ms, mfu 0.65%\n",
            "iter 2280: loss 1.2403, time 526.55ms, mfu 0.66%\n",
            "iter 2290: loss 1.2463, time 525.23ms, mfu 0.66%\n",
            "iter 2300: loss 1.2158, time 527.68ms, mfu 0.67%\n",
            "iter 2310: loss 1.2099, time 526.00ms, mfu 0.67%\n",
            "iter 2320: loss 1.1901, time 523.84ms, mfu 0.67%\n",
            "iter 2330: loss 1.2138, time 524.46ms, mfu 0.68%\n",
            "iter 2340: loss 1.2622, time 525.64ms, mfu 0.68%\n",
            "iter 2350: loss 1.2808, time 525.82ms, mfu 0.68%\n",
            "iter 2360: loss 1.2363, time 525.17ms, mfu 0.69%\n",
            "iter 2370: loss 1.2540, time 528.97ms, mfu 0.69%\n",
            "iter 2380: loss 1.2315, time 525.16ms, mfu 0.69%\n",
            "iter 2390: loss 1.2622, time 525.56ms, mfu 0.69%\n",
            "iter 2400: loss 1.1913, time 525.67ms, mfu 0.69%\n",
            "iter 2410: loss 1.2362, time 526.59ms, mfu 0.70%\n",
            "iter 2420: loss 1.2331, time 525.23ms, mfu 0.70%\n",
            "iter 2430: loss 1.2303, time 527.37ms, mfu 0.70%\n",
            "iter 2440: loss 1.2216, time 527.51ms, mfu 0.70%\n",
            "iter 2450: loss 1.2049, time 526.94ms, mfu 0.70%\n",
            "iter 2460: loss 1.2286, time 525.92ms, mfu 0.70%\n",
            "iter 2470: loss 1.2391, time 527.02ms, mfu 0.70%\n",
            "iter 2480: loss 1.2200, time 525.60ms, mfu 0.70%\n",
            "iter 2490: loss 1.2090, time 525.44ms, mfu 0.70%\n",
            "step 2500: train loss 1.1473, val loss 1.2177\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2500: loss 1.1959, time 74405.09ms, mfu 0.63%\n",
            "iter 2510: loss 1.1809, time 523.32ms, mfu 0.64%\n",
            "iter 2520: loss 1.2599, time 526.65ms, mfu 0.65%\n",
            "iter 2530: loss 1.2151, time 525.42ms, mfu 0.65%\n",
            "iter 2540: loss 1.2524, time 525.78ms, mfu 0.66%\n",
            "iter 2550: loss 1.2185, time 525.91ms, mfu 0.67%\n",
            "iter 2560: loss 1.2300, time 526.30ms, mfu 0.67%\n",
            "iter 2570: loss 1.1960, time 525.89ms, mfu 0.67%\n",
            "iter 2580: loss 1.2211, time 524.30ms, mfu 0.68%\n",
            "iter 2590: loss 1.2075, time 525.26ms, mfu 0.68%\n",
            "iter 2600: loss 1.2238, time 524.58ms, mfu 0.68%\n",
            "iter 2610: loss 1.2107, time 524.77ms, mfu 0.69%\n",
            "iter 2620: loss 1.1995, time 527.63ms, mfu 0.69%\n",
            "iter 2630: loss 1.2440, time 524.58ms, mfu 0.69%\n",
            "iter 2640: loss 1.1982, time 526.97ms, mfu 0.69%\n",
            "iter 2650: loss 1.2431, time 526.86ms, mfu 0.69%\n",
            "iter 2660: loss 1.2292, time 526.74ms, mfu 0.70%\n",
            "iter 2670: loss 1.2320, time 525.30ms, mfu 0.70%\n",
            "iter 2680: loss 1.2218, time 527.39ms, mfu 0.70%\n",
            "iter 2690: loss 1.2190, time 525.84ms, mfu 0.70%\n",
            "iter 2700: loss 1.2289, time 526.51ms, mfu 0.70%\n",
            "iter 2710: loss 1.2271, time 524.85ms, mfu 0.70%\n",
            "iter 2720: loss 1.2016, time 526.89ms, mfu 0.70%\n",
            "iter 2730: loss 1.1856, time 524.63ms, mfu 0.70%\n",
            "iter 2740: loss 1.2263, time 527.25ms, mfu 0.70%\n",
            "step 2750: train loss 1.1241, val loss 1.2076\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2750: loss 1.1767, time 74113.91ms, mfu 0.63%\n",
            "iter 2760: loss 1.1726, time 524.04ms, mfu 0.64%\n",
            "iter 2770: loss 1.1796, time 527.62ms, mfu 0.65%\n",
            "iter 2780: loss 1.1881, time 528.46ms, mfu 0.65%\n",
            "iter 2790: loss 1.2102, time 526.69ms, mfu 0.66%\n",
            "iter 2800: loss 1.2129, time 524.85ms, mfu 0.66%\n",
            "iter 2810: loss 1.1664, time 524.04ms, mfu 0.67%\n",
            "iter 2820: loss 1.1624, time 526.24ms, mfu 0.67%\n",
            "iter 2830: loss 1.1559, time 524.80ms, mfu 0.68%\n",
            "iter 2840: loss 1.2023, time 525.53ms, mfu 0.68%\n",
            "iter 2850: loss 1.1917, time 523.59ms, mfu 0.68%\n",
            "iter 2860: loss 1.2324, time 525.83ms, mfu 0.69%\n",
            "iter 2870: loss 1.1695, time 525.49ms, mfu 0.69%\n",
            "iter 2880: loss 1.2203, time 525.00ms, mfu 0.69%\n",
            "iter 2890: loss 1.2123, time 522.65ms, mfu 0.69%\n",
            "iter 2900: loss 1.1777, time 524.93ms, mfu 0.70%\n",
            "iter 2910: loss 1.1562, time 524.11ms, mfu 0.70%\n",
            "iter 2920: loss 1.1610, time 525.50ms, mfu 0.70%\n",
            "iter 2930: loss 1.1352, time 525.59ms, mfu 0.70%\n",
            "iter 2940: loss 1.1918, time 525.88ms, mfu 0.70%\n",
            "iter 2950: loss 1.1745, time 526.12ms, mfu 0.70%\n",
            "iter 2960: loss 1.1692, time 528.14ms, mfu 0.70%\n",
            "iter 2970: loss 1.1575, time 526.97ms, mfu 0.70%\n",
            "iter 2980: loss 1.1364, time 526.02ms, mfu 0.70%\n",
            "iter 2990: loss 1.1857, time 528.65ms, mfu 0.70%\n",
            "step 3000: train loss 1.1047, val loss 1.1979\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3000: loss 1.1585, time 74057.77ms, mfu 0.63%\n",
            "iter 3010: loss 1.1903, time 522.95ms, mfu 0.64%\n",
            "iter 3020: loss 1.1375, time 525.64ms, mfu 0.65%\n",
            "iter 3030: loss 1.1489, time 528.55ms, mfu 0.65%\n",
            "iter 3040: loss 1.1441, time 525.53ms, mfu 0.66%\n",
            "iter 3050: loss 1.1732, time 524.88ms, mfu 0.67%\n",
            "iter 3060: loss 1.1651, time 526.22ms, mfu 0.67%\n",
            "iter 3070: loss 1.1878, time 524.38ms, mfu 0.67%\n",
            "iter 3080: loss 1.1854, time 525.70ms, mfu 0.68%\n",
            "iter 3090: loss 1.2059, time 528.71ms, mfu 0.68%\n",
            "iter 3100: loss 1.1660, time 525.68ms, mfu 0.68%\n",
            "iter 3110: loss 1.1808, time 527.26ms, mfu 0.69%\n",
            "iter 3120: loss 1.1645, time 524.46ms, mfu 0.69%\n",
            "iter 3130: loss 1.1917, time 524.61ms, mfu 0.69%\n",
            "iter 3140: loss 1.1544, time 525.21ms, mfu 0.69%\n",
            "iter 3150: loss 1.1440, time 527.42ms, mfu 0.69%\n",
            "iter 3160: loss 1.1578, time 527.44ms, mfu 0.70%\n",
            "iter 3170: loss 1.1569, time 525.52ms, mfu 0.70%\n",
            "iter 3180: loss 1.2027, time 527.69ms, mfu 0.70%\n",
            "iter 3190: loss 1.1878, time 527.26ms, mfu 0.70%\n",
            "iter 3200: loss 1.1924, time 526.09ms, mfu 0.70%\n",
            "iter 3210: loss 1.1652, time 525.64ms, mfu 0.70%\n",
            "iter 3220: loss 1.1651, time 527.46ms, mfu 0.70%\n",
            "iter 3230: loss 1.2107, time 525.94ms, mfu 0.70%\n",
            "iter 3240: loss 1.1603, time 524.88ms, mfu 0.70%\n",
            "step 3250: train loss 1.0903, val loss 1.1878\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3250: loss 1.1656, time 74176.08ms, mfu 0.63%\n",
            "iter 3260: loss 1.1520, time 523.05ms, mfu 0.64%\n",
            "iter 3270: loss 1.1822, time 528.16ms, mfu 0.65%\n",
            "iter 3280: loss 1.1622, time 527.58ms, mfu 0.65%\n",
            "iter 3290: loss 1.1162, time 524.28ms, mfu 0.66%\n",
            "iter 3300: loss 1.1683, time 525.93ms, mfu 0.66%\n",
            "iter 3310: loss 1.1815, time 528.99ms, mfu 0.67%\n",
            "iter 3320: loss 1.1260, time 527.67ms, mfu 0.67%\n",
            "iter 3330: loss 1.1511, time 526.31ms, mfu 0.68%\n",
            "iter 3340: loss 1.1198, time 527.27ms, mfu 0.68%\n",
            "iter 3350: loss 1.1436, time 528.07ms, mfu 0.68%\n",
            "iter 3360: loss 1.1683, time 527.02ms, mfu 0.69%\n",
            "iter 3370: loss 1.1555, time 529.30ms, mfu 0.69%\n",
            "iter 3380: loss 1.1357, time 526.00ms, mfu 0.69%\n",
            "iter 3390: loss 1.1527, time 525.77ms, mfu 0.69%\n",
            "iter 3400: loss 1.1545, time 526.61ms, mfu 0.69%\n",
            "iter 3410: loss 1.1361, time 527.58ms, mfu 0.69%\n",
            "iter 3420: loss 1.1368, time 526.25ms, mfu 0.70%\n",
            "iter 3430: loss 1.1839, time 525.96ms, mfu 0.70%\n",
            "iter 3440: loss 1.1527, time 526.35ms, mfu 0.70%\n",
            "iter 3450: loss 1.1494, time 528.45ms, mfu 0.70%\n",
            "iter 3460: loss 1.1286, time 527.48ms, mfu 0.70%\n",
            "iter 3470: loss 1.1270, time 527.19ms, mfu 0.70%\n",
            "iter 3480: loss 1.1939, time 525.59ms, mfu 0.70%\n",
            "iter 3490: loss 1.1861, time 526.47ms, mfu 0.70%\n",
            "step 3500: train loss 1.0712, val loss 1.1829\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3500: loss 1.1335, time 74318.37ms, mfu 0.63%\n",
            "iter 3510: loss 1.1350, time 523.63ms, mfu 0.64%\n",
            "iter 3520: loss 1.1192, time 527.96ms, mfu 0.65%\n",
            "iter 3530: loss 1.1508, time 526.14ms, mfu 0.65%\n",
            "iter 3540: loss 1.1433, time 529.61ms, mfu 0.66%\n",
            "iter 3550: loss 1.1454, time 526.72ms, mfu 0.66%\n",
            "iter 3560: loss 1.1756, time 525.20ms, mfu 0.67%\n",
            "iter 3570: loss 1.1710, time 527.01ms, mfu 0.67%\n",
            "iter 3580: loss 1.1623, time 527.39ms, mfu 0.68%\n",
            "iter 3590: loss 1.1249, time 527.33ms, mfu 0.68%\n",
            "iter 3600: loss 1.1490, time 528.54ms, mfu 0.68%\n",
            "iter 3610: loss 1.1317, time 526.38ms, mfu 0.68%\n",
            "iter 3620: loss 1.1248, time 525.29ms, mfu 0.69%\n",
            "iter 3630: loss 1.1131, time 523.29ms, mfu 0.69%\n",
            "iter 3640: loss 1.1695, time 525.80ms, mfu 0.69%\n",
            "iter 3650: loss 1.1528, time 526.18ms, mfu 0.69%\n",
            "iter 3660: loss 1.1150, time 527.70ms, mfu 0.70%\n",
            "iter 3670: loss 1.1210, time 523.82ms, mfu 0.70%\n",
            "iter 3680: loss 1.1130, time 527.78ms, mfu 0.70%\n",
            "iter 3690: loss 1.1234, time 527.66ms, mfu 0.70%\n",
            "iter 3700: loss 1.1443, time 526.43ms, mfu 0.70%\n",
            "iter 3710: loss 1.1171, time 527.31ms, mfu 0.70%\n",
            "iter 3720: loss 1.1515, time 526.76ms, mfu 0.70%\n",
            "iter 3730: loss 1.1353, time 525.95ms, mfu 0.70%\n",
            "iter 3740: loss 1.1574, time 525.63ms, mfu 0.70%\n",
            "step 3750: train loss 1.0570, val loss 1.1755\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3750: loss 1.1443, time 74144.63ms, mfu 0.63%\n",
            "iter 3760: loss 1.1419, time 522.80ms, mfu 0.64%\n",
            "iter 3770: loss 1.1071, time 525.65ms, mfu 0.65%\n",
            "iter 3780: loss 1.1555, time 528.12ms, mfu 0.65%\n",
            "iter 3790: loss 1.1270, time 523.65ms, mfu 0.66%\n",
            "iter 3800: loss 1.1787, time 523.96ms, mfu 0.67%\n",
            "iter 3810: loss 1.1411, time 524.50ms, mfu 0.67%\n",
            "iter 3820: loss 1.1506, time 524.68ms, mfu 0.67%\n",
            "iter 3830: loss 1.1084, time 525.98ms, mfu 0.68%\n",
            "iter 3840: loss 1.1312, time 527.18ms, mfu 0.68%\n",
            "iter 3850: loss 1.1253, time 526.48ms, mfu 0.68%\n",
            "iter 3860: loss 1.1094, time 525.76ms, mfu 0.69%\n",
            "iter 3870: loss 1.1530, time 527.06ms, mfu 0.69%\n",
            "iter 3880: loss 1.1274, time 527.83ms, mfu 0.69%\n",
            "iter 3890: loss 1.1210, time 526.95ms, mfu 0.69%\n",
            "iter 3900: loss 1.1047, time 528.19ms, mfu 0.69%\n",
            "iter 3910: loss 1.1069, time 527.30ms, mfu 0.70%\n",
            "iter 3920: loss 1.1548, time 529.26ms, mfu 0.70%\n",
            "iter 3930: loss 1.1136, time 527.92ms, mfu 0.70%\n",
            "iter 3940: loss 1.1059, time 527.11ms, mfu 0.70%\n",
            "iter 3950: loss 1.1574, time 527.33ms, mfu 0.70%\n",
            "iter 3960: loss 1.0874, time 527.84ms, mfu 0.70%\n",
            "iter 3970: loss 1.1480, time 528.30ms, mfu 0.70%\n",
            "iter 3980: loss 1.1411, time 526.14ms, mfu 0.70%\n",
            "iter 3990: loss 1.1270, time 524.84ms, mfu 0.70%\n",
            "step 4000: train loss 1.0446, val loss 1.1681\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 4000: loss 1.1190, time 74063.90ms, mfu 0.63%\n",
            "iter 4010: loss 1.0915, time 522.81ms, mfu 0.64%\n",
            "iter 4020: loss 1.1068, time 529.50ms, mfu 0.65%\n",
            "iter 4030: loss 1.1044, time 525.03ms, mfu 0.65%\n",
            "iter 4040: loss 1.1154, time 526.83ms, mfu 0.66%\n",
            "iter 4050: loss 1.0991, time 525.75ms, mfu 0.66%\n",
            "iter 4060: loss 1.1227, time 527.36ms, mfu 0.67%\n",
            "iter 4070: loss 1.1463, time 523.94ms, mfu 0.67%\n",
            "iter 4080: loss 1.1695, time 529.06ms, mfu 0.68%\n",
            "iter 4090: loss 1.1448, time 525.05ms, mfu 0.68%\n",
            "iter 4100: loss 1.1148, time 524.24ms, mfu 0.68%\n",
            "iter 4110: loss 1.0903, time 525.91ms, mfu 0.69%\n",
            "iter 4120: loss 1.1238, time 525.97ms, mfu 0.69%\n",
            "iter 4130: loss 1.1441, time 526.56ms, mfu 0.69%\n",
            "iter 4140: loss 1.1304, time 524.32ms, mfu 0.69%\n",
            "iter 4150: loss 1.1309, time 525.54ms, mfu 0.69%\n",
            "iter 4160: loss 1.0580, time 526.71ms, mfu 0.70%\n",
            "iter 4170: loss 1.0859, time 527.49ms, mfu 0.70%\n",
            "iter 4180: loss 1.1398, time 525.61ms, mfu 0.70%\n",
            "iter 4190: loss 1.1274, time 525.01ms, mfu 0.70%\n",
            "iter 4200: loss 1.0934, time 527.65ms, mfu 0.70%\n",
            "iter 4210: loss 1.1253, time 527.24ms, mfu 0.70%\n",
            "iter 4220: loss 1.1136, time 527.93ms, mfu 0.70%\n",
            "iter 4230: loss 1.0890, time 528.24ms, mfu 0.70%\n",
            "iter 4240: loss 1.0792, time 528.01ms, mfu 0.70%\n",
            "step 4250: train loss 1.0329, val loss 1.1636\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 4250: loss 1.1222, time 74391.24ms, mfu 0.63%\n",
            "iter 4260: loss 1.1141, time 524.81ms, mfu 0.64%\n",
            "iter 4270: loss 1.1382, time 525.87ms, mfu 0.65%\n",
            "iter 4280: loss 1.1429, time 525.72ms, mfu 0.65%\n",
            "iter 4290: loss 1.1180, time 525.84ms, mfu 0.66%\n",
            "iter 4300: loss 1.0812, time 526.24ms, mfu 0.66%\n",
            "iter 4310: loss 1.0625, time 527.32ms, mfu 0.67%\n",
            "iter 4320: loss 1.1072, time 526.91ms, mfu 0.67%\n",
            "iter 4330: loss 1.1321, time 525.48ms, mfu 0.68%\n",
            "iter 4340: loss 1.1292, time 527.80ms, mfu 0.68%\n",
            "iter 4350: loss 1.1045, time 527.03ms, mfu 0.68%\n",
            "iter 4360: loss 1.0879, time 526.13ms, mfu 0.69%\n",
            "iter 4370: loss 1.1290, time 527.12ms, mfu 0.69%\n",
            "iter 4380: loss 1.1150, time 529.01ms, mfu 0.69%\n",
            "iter 4390: loss 1.0813, time 525.69ms, mfu 0.69%\n",
            "iter 4400: loss 1.0933, time 526.16ms, mfu 0.69%\n",
            "iter 4410: loss 1.1091, time 525.30ms, mfu 0.70%\n",
            "iter 4420: loss 1.0801, time 525.77ms, mfu 0.70%\n",
            "iter 4430: loss 1.0962, time 526.77ms, mfu 0.70%\n",
            "iter 4440: loss 1.0969, time 525.73ms, mfu 0.70%\n",
            "iter 4450: loss 1.1007, time 527.61ms, mfu 0.70%\n",
            "iter 4460: loss 1.1228, time 525.69ms, mfu 0.70%\n",
            "iter 4470: loss 1.1231, time 526.10ms, mfu 0.70%\n",
            "iter 4480: loss 1.1124, time 525.33ms, mfu 0.70%\n",
            "iter 4490: loss 1.1388, time 526.30ms, mfu 0.70%\n",
            "step 4500: train loss 1.0238, val loss 1.1614\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 4500: loss 1.0848, time 74286.04ms, mfu 0.63%\n",
            "iter 4510: loss 1.0950, time 525.68ms, mfu 0.64%\n",
            "iter 4520: loss 1.0937, time 527.09ms, mfu 0.65%\n",
            "iter 4530: loss 1.0855, time 528.55ms, mfu 0.65%\n",
            "iter 4540: loss 1.0726, time 527.31ms, mfu 0.66%\n",
            "iter 4550: loss 1.1104, time 525.50ms, mfu 0.66%\n",
            "iter 4560: loss 1.1062, time 524.73ms, mfu 0.67%\n",
            "iter 4570: loss 1.1031, time 524.61ms, mfu 0.67%\n",
            "iter 4580: loss 1.1260, time 526.07ms, mfu 0.68%\n",
            "iter 4590: loss 1.1065, time 523.35ms, mfu 0.68%\n",
            "iter 4600: loss 1.0706, time 524.12ms, mfu 0.68%\n",
            "iter 4610: loss 1.0887, time 526.45ms, mfu 0.69%\n",
            "iter 4620: loss 1.1053, time 526.79ms, mfu 0.69%\n",
            "iter 4630: loss 1.1146, time 525.46ms, mfu 0.69%\n",
            "iter 4640: loss 1.1227, time 527.42ms, mfu 0.69%\n",
            "iter 4650: loss 1.0635, time 525.54ms, mfu 0.69%\n",
            "iter 4660: loss 1.0940, time 524.78ms, mfu 0.70%\n",
            "iter 4670: loss 1.1159, time 526.38ms, mfu 0.70%\n",
            "iter 4680: loss 1.0826, time 525.52ms, mfu 0.70%\n",
            "iter 4690: loss 1.0885, time 524.43ms, mfu 0.70%\n",
            "iter 4700: loss 1.1015, time 526.27ms, mfu 0.70%\n",
            "iter 4710: loss 1.0873, time 527.03ms, mfu 0.70%\n",
            "iter 4720: loss 1.1178, time 524.39ms, mfu 0.70%\n",
            "iter 4730: loss 1.0777, time 526.80ms, mfu 0.70%\n",
            "iter 4740: loss 1.1204, time 524.28ms, mfu 0.70%\n",
            "step 4750: train loss 1.0159, val loss 1.1566\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 4750: loss 1.0767, time 74125.80ms, mfu 0.63%\n",
            "iter 4760: loss 1.1110, time 524.11ms, mfu 0.64%\n",
            "iter 4770: loss 1.1062, time 525.66ms, mfu 0.65%\n",
            "iter 4780: loss 1.1043, time 527.20ms, mfu 0.65%\n",
            "iter 4790: loss 1.0899, time 525.08ms, mfu 0.66%\n",
            "iter 4800: loss 1.0925, time 525.12ms, mfu 0.67%\n",
            "iter 4810: loss 1.0990, time 525.98ms, mfu 0.67%\n",
            "iter 4820: loss 1.1135, time 526.13ms, mfu 0.67%\n",
            "iter 4830: loss 1.0706, time 524.46ms, mfu 0.68%\n",
            "iter 4840: loss 1.1184, time 525.16ms, mfu 0.68%\n",
            "iter 4850: loss 1.1008, time 526.18ms, mfu 0.68%\n",
            "iter 4860: loss 1.0977, time 526.45ms, mfu 0.69%\n",
            "iter 4870: loss 1.0849, time 525.10ms, mfu 0.69%\n",
            "iter 4880: loss 1.1318, time 525.76ms, mfu 0.69%\n",
            "iter 4890: loss 1.1065, time 527.77ms, mfu 0.69%\n",
            "iter 4900: loss 1.0970, time 525.59ms, mfu 0.69%\n",
            "iter 4910: loss 1.0947, time 526.24ms, mfu 0.70%\n",
            "iter 4920: loss 1.1103, time 525.60ms, mfu 0.70%\n",
            "iter 4930: loss 1.1035, time 527.09ms, mfu 0.70%\n",
            "iter 4940: loss 1.0696, time 524.91ms, mfu 0.70%\n",
            "iter 4950: loss 1.1019, time 525.30ms, mfu 0.70%\n",
            "iter 4960: loss 1.1256, time 526.18ms, mfu 0.70%\n",
            "iter 4970: loss 1.0858, time 527.59ms, mfu 0.70%\n",
            "iter 4980: loss 1.1264, time 527.94ms, mfu 0.70%\n",
            "iter 4990: loss 1.1008, time 526.83ms, mfu 0.70%\n",
            "step 5000: train loss 1.0117, val loss 1.1538\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 5000: loss 1.0905, time 74150.91ms, mfu 0.63%\n"
          ]
        }
      ],
      "source": [
        "# Inicia o treinamento!\n",
        "# Isso pode levar de 5 a 15 minutos para mostrar resultados decentes.\n",
        "# O modelo salvará 'checkpoints' na pasta 'out-shakespeare-char'\n",
        "!python train.py config/train_shakespeare_char.py"
      ],
      "id": "JyEDdI2GxaWL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onwrQZNexaWL"
      },
      "source": [
        "**Nota sobre o Treinamento:**\n",
        "\n",
        "O script padrão roda por `max_iters = 5000` iterações. Para uma demonstração, você não precisa esperar até o fim. Você pode **interromper a execução manualmente** (clicando no botão \"Stop\") quando o `val loss` (loss de validação) estiver baixo (algo em torno de 1.5 ou 1.6). O modelo já será capaz de gerar texto reconhecível."
      ],
      "id": "onwrQZNexaWL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNOb46iJxaWM"
      },
      "source": [
        "### Passo 5: Gerar Texto (A Demonstração)\n",
        "\n",
        "Após o modelo ter treinado por um tempo, ele terá salvo um *checkpoint*. Vamos usar o script `sample.py` para carregar esse checkpoint e gerar texto novo."
      ],
      "id": "cNOb46iJxaWM"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR4FfxP4xaWM",
        "outputId": "e2471279-baf3-498e-b239-6bfdc208e421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "number of parameters: 10.67M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            " «O disco escolhimento técnico é de o fato é que há um desconsumidor desse prazo . \n",
            " Em novembro de 1994, a estratégia de onde provocou por mais rápidas compostas dos séculos 10 na Bósnia e do desfile de «Preconcorrência do Banco do Brasil» . \n",
            " O BB tem ocorrido . \n",
            "  (Leonardo Parreira)  \n",
            " A decisão do Fernando Henrique não quis votar em conta com o PSDB não convencer . \n",
            " Os alunos e FHC foram expulsos por livro, mas até agora as redes de estabilização do setor . \n",
            " A greve de Atenas, foi exibida\n",
            "---------------\n",
            "\n",
            " O livro Tribunal Regional Federal pediu ontem uma pressão do ministro do Plano Real . \n",
            " Os programas de cerca de 190 mil pessoas ficaram fechadas para aproveitar a sociedade de cada um desconto de atos do ABCD . \n",
            " O terceiro artigo antes da Constituição (região civil para a Constituição dos Estados Unidos e Unidos), onde Lula criaram desde a maioria dos anos, atualmente, respondeu por uma perda de um pior decartamento de pancos anteriores . \n",
            "  Hoje o embaixador  \n",
            " Ao contrário do anúncio do lad\n",
            "---------------\n",
            "\n",
            " Com o presidente do BC, as guardas contaminadas pelo TST paulista e outras datas . \n",
            " Não me reiniciou de claras que não dobram a continuidade do pagamento . \n",
            "  Novo tempo da gravação e benefícios  \n",
            "  Da Reportagem Local  \n",
            " A decisão pode constitucionar que espera construir mais altos no SPS . \n",
            " Os dois profissionais do salário serão revelados ao próximo dia 40 de março na estabilização de poupança financeira . \n",
            " A variação do diretor não aprova em relação ao Plano Real de Justiça (PFL-AM), Alex\n",
            "---------------\n",
            "\n",
            " O objetivo de mais oferta é grande exposição do programa de empresas no primeiro turno . \n",
            " A troca de madrugada de TV fez controlada no chão do seu volante Antonio Carlos Melo . \n",
            " Ao contrário, a seleção do ano passado com seu partido na Itália, que tenta ser bem controlada no começo das artilhas . \n",
            " O corregedor tucano acha que seu banco está atraente . \n",
            " Como falta dos sérvios, eles negaram a vitória para o rosto da margem . \n",
            " E também é o melhor dos quais o nascimento . \n",
            " De ontem, o Banco C\n",
            "---------------\n",
            "\n",
            " Ontem, os motoristas estaduais já estão arrelados em corrosporte por investimentos foram considerados as indústrias dos terrenos em alta de maio . \n",
            " Em seguida, com o Projeto de Agricultura de São Paulo, o presidente do Banespa (Banco Central) aumentou 20% mais de 7,5% . \n",
            "  Pela instalação de carta de cerca de 10 partidas  \n",
            "  Serviços políticos querem ter 10% dos votos  \n",
            "  Da Reportagem Local  \n",
            " A estimativa de preços de US$ 0,8% a setembro com preços para US$ 7,48 . \n",
            " O Metropolitana de São Pa\n",
            "---------------\n",
            "\n",
            " O artista provocou uma viagem no máximo ano passado pelo uso do desempenho brasileiro de nossos últimos serviços entre os filmes e a melhor no dia 20 de julho . \n",
            " Segundo o programa de convênio no México (ONGs), o presidente da Câmara dos Deputados, Antonio Vallelasco (PFL-PE), disse que o presidente salarial . \n",
            " A atenção não resulta o senador Paulo Manuel (PPR-RJ) que o ministro da Fazenda, Marcelo Fernando Henrique Cardoso foi decidido . \n",
            " «Isso não está houve produtos para recuperar a compr\n",
            "---------------\n",
            "\n",
            " O motorista achou que a sociedade do país havia projetado em março último dia 5 de março para o ano . \n",
            " Desses crescimentos, a polícia está feita por segunda-feira . \n",
            " Para eles, a previsão é de que a representação do governo norte-americano tem o nível de animais conhecido . \n",
            " «Isso não acaba definido nem das reservas do dinheiro, que confirmou a substituição de novos planos, foi preso para a despeito de anunciar entre as regras . \n",
            " O Congresso teve a pressão de tempo para estabelecer nos fund\n",
            "---------------\n",
            "\n",
            "  Sérvia de Tráfego  \n",
            " O presidente da Casas Paulistas, Redato Impediu ontem o mercado de artesanato anterior, que deixou hoje às 21h30 . \n",
            " Porém, a equipe de Torresi é de ontem a segunda a tarde, na área do voto Romário, em qualquer abertura há condições de 10 anos . \n",
            " O motorista era considerado por pescadores transformadores do governo Fernando Henrique Cardoso, presidente da CEI (Comissão de Energia) e deputados federais à Comissão de Deputados da Casa Brasileira de Valor . \n",
            " Seu mais comerc\n",
            "---------------\n",
            "\n",
            " Depois, a seleção brasileira é preciso over com o boestil de doenças de bancos e mais vendas em cem de suas arras . \n",
            " A Intenner vai ficar num time do Estado e mundial do Campeonato Paulista e o Santos, começa nos prédios, em seu segmento, com contato com o atual valor da média de real, na baixa de R$ 15 . \n",
            " Segundo o deputado José Paulo Pereira, a assessoria de impostos da Câmara do Município de Saúde realizou um reajuste de 2,15% do total de compras de ações e tiros ostentaram aceitar os cand\n",
            "---------------\n",
            "\n",
            "  Da Reportagem Local  \n",
            " A candidatura deverá se enfrentar no máximo 15 mil pessoas no mercado internacional e de juros . \n",
            " A previsão do PFL aprovou descrever durante a operação de cada uma inflação entre a cada novembro e o agente foi repetido com o PSDB . \n",
            " O deputado Internacional de Covas (PPR-RJ) fechou a participação de segundo turno no Plano Real . \n",
            " «O PPR venceu, mas não consegue por aprovação da demissões que deve tirar um problema de parlamentares em torno de importância e a associaç\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# Gera 500 caracteres de texto novo\n",
        "# Ele automaticamente carrega o checkpoint mais recente da pasta 'out-shakespeare-char'\n",
        "!python sample.py --out_dir=out-shakespeare-char"
      ],
      "id": "LR4FfxP4xaWM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6oKfXCNxaWM"
      },
      "source": [
        "**O que observar:**\n",
        "\n",
        "O texto gerado deve se parecer com o inglês de Shakespeare. Você verá nomes de personagens (ex: `KING:`, `JULIET:`) e palavras no estilo arcaico. Isso demonstra que o modelo não *decorou* o texto, mas sim *aprendeu os padrões e o estilo* do corpus.\n",
        "\n",
        "---\n",
        "\n",
        "### (Opcional) Passo 6: Treinar com seu Próprio Corpus\n",
        "\n",
        "Se quiser treinar com um corpus diferente (ex: poemas em português):\n",
        "\n",
        "1.  No painel de arquivos à esquerda, navegue até `nanoGPT/data/`.\n",
        "2.  Crie uma nova pasta (ex: `meu_corpus`).\n",
        "3.  Faça o upload do seu arquivo de texto para essa pasta e renomeie-o para `input.txt`.\n",
        "4.  Execute os comandos abaixo."
      ],
      "id": "m6oKfXCNxaWM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlmfRoYoxaWN"
      },
      "outputs": [],
      "source": [
        "# 1. Crie o diretório (apenas para garantir)\n",
        "!mkdir -p data/meu_corpus\n",
        "\n",
        "#\n",
        "# ----------------------------------------------------------------------\n",
        "# (AGORA, FAÇA O UPLOAD DO SEU 'input.txt' PARA A PASTA 'data/meu_corpus')\n",
        "# ----------------------------------------------------------------------\n",
        "#\n",
        "\n",
        "# 2. Copie o script de preparação\n",
        "!cp data/shakespeare_char/prepare.py data/meu_corpus/prepare.py\n",
        "\n",
        "# 3. Rode a preparação (ele irá tokenizar seu 'input.txt')\n",
        "# (Descomente a linha abaixo após fazer o upload)\n",
        "#!python data/meu_corpus/prepare.py"
      ],
      "id": "SlmfRoYoxaWN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYnQqJKixaWN"
      },
      "outputs": [],
      "source": [
        "# 4. Copie o arquivo de configuração\n",
        "!cp config/train_shakespeare_char.py config/train_meu_corpus.py\n",
        "\n",
        "# 5. EDITE O NOVO ARQUIVO DE CONFIG:\n",
        "#   - Abra 'config/train_meu_corpus.py' no editor de arquivos\n",
        "#   - Mude a linha: dataset = 'shakespeare_char' PARA dataset = 'meu_corpus'\n",
        "#   - Mude a linha: out_dir = 'out-shakespeare-char' PARA out_dir = 'out-meu-corpus'\n",
        "\n",
        "# 6. TREINE com seu corpus\n",
        "# (Descomente a linha abaixo após editar o arquivo de config)\n",
        "#!python train.py config/train_meu_corpus.py"
      ],
      "id": "MYnQqJKixaWN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWCtaDefxaWN"
      },
      "outputs": [],
      "source": [
        "# 7. GERE TEXTO a partir do seu modelo\n",
        "# (Descomente a linha abaixo após o treinamento)\n",
        "#!python sample.py --out_dir=out-meu_corpus --n=500"
      ],
      "id": "GWCtaDefxaWN"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}